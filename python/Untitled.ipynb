{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b654a83-660b-4760-b471-4d044366ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -O https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "chmod +x mc\n",
    "mc alias set opensky https://s3.opensky-network.org k4tAd3Qr1h7x8uGn DLGuWfiVn3xyN5CHOC9TANredkW5ifSE\n",
    "./mc find opensky/ec-datadump/ --path \"*/states_2023-*.parquet\" | while read -r file; do ./mc cp \"$file\" data/ec-datadump/; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88838a20-695a-4088-bbea-cbb67f9e9c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -O https://dl.min.io/client/mc/release/linux-amd64/mc\n",
    "chmod +x mc\n",
    "mc alias set opensky https://s3.opensky-network.org $OSN_USERNAME $OSN_KEY\n",
    "./mc find opensky/ec-datadump/ --path \"*/states_2023-*.parquet\" | while read -r file; do ./mc cp \"$file\" data/ec-datadump/; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47529975-5cfb-45c1-b6cb-269073bc43d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to quinten.goens\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"project_aiu\")\\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\",\"eur-app-aiu-dev\")\\\n",
    "    .config(\"spark.yarn.access.hadoopFileSystems\",\"abfs://storage-fs@cdpdldev0.dfs.core.windows.net/data/project/aiu.db/unmanaged\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "directory_path = \"data/ec-datadump/\"\n",
    "\n",
    "for file_name in os.listdir(directory_path):\n",
    "    if file_name.endswith('.parquet'):\n",
    "        file_path = os.path.join(directory_path, file_name)\n",
    "        \n",
    "        # read each parquet file in the directory\n",
    "        df = pd.read_parquet(file_path)\n",
    "\n",
    "        # Convert the NumPy array to a list in the 'serials' column\n",
    "        df['serials'] = df['serials'].apply(lambda arr: arr.tolist() if isinstance(arr, np.ndarray) else arr)\n",
    "\n",
    "        # Convert the pandas DataFrame to a Spark DataFrame\n",
    "        spark_df = spark.createDataFrame(df)\n",
    "\n",
    "        # Write the Spark DataFrame to the Hive table\n",
    "        spark_df.write.mode(\"append\").insertInto(\"project_aiu.osn_ec_datadump\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddd1fd2b-95ff-4902-9873-5273b141b062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/lib/pyspark.zip/pyspark/sql/pandas/conversion.py:371: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c42679a-c31d-4ea1-bf17-9da11e60309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824d7778-c968-4d81-8589-3a4232e49e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cafaca-33ea-42d3-8e0a-2ce37bc50a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to use the \"project_aiu\" database\n",
    "#spark.sql(\"show databases;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14036474-60a0-4dca-9ef8-22da573eb1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to use the \"project_aiu\" database\n",
    "#spark.sql(\"show tables in project_aiu;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2131df95-a6fc-420e-8078-065597fc2485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7fd5c1-d56f-463b-9d34-fccd8ca7b330",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b8dbe8-4111-41ee-a1d8-00814820f777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0154005-9053-4b39-8b35-4d3dc0bea78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659da444-e356-4de5-a638-7d121e2fc6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/daily/borealis-osn-3dpi_EGLL_EIDW_2023-01-01 00:00_2023-01-02 00:00.parquet.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3389bf33-76de-4b89-bc8e-7bd01f76a0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['icao24'] == '4cad7a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5445ac7-0d24-4133-96c5-f8362d0ebc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782ef401-debf-4398-be9b-27f097761fad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42dcfb7-6424-431e-965b-a5e50439408f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48db6aa-0ffa-4090-bd52-d432db24adcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to use the \"project_aiu\" database\n",
    "spark.sql(\"select * from project_aiu.your_table_name2;\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df162bd2-6efd-46ff-82a4-7a17658269bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de68d44-e1ce-4939-aca8-c5e486da0f90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a0e96-921b-4952-a5b4-15024cd708fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd613e5-1907-4864-a709-e99babcd6ac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159fb8ed-5002-46c8-91f7-58ab8e44a914",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0117e763-7fe4-4ee7-a07c-5b52db8c5aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Write the Spark DataFrame to the Hive table\n",
    "spark_df.write.mode(\"append\").insertInto(\"your_table_name\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50c6e77-c175-4fec-bcc8-a1064e8ec0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66f65a-c990-4187-a502-4d31057bbd5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a512d-b0f9-45f5-969b-87128c8df276",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62462a1-4cd8-4728-b729-682a09e932a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d566c2f2-7489-4080-869a-ff54695886ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd32c886-d802-4f9a-a6cf-e0ee69ba7a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43960af-458c-4f3f-bb9b-c3466e21e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.read_parquet(\"data/daily/borealis-osn-3dpi_EGLL_EIDW_2023-05-17 00:00_2023-05-18 00:00.parquet.gz\").to_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ece808-5260-49ee-b6dd-abb64648016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic.data.adsb.opensky_impala import Impala\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from traffic.data import opensky\n",
    "import itertools\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "cache_dir_path = \"/Users/quintengoens/opensky_cache\"\n",
    "cache_dir = Path(cache_dir_path)\n",
    "\n",
    "def fetch_data(start_time, end_time, ADEP, ADES):\n",
    "    # Convert to datetime\n",
    "    start_time = pd.to_datetime(start_time)\n",
    "    end_time = pd.to_datetime(end_time)\n",
    "    \n",
    "    # Generate a list of dates from start_time to end_time\n",
    "    date_range = pd.date_range(start_time, end_time)\n",
    "    \n",
    "    for date in date_range:\n",
    "        \n",
    "        # Format date for file naming and for opensky.history input\n",
    "        date_str = date.strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "        # Calculate the stop time for this day\n",
    "        stop_time_str = (date + timedelta(days=1)).strftime('%Y-%m-%d %H:%M')\n",
    "        \n",
    "        print()\n",
    "        print(\"-\"*20)\n",
    "        print(f\"Fetching data for period {date_str} - {stop_time_str}...\")\n",
    "        \n",
    "        # If the file already exists, do not rerun\n",
    "        if Path(f\"data/daily/borealis-osn-3dpi_{ADEP}_{ADES}_{date_str}_{stop_time_str}.parquet.gz\").exists():\n",
    "            continue\n",
    "        df_dep = opensky.history(\n",
    "            start=date_str,\n",
    "            stop=stop_time_str,\n",
    "            departure_airport=ADEP,\n",
    "            arrival_airport=ADES,\n",
    "            progressbar=False\n",
    "        )\n",
    "        \n",
    "        df_arr = opensky.history(\n",
    "            start=date_str,\n",
    "            stop=stop_time_str,\n",
    "            departure_airport=ADES,\n",
    "            arrival_airport=ADEP,\n",
    "            progressbar=False\n",
    "        )\n",
    "        \n",
    "        if pd.isnull(df_dep) and pd.isnull(df_arr):\n",
    "            df = pd.DataFrame()\n",
    "            print(\"Both are NULL\")\n",
    "\n",
    "        if pd.isnull(df_dep) and not pd.isnull(df_arr):\n",
    "            df = df_arr.data\n",
    "            print(\"df_dep is NULL\")\n",
    "\n",
    "        if pd.isnull(df_arr) and not pd.isnull(df_dep):\n",
    "            df = df_dep.data\n",
    "            print(\"df_arr is NULL\")\n",
    "        \n",
    "        if not pd.isnull(df_arr) and not pd.isnull(df_dep):\n",
    "            print(\"both not NULL\")\n",
    "            df = pd.concat([df_dep.data, df_arr.data])\n",
    "        \n",
    "        # Convert timestamp column(s) to a lower precision (like microseconds)\n",
    "        for col in df.columns:\n",
    "            if pd.api.types.is_datetime64_any_dtype(df[col]):\n",
    "                df[col] = df[col].values.astype('datetime64[us]')\n",
    "        \n",
    "        # Write to compressed parquet file\n",
    "        df.to_parquet(f\"data/daily/borealis-osn-3dpi_{ADEP}_{ADES}_{date_str}_{stop_time_str}.parquet.gz\", compression=\"gzip\")\n",
    "        print(\"-\"*20)\n",
    "        print()\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10301a-d2bd-4a6d-a6a0-4efe93b953dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a293da55-0a9e-4819-87b6-58e1c70f7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function on all the airport combinations possible \n",
    "apts = [\"EGLL\",\"EIDW\", \"EKCH\", \"BIRK\", \"ESSA\"]\n",
    "\n",
    "# Calculate all the combinations of apt pairs\n",
    "start_time = \"2023-01-01 00:00\"\n",
    "end_time = \"2023-06-01 00:00\"\n",
    "\n",
    "for apt1, apt2 in list(itertools.combinations(apts, 2)):\n",
    "    print(apt1, apt2, datetime.now())\n",
    "  fetch_data(start_time, end_time, apt1, apt2)\n",
    "    \n",
    "    print(\"Done!\")\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f203f183-f289-4647-80e3-8a79e3dc62bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
