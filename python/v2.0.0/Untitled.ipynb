{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecfb0597-da0c-4bf7-b7de-39b2d7cbba98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"https://spark-5ue7hy2q572iujml.cml-live.az-live.x9er-zkvz.cloudera.site\">https://spark-5ue7hy2q572iujml.cml-live.az-live.x9er-zkvz.cloudera.site</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import display, HTML\n",
    "# Get environment variables\n",
    "engine_id = os.getenv('CDSW_ENGINE_ID')\n",
    "domain = os.getenv('CDSW_DOMAIN')\n",
    "\n",
    "# Format the URL\n",
    "url = f\"https://spark-{engine_id}.{domain}\"\n",
    "\n",
    "# Display the clickable URL\n",
    "display(HTML(f'<a href=\"{url}\">{url}</a>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e3341d2-b4cd-4813-993b-41696119f71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Query to create OPDI Flight Events table:\n",
      " \n",
      "    CREATE TABLE IF NOT EXISTS `project_opdi`.`opdi_flight_events` (\n",
      "\n",
      "        id STRING COMMENT 'Primary Key: Unique milestone identifier for each record.',\n",
      "        flight_id STRING COMMENT 'Foreign Key: Identifier linking the flight event to the flight trajectory table.',\n",
      "\n",
      "        milestone_type STRING COMMENT 'Type of the flight event (milestone) being recorded.',\n",
      "        event_time BIGINT COMMENT 'Timestamp for the flight event.',\n",
      "        longitude DOUBLE COMMENT 'Longitude coordinate of the flight event.',\n",
      "        latitude DOUBLE COMMENT 'Latitude coordinate of the flight event.',\n",
      "        altitude DOUBLE COMMENT 'Altitude at which the flight event occurred.',\n",
      "        \n",
      "        source STRING COMMENT 'Source of the trajectory data.',\n",
      "        version STRING COMMENT 'Version of the flight event (milestone) determination algorithm.',\n",
      "        info STRING COMMENT 'Additional information or description of the flight event.'\n",
      "    )\n",
      "    COMMENT '`project_opdi`.`opdi_flight_events` table containing various OSN trajectory flight events. Last updated: 05 August 2024.'\n",
      "    STORED AS parquet\n",
      "    TBLPROPERTIES ('transactional'='false');\n",
      "\n",
      " Query to create OSN Measurements table:\n",
      " \n",
      "    CREATE TABLE IF NOT EXISTS `project_opdi`.`opdi_measurements` (\n",
      "        \n",
      "        id STRING COMMENT 'Primary Key: Unique measurement identifier for each record.',\n",
      "        milestone_id STRING COMMENT 'Foreign Key: Identifier linking the measurement to the corresponding event in the flight events table.',\n",
      "        \n",
      "        type STRING COMMENT 'Type of measurement, e.g. flown distance or fuel consumption.',\n",
      "        value DOUBLE COMMENT 'Value of the measurement.',\n",
      "        version STRING COMMENT 'The version of the measurement calculation.'\n",
      "    )\n",
      "    COMMENT '`project_opdi`.opdi_measurements` table containing various measurements for OPDI flight events. Last updated: 05 August 2024.'\n",
      "    STORED AS parquet\n",
      "    TBLPROPERTIES ('transactional'='false');\n",
      "\n",
      "------------------------------\n",
      "Starting milestone extraction process for timeframe 2022-01-01 00:00:00 until 2023-08-31 00:00:00...\n",
      "Processing flight events for month: 2022-02-01\n",
      "Calculating first_seen/last_seen events for batch_id: 20220201_\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'logs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4622/2448628109.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    985\u001b[0m           \u001b[0mprocessed_months_horizontal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m           \u001b[0mprocessed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'months'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mprocessed_months_horizontal\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 987\u001b[0;31m           \u001b[0mprocessed_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath_horizontal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcalc_vertical\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m           \u001b[0mprocessed_months_vertical\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2973\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparquet\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mto_parquet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2975\u001b[0;31m         return to_parquet(\n\u001b[0m\u001b[1;32m   2976\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2977\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFilePath\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mWriteBuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     impl.write(\n\u001b[0m\u001b[1;32m    431\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mpath_or_buf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mwrite\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfrom_pandas_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m         path_or_handle, handles, kwargs[\"filesystem\"] = _get_path_or_handle(\n\u001b[0m\u001b[1;32m    177\u001b[0m             \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"filesystem\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36m_get_path_or_handle\u001b[0;34m(path, fs, storage_options, mode, is_dir)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;31m# fsspec resources can also point to directories\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# this branch is used for example when reading from non-fsspec URLs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         handles = get_handle(\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_text\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    732\u001b[0m     \u001b[0;31m# Only for write methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m         \u001b[0mcheck_parent_directory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mcheck_parent_directory\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0mparent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mrf\"Cannot save file into a non-existent directory: '{parent}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'logs'"
     ]
    }
   ],
   "source": [
    "# Regular imports\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from openap.phase import FlightPhase\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Spark imports\n",
    "\n",
    "\n",
    "# Hotfix\n",
    "#!cp /runtime-addons/cmladdon-2.0.40-b150/log4j.properties /etc/spark/conf/\n",
    "\n",
    "# Spark Session Initialization\n",
    "#shutil.copy(\"/runtime-addons/cmladdon-2.0.40-b150/log4j.properties\", \"/etc/spark/conf/\") # Setting logging properties\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OPDI flight events and measurements ETL\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\")\\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"5G\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"15\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\")\\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config('spark.ui.showConsoleProgress', False) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Database prep\n",
    "\n",
    "# Settings\n",
    "project = \"project_opdi\"\n",
    "recreate_flight_event_table = False\n",
    "recreate_measurement_table = False\n",
    "\n",
    "## Range for processing\n",
    "start_date = datetime.strptime('2022-01-01', '%Y-%m-%d')\n",
    "end_date = datetime.strptime('2023-08-31', '%Y-%m-%d')\n",
    "\n",
    "\n",
    "# Getting today's date\n",
    "today = datetime.today().strftime('%d %B %Y')\n",
    "\n",
    "create_flight_events_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS `{project}`.`opdi_flight_events` (\n",
    "\n",
    "        id STRING COMMENT 'Primary Key: Unique milestone identifier for each record.',\n",
    "        flight_id STRING COMMENT 'Foreign Key: Identifier linking the flight event to the flight trajectory table.',\n",
    "\n",
    "        milestone_type STRING COMMENT 'Type of the flight event (milestone) being recorded.',\n",
    "        event_time BIGINT COMMENT 'Timestamp for the flight event.',\n",
    "        longitude DOUBLE COMMENT 'Longitude coordinate of the flight event.',\n",
    "        latitude DOUBLE COMMENT 'Latitude coordinate of the flight event.',\n",
    "        altitude DOUBLE COMMENT 'Altitude at which the flight event occurred.',\n",
    "        \n",
    "        source STRING COMMENT 'Source of the trajectory data.',\n",
    "        version STRING COMMENT 'Version of the flight event (milestone) determination algorithm.',\n",
    "        info STRING COMMENT 'Additional information or description of the flight event.'\n",
    "    )\n",
    "    COMMENT '`{project}`.`opdi_flight_events` table containing various OSN trajectory flight events. Last updated: {today}.'\n",
    "    STORED AS parquet\n",
    "    TBLPROPERTIES ('transactional'='false');\n",
    "\"\"\"\n",
    "\n",
    "print(f' Query to create OPDI Flight Events table:\\n {create_flight_events_table_sql}')\n",
    "if recreate_flight_event_table:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{project}`.`opdi_flight_events`;\")\n",
    "    spark.sql(create_flight_events_table_sql)\n",
    "\n",
    "create_measurement_table_sql = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS `{project}`.`opdi_measurements` (\n",
    "        \n",
    "        id STRING COMMENT 'Primary Key: Unique measurement identifier for each record.',\n",
    "        milestone_id STRING COMMENT 'Foreign Key: Identifier linking the measurement to the corresponding event in the flight events table.',\n",
    "        \n",
    "        type STRING COMMENT 'Type of measurement, e.g. flown distance or fuel consumption.',\n",
    "        value DOUBLE COMMENT 'Value of the measurement.',\n",
    "        version STRING COMMENT 'The version of the measurement calculation.'\n",
    "    )\n",
    "    COMMENT '`{project}`.opdi_measurements` table containing various measurements for OPDI flight events. Last updated: {today}.'\n",
    "    STORED AS parquet\n",
    "    TBLPROPERTIES ('transactional'='false');\n",
    "\"\"\"\n",
    "\n",
    "print(f' Query to create OSN Measurements table:\\n {create_measurement_table_sql}')\n",
    "if recreate_measurement_table: \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS `{project}`.`osn_measurements`;\")\n",
    "    spark.sql(create_measurement_table_sql)\n",
    "\n",
    "def smooth_baro_alt(df):\n",
    "    # Define the acceptable rate of climb/descent threshold (25.4 meters per second, i.e. 5000ft/min)\n",
    "    rate_threshold = 25.4\n",
    "\n",
    "    # Calculate the rate of climb/descent in meters per second for each track_id\n",
    "    window_spec = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "    df = df.withColumn(\"prev_baro_altitude\", lag(\"baro_altitude\").over(window_spec))\n",
    "    df = df.withColumn(\"prev_event_time\", lag(\"event_time\").over(window_spec))\n",
    "\n",
    "    df = df.withColumn(\"time_diff\", (col(\"event_time\") - col(\"prev_event_time\")))\n",
    "    df = df.withColumn(\"altitude_diff\", (col(\"baro_altitude\") - col(\"prev_baro_altitude\")))\n",
    "    df = df.withColumn(\"rate_of_climb\", col(\"altitude_diff\") / col(\"time_diff\"))\n",
    "\n",
    "    # Create a window for calculating the rolling average\n",
    "    # Define a time window for the rolling average (e.g., 300 seconds or 5 min)\n",
    "    time_window = 300\n",
    "\n",
    "    # Create a window specification for calculating the rolling average based on time\n",
    "    window_spec_avg = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rangeBetween(-time_window, time_window)\n",
    "\n",
    "    # Calculate the rolling average (smoothing)\n",
    "    df = df.withColumn(\"smoothed_baro_altitude\", avg(\"baro_altitude\").over(window_spec_avg))\n",
    "\n",
    "    # Replace unrealistic climb/descent points with the smoothed values\n",
    "    df = df.withColumn(\"baro_altitude\",\n",
    "                       when((abs(col(\"rate_of_climb\")) > rate_threshold),\n",
    "                            col(\"smoothed_baro_altitude\")).otherwise(col(\"baro_altitude\")))\n",
    "\n",
    "    # Drop the temporary columns\n",
    "    df = df.drop(\"smoothed_baro_altitude\", \"prev_baro_altitude\", \"prev_event_time\", \"time_diff\", \"altitude_diff\", \"rate_of_climb\")\n",
    "    return df\n",
    "\n",
    "from pyspark.sql import Window, functions as F\n",
    "from pyspark.sql.functions import col, lag, lead, min, max, when, explode\n",
    "\n",
    "def zmf(col, a, b):\n",
    "    \"\"\"Zero-order membership function (ZMF).\"\"\"\n",
    "    return F.when(col <= a, 1).when(col >= b, 0).otherwise((b - col) / (b - a))\n",
    "\n",
    "def gaussmf(col, mean, sigma):\n",
    "    \"\"\"Gaussian membership function (GaussMF).\"\"\"\n",
    "    return F.exp(-((col - mean) ** 2) / (2 * sigma ** 2))\n",
    "\n",
    "def smf(col, a, b):\n",
    "    \"\"\"S-shaped membership function (SMF).\"\"\"\n",
    "    return F.when(col <= a, 0).when(col >= b, 1).otherwise((col - a) / (b - a))\n",
    "\n",
    "def calculate_horizontal_segment_events(sdf_input):\n",
    "    \"\"\"\n",
    "    Calculate horizontal segment events for flight data.\n",
    "    \n",
    "    Args:\n",
    "        sdf_input (DataFrame): Input Spark DataFrame containing flight data.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with calculated segment events.\n",
    "    \"\"\"\n",
    "    df = sdf_input.select(\n",
    "        \"track_id\", \"lat\", \"lon\", \"event_time\", \"baro_altitude\",\n",
    "        \"vert_rate\", \"velocity\", \"cumulative_distance_nm\", \"cumulative_time_s\"\n",
    "    )\n",
    "\n",
    "    # Extracting OpenAP phases\n",
    "    df = df.withColumn(\"alt\", col(\"baro_altitude\") * 3.28084)\n",
    "    df = df.withColumn(\"roc\", col(\"vert_rate\") * 196.850394)\n",
    "    df = df.withColumn(\"spd\", col(\"velocity\") * 1.94384)\n",
    "\n",
    "    # Apply membership functions to the DataFrame\n",
    "    df = df.withColumn(\"alt_gnd\", zmf(col(\"alt\"), 0, 200))\n",
    "    df = df.withColumn(\"alt_lo\", gaussmf(col(\"alt\"), 10000, 10000))\n",
    "    df = df.withColumn(\"alt_hi\", gaussmf(col(\"alt\"), 35000, 20000))\n",
    "    df = df.withColumn(\"roc_zero\", gaussmf(col(\"roc\"), 0, 100))\n",
    "    df = df.withColumn(\"roc_plus\", smf(col(\"roc\"), 10, 1000))\n",
    "    df = df.withColumn(\"roc_minus\", zmf(col(\"roc\"), -1000, -10))\n",
    "    df = df.withColumn(\"spd_hi\", gaussmf(col(\"spd\"), 600, 100))\n",
    "    df = df.withColumn(\"spd_md\", gaussmf(col(\"spd\"), 300, 100))\n",
    "    df = df.withColumn(\"spd_lo\", gaussmf(col(\"spd\"), 0, 50))\n",
    "\n",
    "    # Cache the DataFrame after initial transformations\n",
    "    df.cache()\n",
    "\n",
    "    # Define window specification for time windows\n",
    "    window_spec = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rangeBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    # Aggregate and apply fuzzy logic within each window\n",
    "    df = df.withColumn(\"alt_mean\", F.avg(\"alt\").over(window_spec))\n",
    "    df = df.withColumn(\"spd_mean\", F.avg(\"spd\").over(window_spec))\n",
    "    df = df.withColumn(\"roc_mean\", F.avg(\"roc\").over(window_spec))\n",
    "\n",
    "    # Apply fuzzy logic rules\n",
    "    df = df.withColumn(\"rule_ground\", F.least(col(\"alt_gnd\"), col(\"roc_zero\"), col(\"spd_lo\")))\n",
    "    df = df.withColumn(\"rule_climb\", F.least(col(\"alt_lo\"), col(\"roc_plus\"), col(\"spd_md\")))\n",
    "    df = df.withColumn(\"rule_descent\", F.least(col(\"alt_lo\"), col(\"roc_minus\"), col(\"spd_md\")))\n",
    "    df = df.withColumn(\"rule_cruise\", F.least(col(\"alt_hi\"), col(\"roc_zero\"), col(\"spd_hi\")))\n",
    "    df = df.withColumn(\"rule_level\", F.least(col(\"alt_lo\"), col(\"roc_zero\"), col(\"spd_md\")))\n",
    "\n",
    "    # Aggregate and determine phase label\n",
    "    df = df.withColumn(\"aggregated\", F.greatest(\n",
    "        col(\"rule_ground\"), col(\"rule_climb\"), col(\"rule_descent\"),\n",
    "        col(\"rule_cruise\"), col(\"rule_level\"))\n",
    "    )\n",
    "\n",
    "    df = df.withColumn(\n",
    "        \"flight_phase\", \n",
    "        F.when(col(\"aggregated\") == col(\"rule_ground\"), \"GND\")\n",
    "         .when(col(\"aggregated\") == col(\"rule_climb\"), \"CL\")\n",
    "         .when(col(\"aggregated\") == col(\"rule_descent\"), \"DE\")\n",
    "         .when(col(\"aggregated\") == col(\"rule_cruise\"), \"CR\")\n",
    "         .when(col(\"aggregated\") == col(\"rule_level\"), \"LVL\")\n",
    "    )\n",
    "\n",
    "    df = df.withColumnRenamed(\"alt\", \"altitude_ft\") \\\n",
    "           .withColumnRenamed(\"roc\", \"roc_ft_min\") \\\n",
    "           .withColumnRenamed(\"spd\", \"speed_kt\")\n",
    "\n",
    "    df = df.select(\n",
    "        \"track_id\", \"lat\", \"lon\", \"event_time\", \"cumulative_distance_nm\",\n",
    "        \"cumulative_time_s\", \"altitude_ft\", \"roc_ft_min\", \"speed_kt\", \"flight_phase\"\n",
    "    )\n",
    "\n",
    "    # Cache the DataFrame before applying window functions for phase detection\n",
    "    df.cache()\n",
    "\n",
    "    # Window specification for detecting phase changes and for TOC/TOD\n",
    "    window_spec_phase = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "    window_spec_cumulative = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing)\n",
    "\n",
    "    # Detect phase changes\n",
    "    df_with_phases = df.withColumn(\"prev_phase\", lag(\"flight_phase\", 1, \"None\").over(window_spec_phase))\n",
    "    df_with_phases = df_with_phases.withColumn(\"next_phase\", lead(\"flight_phase\", 1, \"None\").over(window_spec_phase))\n",
    "\n",
    "    # Cache the DataFrame after phase detection\n",
    "    df_with_phases.cache()\n",
    "\n",
    "    # Identify TOC and TOD for 'CR' phases\n",
    "    df_with_phases = df_with_phases.withColumn(\"first_cr_time\", min(when(col(\"flight_phase\") == \"CR\", col(\"event_time\"))).over(window_spec_cumulative))\n",
    "    df_with_phases = df_with_phases.withColumn(\"last_cr_time\", max(when(col(\"flight_phase\") == \"CR\", col(\"event_time\"))).over(window_spec_cumulative))\n",
    "\n",
    "    # Create event types\n",
    "    start_of_segment = (col(\"flight_phase\").isin(\"CR\", \"LVL\")) & (col(\"prev_phase\") != col(\"flight_phase\"))\n",
    "    df_with_phases = df_with_phases.withColumn(\"start_of_segment\", start_of_segment)\n",
    "\n",
    "    df_with_phases = df_with_phases.withColumn(\"segment_count\", F.sum(when(col(\"start_of_segment\"), 1).otherwise(0)).over(window_spec_cumulative))\n",
    "\n",
    "    df_with_phases = df_with_phases.withColumn(\n",
    "        \"milestone_types\",\n",
    "        F.when(col(\"event_time\") == col(\"first_cr_time\"), F.array(F.lit(\"level-start\"), F.lit(\"top-of-climb\")))\n",
    "         .when(col(\"event_time\") == col(\"last_cr_time\"), F.array(F.lit(\"level-end\"), F.lit(\"top-of-descent\")))\n",
    "         .when(start_of_segment, F.array(F.lit(\"level-start\")))\n",
    "         .when((col(\"flight_phase\").isin(\"CR\", \"LVL\")) & (col(\"next_phase\") != col(\"flight_phase\")), F.array(F.lit('level-end')))\n",
    "         .when((col(\"prev_phase\") == \"GND\") & (col(\"next_phase\") == \"CL\"), F.array(F.lit(\"take-off\")))\n",
    "         .when((col(\"prev_phase\") == \"DE\") & (col(\"flight_phase\") == \"GND\"), F.array(F.lit(\"landing\")))\n",
    "         .otherwise(F.array())\n",
    "    )\n",
    "\n",
    "    # Explode the event_types array to create rows for each event\n",
    "    df_exploded = df_with_phases.select(\"*\", explode(col(\"milestone_types\")).alias(\"type\"))\n",
    "\n",
    "    # Filter out rows with no relevant events\n",
    "    df_exploded = df_exploded.filter(\"type IS NOT NULL\")\n",
    "\n",
    "    # Reformat\n",
    "    df_openap_events = df_exploded.select(\n",
    "        col('track_id'),\n",
    "        col('type'),\n",
    "        col('event_time'),\n",
    "        col('lon'),\n",
    "        col('lat'),\n",
    "        col('altitude_ft'),\n",
    "        col('cumulative_distance_nm'),\n",
    "        col('cumulative_time_s')\n",
    "    )\n",
    "\n",
    "    df_openap_events = df_openap_events.dropDuplicates(['track_id', 'type', 'event_time'])\n",
    "    \n",
    "    df.cache()\n",
    "    \n",
    "    return df_openap_events\n",
    "\n",
    "\n",
    "\n",
    "def calculate_horizontal_segment_events_old(sdf_input):\n",
    "    df = sdf_input\n",
    "    \n",
    "    # Extracting OpenAP phases\n",
    "    ## Defining variables\n",
    "    df = df.withColumn(\"altitude_ft\", col(\"baro_altitude\") * 3.28084)\n",
    "    df = df.withColumn(\"roc_ft_min\", col(\"vert_rate\") * 196.850394)\n",
    "    df = df.withColumn(\"speed_kt\", col(\"velocity\") * 1.94384)\n",
    "\n",
    "    ## Define schema for UDF\n",
    "    df = df.select(\"track_id\", \"lat\", \"lon\", \"event_time\", \"altitude_ft\", \"roc_ft_min\", \"speed_kt\")\n",
    "\n",
    "    ## Define schema for UDF\n",
    "    schema = StructType([\n",
    "        StructField(\"track_id\", StringType()),\n",
    "        #StructField(\"icao24\", StringType()),\n",
    "        #StructField(\"callsign\", StringType()),\n",
    "        StructField(\"lat\", DoubleType()),\n",
    "        StructField(\"lon\", DoubleType()),\n",
    "        StructField(\"event_time\", IntegerType()),\n",
    "        #StructField(\"baro_altitude\", DoubleType()),\n",
    "        #StructField(\"velocity\", DoubleType()),\n",
    "        #StructField(\"vert_rate\", DoubleType()),\n",
    "        #StructField(\"cumulative_distance_nm\", DoubleType()),\n",
    "        #StructField(\"cumulative_time_s\", IntegerType()),\n",
    "        StructField(\"altitude_ft\", DoubleType()),\n",
    "        StructField(\"roc_ft_min\", DoubleType()),\n",
    "        StructField(\"speed_kt\", DoubleType()),\n",
    "        StructField(\"flight_phase\", StringType())\n",
    "    ])\n",
    "\n",
    "\n",
    "    @pandas_udf(schema, PandasUDFType.GROUPED_MAP)\n",
    "    def get_flight_phases(pdf):\n",
    "        try:\n",
    "            fp = FlightPhase()\n",
    "            fp.set_trajectory(pdf['event_time'], pdf['altitude_ft'], pdf['speed_kt'], pdf['roc_ft_min'])\n",
    "            pdf['flight_phase'] = fp.phaselabel()\n",
    "        except: \n",
    "            pdf['flight_phase'] = None\n",
    "        \n",
    "        return pdf\n",
    "\n",
    "    ## Apply UDF to get flight phases\n",
    "    df_with_phases = df.groupby(\"track_id\").apply(get_flight_phases)\n",
    "\n",
    "    ## Window specification for detecting phase changes and for TOC/TOD\n",
    "    windowSpecPhase = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "    windowSpecTOC = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "    windowSpecTOD = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rowsBetween(0, Window.unboundedFollowing)\n",
    "\n",
    "    ## Detect phase changes\n",
    "    df_with_phases = df_with_phases.withColumn(\"prev_phase\", lag(\"flight_phase\", 1, \"None\").over(windowSpecPhase))\n",
    "    df_with_phases = df_with_phases.withColumn(\"next_phase\", lead(\"flight_phase\", 1, \"None\").over(windowSpecPhase))\n",
    "\n",
    "    ## Identify TOC and TOD for 'CR' phases\n",
    "    df_with_phases = df_with_phases.withColumn(\"first_cr_time\", min(when(col(\"flight_phase\") == \"CR\", col(\"event_time\"))).over(windowSpecTOC))\n",
    "    df_with_phases = df_with_phases.withColumn(\"last_cr_time\", max(when(col(\"flight_phase\") == \"CR\", col(\"event_time\"))).over(windowSpecTOD))\n",
    "\n",
    "    ## Create event types\n",
    "\n",
    "    ### Define a window spec for a cumulative sum, partitioned by track_id\n",
    "    windowSpecCumulative = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    ### Column to indicate the start of a level segment\n",
    "    start_of_segment = (F.col(\"flight_phase\").isin(\"CR\", \"LVL\")) & (F.col(\"prev_phase\") != F.col(\"flight_phase\"))\n",
    "    df_with_phases = df_with_phases.withColumn(\"start_of_segment\", start_of_segment)\n",
    "\n",
    "    ### Cumulative count of level segment starts\n",
    "    df_with_phases = df_with_phases.withColumn(\"segment_count\", F.sum(F.when(F.col(\"start_of_segment\"), 1).otherwise(0)).over(windowSpecCumulative))\n",
    "\n",
    "    ### Update milestone_types to include segment number\n",
    "    df_with_phases = df_with_phases.withColumn(\n",
    "        \"milestone_types\",\n",
    "        F.when(F.col(\"event_time\") == F.col(\"first_cr_time\"), F.array(F.lit(\"level-start\"), F.lit(\"top-of-climb\")))\n",
    "         .when(F.col(\"event_time\") == F.col(\"last_cr_time\"), F.array(F.lit('level-end'), F.lit(\"top-of-descent\")))\n",
    "         .when(start_of_segment, F.array(F.lit(\"level-start\")))\n",
    "         .when((F.col(\"flight_phase\").isin(\"CR\", \"LVL\")) & (F.col(\"next_phase\") != F.col(\"flight_phase\")), F.array(F.lit('level-end')))\n",
    "         .when((F.col(\"prev_phase\") == \"GND\") & (F.col(\"next_phase\") == \"CL\"), F.array(F.lit(\"take-off\")))\n",
    "         .when((F.col(\"prev_phase\") == \"DE\") & (F.col(\"flight_phase\") == \"GND\"), F.array(F.lit(\"landing\")))\n",
    "         .otherwise(F.array())\n",
    "    )\n",
    "\n",
    "    ## Explode the event_types array to create rows for each event\n",
    "    df_exploded = df_with_phases.select(\"*\", explode(col(\"milestone_types\")).alias(\"type\"))\n",
    "\n",
    "    ## Filter out rows with no relevant events\n",
    "    df_exploded = df_exploded.filter(\"type IS NOT NULL\")\n",
    "\n",
    "    ## Reformat\n",
    "    df_openap_events = df_exploded.select(\n",
    "        col('track_id'),\n",
    "        col('type'),\n",
    "        col('event_time'),\n",
    "        col('lon'),\n",
    "        col('lat'),\n",
    "        col('altitude_ft'),\n",
    "        col('cumulative_distance_nm'),\n",
    "        col('cumulative_time_s')\n",
    "    )\n",
    "\n",
    "    df_openap_events = df_openap_events.dropDuplicates(['track_id', 'type', 'event_time'])\n",
    "    \n",
    "    return df_openap_events\n",
    "\n",
    "# Flight levels crossing extractions\n",
    "def calculate_vertical_crossing_events(sdf_input):\n",
    "    \"\"\"\n",
    "    Calculate vertical crossing events for flight data.\n",
    "    \n",
    "    Args:\n",
    "        sdf_input (DataFrame): Input Spark DataFrame containing flight data.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with calculated vertical crossing events.\n",
    "    \"\"\"\n",
    "    df = sdf_input.select(\n",
    "        \"track_id\", \"lat\", \"lon\", \"event_time\", \"baro_altitude\",\n",
    "        \"vert_rate\", \"velocity\", \"cumulative_distance_nm\", \"cumulative_time_s\"\n",
    "    )\n",
    "\n",
    "    # Convert meters to flight level and create a new column\n",
    "    df = df.withColumn(\"altitude_ft\", col(\"baro_altitude\") * 3.28084)\n",
    "    df = df.withColumn(\"FL\", (col(\"baro_altitude\") * 3.28084 / 100).cast(DoubleType()))\n",
    "\n",
    "    # Window specification to order by event_time for each track_id\n",
    "    window_spec = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "\n",
    "    # Add columns with lead FL values\n",
    "    df = df.withColumn(\"next_FL\", F.lead(\"FL\").over(window_spec)).cache()\n",
    "\n",
    "    # Define the conditions for a crossing for each flight level of interest\n",
    "    crossing_conditions = [\n",
    "        (col(\"FL\") < 50) & (col(\"next_FL\") >= 50),  # going up\n",
    "        (col(\"FL\") >= 50) & (col(\"next_FL\") < 50),  # going down\n",
    "        (col(\"FL\") < 70) & (col(\"next_FL\") >= 70),  # going up\n",
    "        (col(\"FL\") >= 70) & (col(\"next_FL\") < 70),  # going down\n",
    "        (col(\"FL\") < 100) & (col(\"next_FL\") >= 100),  # going up\n",
    "        (col(\"FL\") >= 100) & (col(\"next_FL\") < 100),  # going down\n",
    "        (col(\"FL\") < 245) & (col(\"next_FL\") >= 245),  # going up\n",
    "        (col(\"FL\") >= 245) & (col(\"next_FL\") < 245),  # going down\n",
    "    ]\n",
    "\n",
    "    # Create a crossing column using multiple when conditions\n",
    "    crossing_column = F.when(crossing_conditions[0], 50)\n",
    "    for condition, flight_level in zip(crossing_conditions[1:], [50, 70, 70, 100, 100, 245, 245]):\n",
    "        crossing_column = crossing_column.when(condition, flight_level)\n",
    "\n",
    "    df = df.withColumn(\"crossing\", crossing_column)\n",
    "\n",
    "    # Filter out only the rows where a crossing was detected\n",
    "    crossing_points = df.filter(col(\"crossing\").isNotNull())\n",
    "\n",
    "    # Filter out only the rows where the absolute difference between crossing and FL is less than 10\n",
    "    crossing_points = crossing_points.filter(F.abs(col(\"crossing\") - col(\"FL\")) < 10).cache()\n",
    "\n",
    "    # Identify first and last crossings by using window functions again\n",
    "    # Note that we need a separate window specification for descending order\n",
    "    window_spec_crossing = Window.partitionBy(\"track_id\", \"crossing\").orderBy(\"event_time\")\n",
    "    window_spec_crossing_desc = Window.partitionBy(\"track_id\", \"crossing\").orderBy(col(\"event_time\").desc())\n",
    "\n",
    "    crossing_points = crossing_points.withColumn(\"row_number_asc\", F.row_number().over(window_spec_crossing))\n",
    "    crossing_points = crossing_points.withColumn(\"row_number_desc\", F.row_number().over(window_spec_crossing_desc))\n",
    "\n",
    "    # Cache the DataFrame after applying window functions for row numbers\n",
    "    crossing_points.cache()\n",
    "\n",
    "    # Filter first and last crossings\n",
    "    first_crossings = crossing_points.filter(col(\"row_number_asc\") == 1)\n",
    "    last_crossings = crossing_points.filter(col(\"row_number_desc\") == 1)\n",
    "\n",
    "    # Drop the temporary columns used for row numbering\n",
    "    first_crossings = first_crossings.drop(\"row_number_asc\", \"row_number_desc\")\n",
    "    last_crossings = last_crossings.drop(\"row_number_asc\", \"row_number_desc\")\n",
    "\n",
    "    # Add a label to indicate first or last crossing\n",
    "    first_crossings = first_crossings.withColumn(\"type\", F.concat(F.lit(\"first-xing-fl\"), col('crossing').cast(\"string\")))\n",
    "    last_crossings = last_crossings.withColumn(\"type\", F.concat(F.lit(\"last-xing-fl\"), col('crossing').cast(\"string\")))\n",
    "\n",
    "    # Combine the first and last crossing data\n",
    "    all_crossings = first_crossings.union(last_crossings)\n",
    "\n",
    "    # Clean up\n",
    "    df_lvl_events = all_crossings.select(\n",
    "        col('track_id'),\n",
    "        col('type'),\n",
    "        col('event_time'),\n",
    "        col('lon'),\n",
    "        col('lat'),\n",
    "        col('altitude_ft'),\n",
    "        col('cumulative_distance_nm'),\n",
    "        col('cumulative_time_s')\n",
    "    )\n",
    "\n",
    "    df_lvl_events = df_lvl_events.dropDuplicates([\n",
    "        'track_id', 'type', 'event_time', 'lon', 'lat', 'altitude_ft', 'cumulative_distance_nm', 'cumulative_time_s'\n",
    "    ])\n",
    "\n",
    "    return df_lvl_events\n",
    "\n",
    "def calculate_firstseen_lastseen_events(sdf_input):\n",
    "    \"\"\"\n",
    "    Calculate the first seen and last seen events for each track_id.\n",
    "\n",
    "    Args:\n",
    "        sdf_input (DataFrame): Input Spark DataFrame containing flight data.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with calculated first seen and last seen events.\n",
    "    \"\"\"\n",
    "    # Select necessary columns\n",
    "    df = sdf_input.select(\n",
    "        \"track_id\", \"lat\", \"lon\", \"event_time\", \"baro_altitude\",\n",
    "        \"vert_rate\", \"velocity\", \"cumulative_distance_nm\", \"cumulative_time_s\"\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\"altitude_ft\", col(\"baro_altitude\") * 3.28084)\n",
    "    \n",
    "    # Window specification to order by event_time for each track_id\n",
    "    window_spec = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "    window_spec_desc = Window.partitionBy(\"track_id\").orderBy(F.col(\"event_time\").desc())\n",
    "\n",
    "    # Add row numbers for ascending and descending order of event_time\n",
    "    df = df.withColumn(\"row_number_asc\", F.row_number().over(window_spec))\n",
    "    df = df.withColumn(\"row_number_desc\", F.row_number().over(window_spec_desc))\n",
    "\n",
    "    # Filter first and last seen events\n",
    "    first_seen = df.filter(F.col(\"row_number_asc\") == 1).drop(\"row_number_asc\", \"row_number_desc\")\n",
    "    last_seen = df.filter(F.col(\"row_number_desc\") == 1).drop(\"row_number_asc\", \"row_number_desc\")\n",
    "\n",
    "    # Add a label to indicate first or last seen event\n",
    "    first_seen = first_seen.withColumn(\"type\", F.lit(\"first_seen\"))\n",
    "    last_seen = last_seen.withColumn(\"type\", F.lit(\"last_seen\"))\n",
    "\n",
    "    # Combine the first and last seen data\n",
    "    all_seen_events = first_seen.union(last_seen)\n",
    "\n",
    "    # Clean up the DataFrame\n",
    "    df_seen_events = all_seen_events.select(\n",
    "        \"track_id\",\n",
    "        \"type\",\n",
    "        \"event_time\",\n",
    "        \"lon\",\n",
    "        \"lat\",\n",
    "        \"altitude_ft\",\n",
    "        \"cumulative_distance_nm\",\n",
    "        \"cumulative_time_s\"\n",
    "    )\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_seen_events = df_seen_events.dropDuplicates([\n",
    "        \"track_id\", \"type\", \"event_time\", \"lon\", \"lat\", \"altitude_ft\", \"cumulative_distance_nm\", \"cumulative_time_s\"\n",
    "    ])\n",
    "\n",
    "    return df_seen_events\n",
    "\n",
    "# Measurement helper functions\n",
    "\n",
    "def add_time_measure(sdf_input):\n",
    "    \"\"\"\n",
    "    Add a cumulative time measure to the input Spark DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        sdf_input (DataFrame): Input Spark DataFrame containing flight data.\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame with added cumulative time measure.\n",
    "    \"\"\"\n",
    "    # Define the window spec partitioned by 'track_id'\n",
    "    window_spec = Window.partitionBy('track_id').orderBy('event_time')\n",
    "\n",
    "    # Add a new column 'cumulative_time_s' based on the minimum event time per track_id\n",
    "    sdf_input = sdf_input.withColumn('min_event_time', F.min('event_time').over(window_spec))\n",
    "\n",
    "    # Calculate the cumulative time in seconds\n",
    "    sdf_input = sdf_input.withColumn(\n",
    "        'cumulative_time_s',\n",
    "        (F.col('event_time').cast(\"long\") - F.col('min_event_time').cast(\"long\"))\n",
    "    )\n",
    "\n",
    "    # Drop the 'min_event_time' as it is no longer needed\n",
    "    sdf_input = sdf_input.drop('min_event_time')\n",
    "\n",
    "    return sdf_input\n",
    "\n",
    "\n",
    "# Distance helper function\n",
    "def add_distance_measure(df):\n",
    "    \"\"\"\n",
    "    Calculate the great circle distance between consecutive points in nautical miles\n",
    "    and the cumulative distance for each track, using native PySpark functions.\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input Spark DataFrame. Assumes columns \"lat\", \"lon\", \"track_id\", \"event_time\".\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with additional columns \"distance_nm\" and \"cumulative_distance_nm\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a window spec for lag operation and cumulative sum\n",
    "    windowSpecLag = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "    windowSpecCumSum = Window.partitionBy(\"track_id\").orderBy(\"event_time\").rowsBetween(Window.unboundedPreceding, 0)\n",
    "\n",
    "    # Convert degrees to radians using PySpark native function\n",
    "    df = df.withColumn(\"lat_rad\", F.radians(F.col(\"lat\")))\n",
    "    df = df.withColumn(\"lon_rad\", F.radians(F.col(\"lon\")))\n",
    "\n",
    "    # Getting previous row's latitude and longitude\n",
    "    df = df.withColumn(\"prev_lat_rad\", F.lag(\"lat_rad\").over(windowSpecLag))\n",
    "    df = df.withColumn(\"prev_lon_rad\", F.lag(\"lon_rad\").over(windowSpecLag))\n",
    "\n",
    "    # Compute the great circle distance using haversine formula in PySpark native functions\n",
    "    df = df.withColumn(\"a\", \n",
    "                       F.sin((F.col(\"lat_rad\") - F.col(\"prev_lat_rad\")) / 2)**2 + \n",
    "                       F.cos(F.col(\"prev_lat_rad\")) * F.cos(F.col(\"lat_rad\")) * \n",
    "                       F.sin((F.col(\"lon_rad\") - F.col(\"prev_lon_rad\")) / 2)**2)\n",
    "\n",
    "    df = df.withColumn(\"c\", 2 * F.atan2(F.sqrt(F.col(\"a\")), F.sqrt(1 - F.col(\"a\"))))\n",
    "\n",
    "    # Radius of Earth in kilometers is 6371\n",
    "    df = df.withColumn(\"distance_km\", 6371 * F.col(\"c\"))\n",
    "\n",
    "    # Convert distance to nautical miles; 1 nautical mile = 1 / 1.852 km\n",
    "    df = df.withColumn(\"segment_distance_nm\", \n",
    "                       F.when(F.col(\"distance_km\").isNull(), 0).otherwise(F.col(\"distance_km\") / 1.852))\n",
    "\n",
    "    # Calculate the cumulative distance\n",
    "    df = df.withColumn(\"cumulative_distance_nm\", F.sum(\"segment_distance_nm\").over(windowSpecCumSum))\n",
    "\n",
    "    # Drop temporary columns used for calculations\n",
    "    df = df.drop(\"lat_rad\", \"lon_rad\", \"prev_lat_rad\", \"prev_lon_rad\", \"a\", \"c\", \"distance_km\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Final ETL - Combining the datasets and writing away.. \n",
    "\n",
    "def etl_flight_events_and_measures(\n",
    "        sdf_input, \n",
    "        batch_id, \n",
    "        calc_vertical = True, \n",
    "        calc_horizontal = True,\n",
    "        calc_hexaero = True, \n",
    "        calc_seen = True):\n",
    "    \n",
    "    # Smooth baro_alt              \n",
    "    sdf_input = smooth_baro_alt(sdf_input)\n",
    "    \n",
    "    # Add measures\n",
    "    sdf_input = add_distance_measure(sdf_input)\n",
    "    sdf_input = add_time_measure(sdf_input)\n",
    "    \n",
    "    # Cache\n",
    "    sdf_input.cache()\n",
    "    \n",
    "    if calc_horizontal:\n",
    "        print(f\"Calculating horizontal events (phase) for batch_id: {batch_id}\")\n",
    "        df_events = calculate_horizontal_segment_events(sdf_input)\n",
    "    else:\n",
    "        df_events = None\n",
    "        \n",
    "    if calc_vertical:\n",
    "        print(f\"Calculating vertical events (FL crossings) for batch_id: {batch_id}\")\n",
    "        df_vertical_events = calculate_vertical_crossing_events(sdf_input)\n",
    "        \n",
    "        if pd.isnull(df_events):\n",
    "            df_events = df_vertical_events\n",
    "        else: \n",
    "            df_events = df_events.union(df_vertical_events)\n",
    "    \n",
    "    if calc_vertical:\n",
    "        print(f\"Calculating hexaero events (airport events) for batch_id: {batch_id}\")\n",
    "        df_hexaero_events = calculate_hexaero_events(sdf_input)\n",
    "        \n",
    "        if pd.isnull(df_events):\n",
    "            df_events = df_hexaero_events\n",
    "        else: \n",
    "            df_events = df_events.union(df_hexaero_events)\n",
    "    \n",
    "    if calc_seen:\n",
    "        print(f\"Calculating first_seen/last_seen events for batch_id: {batch_id}\")\n",
    "        df_seen_events = calculate_firstseen_lastseen_events(sdf_input)\n",
    "        \n",
    "        if pd.isnull(df_events):\n",
    "            df_events = df_seen_events\n",
    "        else: \n",
    "            df_events = df_events.union(df_seen_events)\n",
    "    \n",
    "#    # Calculate flight events \n",
    "#    if (calc_vertical and calc_horizontal): \n",
    "#      print(f\"Calculating both vertical and horizontal events for batch_id: {batch_id}\")\n",
    "#      df_horizontal_events = calculate_horizontal_segment_events(sdf_input)\n",
    "#      df_vertical_events = calculate_vertical_crossing_events(sdf_input)\n",
    "#    \n",
    "#      # Formatting\n",
    "#      df_events = df_horizontal_events.union(df_vertical_events)\n",
    "#    \n",
    "#    if (calc_vertical and not calc_horizontal):\n",
    "#      print(f\"Calculating vertical events for batch_id: {batch_id}\")\n",
    "#      df_vertical_events = calculate_vertical_crossing_events(sdf_input)\n",
    "#    \n",
    "#      # Formatting\n",
    "#      df_events = df_vertical_events\n",
    "#      \n",
    "#    if (not calc_vertical and calc_horizontal): \n",
    "#      print(f\"Calculating horizontal events for batch_id: {batch_id}\")\n",
    "#      df_horizontal_events = calculate_horizontal_segment_events(sdf_input)\n",
    "#      \n",
    "#      # Formatting\n",
    "#      df_events = df_horizontal_events\n",
    "    \n",
    "    df_events.cache()\n",
    "    df_events = df_events.withColumn('source', F.lit('OSN'))\n",
    "    df_events = df_events.withColumn('version', F.lit('events_v0.0.2'))\n",
    "    df_events = df_events.withColumn('info', F.lit(''))\n",
    "\n",
    "    df_events = df_events.withColumn(\"id_tmp\", F.concat(F.lit(batch_id),monotonically_increasing_id().cast('string'))).select(\n",
    "        col('id_tmp'),\n",
    "        col('track_id'),\n",
    "        col('type'),\n",
    "        col('event_time'),\n",
    "        col('lon'),\n",
    "        col('lat'),\n",
    "        col('altitude_ft'),\n",
    "        col('source'),\n",
    "        col('version'),\n",
    "        col('info'),\n",
    "        col('cumulative_distance_nm'),\n",
    "        col('cumulative_time_s')\n",
    "    )\n",
    "\n",
    "    # Milestones table\n",
    "    df_milestones = df_events.select(\n",
    "        col('id_tmp').alias('id'),\n",
    "        col('track_id').alias('track_id'),\n",
    "        col('type').alias('type'),\n",
    "        col('event_time').alias('event_time'),\n",
    "        col('lon').alias('longitude'),\n",
    "        col('lat').alias('latitude'),\n",
    "        col('altitude_ft').alias('altitude'),\n",
    "        col('source').alias('source'),\n",
    "        col('version').alias('version'),\n",
    "        col('info').alias('info'),\n",
    "    )\n",
    "    #df_milestones.write.mode(\"append\").insertInto(f\"`{project}`.`opdi_flight_events`\")\n",
    "    df_milestones.toPandas().to_parquet('events_test.parquet')\n",
    "    \n",
    "    # Measurements table \n",
    "    ## Add Distance flown (NM) - df measure\n",
    "    df_measurements_df = df_events.withColumn('type',F.lit('Distance flown (NM)'))\n",
    "    df_measurements_df = df_measurements_df.withColumn('version',F.lit('distance_v0.0.2'))\n",
    "    df_measurements_df = df_measurements_df.withColumn(\"id\", F.concat(F.lit(batch_id + \"_df_\"),monotonically_increasing_id().cast('string'))).select(\n",
    "        col('id'),\n",
    "        col('id_tmp').alias('milestone_id'),\n",
    "        col('type'),\n",
    "        col('cumulative_distance_nm').alias('value'),\n",
    "        col('version')\n",
    "    )\n",
    "\n",
    "    ## Add Time Passed - df measure\n",
    "    df_measurements_tp = df_events.withColumn('type',F.lit('Time Passed (s)'))\n",
    "    df_measurements_tp = df_measurements_tp.withColumn('version',F.lit('time_v0.0.1'))\n",
    "    df_measurements_tp = df_measurements_tp.withColumn(\"id\", F.concat(F.lit(batch_id + \"_tp_\"),monotonically_increasing_id().cast('string'))).select(\n",
    "        col('id'),\n",
    "        col('id_tmp').alias('milestone_id'),\n",
    "        col('type'),\n",
    "        col('cumulative_time_s').alias('value'),\n",
    "        col('version')\n",
    "    )\n",
    "    \n",
    "    # Merge\n",
    "    df_measurements = df_measurements_df.union(df_measurements_tp)\n",
    "    #df_measurements.write.mode(\"append\").insertInto(f\"`{project}`.`opdi_measurements`\")\n",
    "    df_measurements.toPandas().to_parquet('measurements_test.parquet')\n",
    "    \n",
    "    #df_measurements_tp.write.mode(\"append\").insertInto(f\"`{project}`.`opdi_measurements`\")\n",
    "    return None\n",
    "    \n",
    "# Run code.. \n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_previous_month(dt):\n",
    "    \"\"\"\n",
    "    Get the first day of the previous month.\n",
    "\n",
    "    :param dt: The current date.\n",
    "    :return: A datetime object representing the first day of the previous month.\n",
    "    \"\"\"\n",
    "    year, month = dt.year - (dt.month == 1), dt.month - 1 if dt.month > 1 else 12\n",
    "    return datetime(year, month, 1)\n",
    "\n",
    "def get_previous_day(date):\n",
    "    \"\"\"\n",
    "    Returns the previous day's date for the given date.\n",
    "\n",
    "    :param date: The current date.\n",
    "    :return: Date object representing the previous day.\n",
    "    \"\"\"\n",
    "    return date - timedelta(days=1)\n",
    "\n",
    "print('-'*30)\n",
    "print(f'Starting milestone extraction process for timeframe {start_date} until {end_date}...')\n",
    "\n",
    "## Load logs\n",
    "#fpath = 'logs/04_osn-flight_events-etl-day-by-day-log.parquet'\n",
    "#if os.path.isfile(fpath):\n",
    "#    processed_days = pd.read_parquet(fpath).days.to_list()\n",
    "#else:\n",
    "#    processed_days = []\n",
    "\n",
    "# Loop through time range in daily batches, moving backwards\n",
    "current_date = end_date\n",
    "#while current_date >= start_date:        \n",
    "#    previous_day_date = get_previous_day(current_date)\n",
    "#    previous_day_timestamp = int(previous_day_date.timestamp())\n",
    "#    \n",
    "#    print(f'Currently processing {previous_day_date} - {current_date}')\n",
    "#    if current_date in processed_days:\n",
    "#        print('Already processed')\n",
    "#        current_date = previous_day_date\n",
    "#        continue\n",
    "#    \n",
    "#    # Perform the ETL operation for the current day\n",
    "#    sdf_input = spark.sql(f\"\"\"\n",
    "#        SELECT track_id, icao24, callsign, lat, lon, event_time, baro_altitude, velocity, vert_rate, cumulative_distance_nm\n",
    "#        FROM `{project}`.`osn_tracks_clustered`\n",
    "#        WHERE track_id IN (\n",
    "#            SELECT track_id\n",
    "#            FROM `{project}`.`osn_tracks_clustered`\n",
    "#            GROUP BY track_id\n",
    "#            HAVING MIN(event_time) BETWEEN {previous_day_timestamp} AND {int(current_date.timestamp())}\n",
    "#        )\"\"\").cache()\n",
    "#\n",
    "#    etl_flight_events_and_measures(sdf_input, batch_id=previous_day_date.strftime(\"%Y%m%d\") + '_')\n",
    "#    \n",
    "#    processed_days.append(current_date)\n",
    "#    processed_days_df = pd.DataFrame({'days':processed_days})\n",
    "#    processed_days_df.to_parquet(fpath)\n",
    "#    \n",
    "#    # Move to the previous day\n",
    "#    current_date = previous_day_date\n",
    "\n",
    "# Custom functions\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "\n",
    "def generate_months(start_date, end_date):\n",
    "    \"\"\"Generate a list of dates corresponding to the first day of each month between two dates.\n",
    "\n",
    "    Args:\n",
    "    start_date (datetime.date): The starting date.\n",
    "    end_date (datetime.date): The ending date.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of date objects for the first day of each month within the specified range.\n",
    "    \"\"\"\n",
    "    current = start_date\n",
    "    months = []\n",
    "    while current <= end_date:\n",
    "        months.append(current)\n",
    "        # Increment month\n",
    "        month = current.month\n",
    "        year = current.year\n",
    "        if month == 12:\n",
    "            current = date(year + 1, 1, 1)\n",
    "        else:\n",
    "            current = date(year, month + 1, 1)\n",
    "    return months\n",
    "\n",
    "def get_start_end_of_month(date):\n",
    "    \"\"\"Return a datetime object for the first and last second  of the given month and year.\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    first_second = datetime(year, month, 1, 0, 0, 0)\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    last_second = datetime(year, month, last_day, 23, 59, 59)\n",
    "    return first_second.timestamp(), last_second.timestamp()\n",
    "\n",
    "\n",
    "def get_data_within_timeframe(spark, table_name, month, time_col = 'event_time', unix_time = True):\n",
    "    \"\"\"\n",
    "    Retrieves records from a specified Spark table within the given timeframe.\n",
    "\n",
    "    Args:\n",
    "    spark (SparkSession): The SparkSession object.\n",
    "    table_name (str): The name of the Spark table to query.\n",
    "    month (str): The start date of a month in datetime format.\n",
    "\n",
    "    Returns:\n",
    "    pyspark.sql.dataframe.DataFrame: A DataFrame containing the records within the specified timeframe.\n",
    "    \"\"\"\n",
    "    # Convert dates to POSIX time (seconds since epoch)\n",
    "    start_posix,stop_posix = get_start_end_of_month(month)\n",
    "\n",
    "    # Load the table\n",
    "    df = spark.read.table(table_name).select(\n",
    "        \"track_id\",\n",
    "        \"lat\",\n",
    "        \"lon\",\n",
    "        \"event_time\",\n",
    "        \"baro_altitude\",\n",
    "        \"velocity\",\n",
    "        \"vert_rate\"\n",
    "    )\n",
    "    \n",
    "    if unix_time:\n",
    "        # Filter records based on event_time posix column\n",
    "        df = df.filter((col(time_col) >= start_posix) & (col(time_col) < stop_posix))\n",
    "    else: \n",
    "        df = df.withColumn('unix_time', unix_timestamp(time_col))\n",
    "        df = df.filter((col('unix_time') >= start_posix) & ((col('unix_time') < stop_posix)))\n",
    "                                \n",
    "    return df\n",
    "  \n",
    "  \n",
    "# Actual processing\n",
    "start_month = date(2022, 1, 1)\n",
    "end_month = date(2024, 7, 1)\n",
    "\n",
    "to_process_months = generate_months(start_month, end_month)\n",
    "to_process_months.reverse()\n",
    "\n",
    "## Load logs Horizontal\n",
    "fpath_horizontal = 'logs/04_osn-flight_event-horizontal-etl-log.parquet'\n",
    "if os.path.isfile(fpath_horizontal):\n",
    "    processed_months_horizontal = pd.read_parquet(fpath_horizontal).months.to_list()\n",
    "else:\n",
    "    processed_months_horizontal = []\n",
    "\n",
    "## Load logs vertical\n",
    "fpath_vertical = 'logs/04_osn-flight_event-vertical-etl-log.parquet'\n",
    "if os.path.isfile(fpath_vertical):\n",
    "    processed_months_vertical = pd.read_parquet(fpath_vertical).months.to_list()\n",
    "else:\n",
    "    processed_months_vertical = []\n",
    "\n",
    "## Load hexaero airports\n",
    "fpath_hexaero = 'logs/04_osn-flight_event-hexaero_airport-log.parquet'\n",
    "if os.path.isfile(fpath_hexaero):\n",
    "    processed_months_hexaero = pd.read_parquet(fpath_hexaero).months.to_list()\n",
    "else:\n",
    "    processed_months_hexaero = []\n",
    "\n",
    "## Load first-seen / last-seen logs \n",
    "fpath_seen = 'logs/04_osn-flight_event-first_seen_last_seen-log.parquet'\n",
    "if os.path.isfile(fpath_seen):\n",
    "    processed_months_seen = pd.read_parquet(fpath_seen).months.to_list()\n",
    "else:\n",
    "    processed_months_seen = []\n",
    "\n",
    "# Process loops\n",
    "for month in to_process_months:\n",
    "    print(f'Processing flight events for month: {month}')\n",
    "    calc_horizontal = True\n",
    "    calc_vertical = True\n",
    "    calc_hexaero = False\n",
    "    calc_seen = True\n",
    "    \n",
    "    if month in processed_months_horizontal:\n",
    "        print('Month horizontal processed already (fuzzy labelling)')\n",
    "        calc_horizontal = False\n",
    "    \n",
    "    if month in processed_months_vertical:\n",
    "        print('Month vertical processed already (FLs)')\n",
    "        calc_vertical = False\n",
    "        \n",
    "    if month in processed_months_hexaero:\n",
    "        print('Month hexaero airports already processed')\n",
    "        calc_hexaero = False\n",
    "        \n",
    "    if month in processed_months_hexaero:\n",
    "        print('Month first-seen / last-seen already processed')\n",
    "        calc_seen = False\n",
    "        \n",
    "    if (not calc_horizontal and not calc_vertical and not calc_hexaero and not calc_seen):\n",
    "        print('All elements for this month are already processed..')\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        # Perform the ETL operation for the current day\n",
    "        sdf_input = get_data_within_timeframe( # State Vectors sv\n",
    "            spark = spark, \n",
    "            table_name = f'{project}.osn_tracks_clustered', \n",
    "            month = month).cache()\n",
    "\n",
    "        etl_flight_events_and_measures(sdf_input, \n",
    "                                       batch_id=month.strftime(\"%Y%m%d\") + '_', \n",
    "                                       calc_vertical = calc_vertical, \n",
    "                                       calc_horizontal = calc_horizontal,\n",
    "                                       calc_hexaero = calc_hexaero,\n",
    "                                       calc_seen = calc_seen\n",
    "                                      )\n",
    "        \n",
    "        ## Logging\n",
    "        if calc_horizontal:\n",
    "          processed_months_horizontal.append(month)\n",
    "          processed_df = pd.DataFrame({'months':processed_months_horizontal})\n",
    "          processed_df.to_parquet(fpath_horizontal)\n",
    "        if calc_vertical:\n",
    "          processed_months_vertical.append(month)\n",
    "          processed_df = pd.DataFrame({'months':processed_months_vertical})\n",
    "          processed_df.to_parquet(fpath_vertical)\n",
    "        if calc_hexaero:\n",
    "          processed_months_hexaero.append(month)\n",
    "          processed_df = pd.DataFrame({'months':processed_months_hexaero})\n",
    "          processed_df.to_parquet(fpath_hexaero)\n",
    "        if calc_vertical:\n",
    "          processed_months_seen.append(month)\n",
    "          processed_df = pd.DataFrame({'months':processed_months_seen})\n",
    "          processed_df.to_parquet(fpath_seen)\n",
    "          \n",
    "    spark.catalog.clearCache()\n",
    "# Cleanup\n",
    "# Uncomment the following line if you want to drop the table\n",
    "# spark.sql(f\"\"\"DROP TABLE IF EXISTS `{project}`.`osn_tracks`;\"\"\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f04cefe4-d88b-48aa-80d2-90b34b90d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "489af970-09e1-420f-9795-737252a4be66",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('events_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ffa3d809-20c7-4430-99da-a05c38005fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>track_id</th>\n",
       "      <th>type</th>\n",
       "      <th>event_time</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>altitude</th>\n",
       "      <th>source</th>\n",
       "      <th>version</th>\n",
       "      <th>info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20220201_10</td>\n",
       "      <td>637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d26...</td>\n",
       "      <td>first_seen</td>\n",
       "      <td>1644916760</td>\n",
       "      <td>-17.460022</td>\n",
       "      <td>65.814835</td>\n",
       "      <td>34000.001088</td>\n",
       "      <td>OSN</td>\n",
       "      <td>events_v0.0.2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113984</th>\n",
       "      <td>20220201_68719488388</td>\n",
       "      <td>637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d26...</td>\n",
       "      <td>last_seen</td>\n",
       "      <td>1644916860</td>\n",
       "      <td>-17.994690</td>\n",
       "      <td>65.857544</td>\n",
       "      <td>34000.001088</td>\n",
       "      <td>OSN</td>\n",
       "      <td>events_v0.0.2</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          id  \\\n",
       "10               20220201_10   \n",
       "113984  20220201_68719488388   \n",
       "\n",
       "                                                 track_id        type  \\\n",
       "10      637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d26...  first_seen   \n",
       "113984  637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d26...   last_seen   \n",
       "\n",
       "        event_time  longitude   latitude      altitude source        version  \\\n",
       "10      1644916760 -17.460022  65.814835  34000.001088    OSN  events_v0.0.2   \n",
       "113984  1644916860 -17.994690  65.857544  34000.001088    OSN  events_v0.0.2   \n",
       "\n",
       "       info  \n",
       "10           \n",
       "113984       "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.track_id == df.track_id.unique()[10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d2a5941-c05e-489f-9168-80e4dc6ce403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d264af787ca30916d6479_28_2022_2',\n",
       "       '637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d264af787ca30916d6479_28_2022_2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.track_id == df.track_id.unique()[10]].track_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531bf6f3-4f41-4672-86aa-5c7c652b206c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1644591425-1644591345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a40cb337-7dcf-45ea-ace5-4f6b0b37c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.read_parquet('/home/cdsw/data/OPDI/v002/flight_list/flight_list_v002_20220201_20220228.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fcc3f0e5-28b1-41b0-a61b-51ace5c75bee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>adep</th>\n",
       "      <th>ades</th>\n",
       "      <th>icao24</th>\n",
       "      <th>FLT_ID</th>\n",
       "      <th>first_seen</th>\n",
       "      <th>last_seen</th>\n",
       "      <th>DOF</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, adep, ades, icao24, FLT_ID, first_seen, last_seen, DOF, version]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp[tmp.id == '637d53c39fbc79b3ab1f24bfa42cf3192c6f2ee7768d264af787ca30916d6479_28_2022_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cd0d0-4b94-47f6-86ab-ad68c2635a59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63294adc-d7b7-491b-afa4-7a0fa2002ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wef = date(2022,6,1)\n",
    "til = date(2023,7,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "39c9f9dc-7214-4603-9284-79c11a6ffaeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2022, 6, 1),\n",
       " datetime.date(2022, 7, 1),\n",
       " datetime.date(2022, 8, 1),\n",
       " datetime.date(2022, 9, 1),\n",
       " datetime.date(2022, 10, 1),\n",
       " datetime.date(2022, 11, 1),\n",
       " datetime.date(2022, 12, 1),\n",
       " datetime.date(2023, 1, 1),\n",
       " datetime.date(2023, 2, 1),\n",
       " datetime.date(2023, 3, 1),\n",
       " datetime.date(2023, 4, 1),\n",
       " datetime.date(2023, 5, 1),\n",
       " datetime.date(2023, 6, 1),\n",
       " datetime.date(2023, 7, 1)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_months(wef,til)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9db41e-0bc1-4b58-937c-aa5c5f431c60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
