{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2f13147-3016-41c2-bc5a-7d5594fe05a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to quinten.goens\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, pandas_udf, col, PandasUDFType, lit, round, array_contains, from_unixtime\n",
    "from pyspark.sql.functions import col, radians, sin, cos, sqrt, atan2, array, collect_list, struct, row_number, expr\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, col\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField, IntegerType\n",
    "from pyspark.sql.functions import when, broadcast, split, col, concat_ws,  min, max, to_date, unix_timestamp\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col, expr, udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, TimestampType\n",
    "from pyspark.sql.functions import concat_ws, collect_list, array_sort\n",
    "\n",
    "# Regular imports\n",
    "from IPython.display import display, HTML\n",
    "import os, time\n",
    "import subprocess\n",
    "import os,shutil\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3pandas\n",
    "import h3\n",
    "import math\n",
    "\n",
    "# Custom functions\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "\n",
    "def generate_months(start_date, end_date):\n",
    "    \"\"\"Generate a list of dates corresponding to the first day of each month between two dates.\n",
    "\n",
    "    Args:\n",
    "    start_date (datetime.date): The starting date.\n",
    "    end_date (datetime.date): The ending date.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of date objects for the first day of each month within the specified range.\n",
    "    \"\"\"\n",
    "    current = start_date\n",
    "    months = []\n",
    "    while current <= end_date:\n",
    "        months.append(current)\n",
    "        # Increment month\n",
    "        month = current.month\n",
    "        year = current.year\n",
    "        if month == 12:\n",
    "            current = date(year + 1, 1, 1)\n",
    "        else:\n",
    "            current = date(year, month + 1, 1)\n",
    "    return months\n",
    "\n",
    "def get_start_end_of_month(date):\n",
    "    \"\"\"\n",
    "    Return the Unix timestamp for the first and last second of the given month and year.\n",
    "\n",
    "    Args:\n",
    "        date (datetime): A datetime object representing any date within the desired month.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the Unix timestamp of the first second and last second of the month.\n",
    "    \"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    # Calculate first and last second of the month\n",
    "    first_second = datetime(year, month, 1, 0, 0, 0)\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    last_second = datetime(year, month, last_day, 23, 59, 59)\n",
    "    \n",
    "    return first_second.timestamp(), last_second.timestamp()\n",
    "\n",
    "# Settings\n",
    "project = \"project_opdi\"\n",
    "resolution = 7\n",
    "\n",
    "start_month = date(2022, 1, 1)\n",
    "end_month = date(2025, 1, 1)\n",
    "\n",
    "# Getting today's date\n",
    "today = datetime.today().strftime('%d %B %Y')\n",
    "\n",
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OPDI Flight Table\") \\\n",
    "    .config(\"spark.rpc.message.maxSize\", 512) \\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3G\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"20\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "## Query testing sample function\n",
    "\n",
    "from pyspark.sql.functions import col, lit, from_unixtime, to_timestamp\n",
    "\n",
    "def get_data_within_timeframe(spark, table_name, month, time_col='event_time'):\n",
    "    \"\"\"\n",
    "    Retrieves records from a specified Spark table within the given timeframe.\n",
    "\n",
    "    Args:\n",
    "        spark (SparkSession): The SparkSession object.\n",
    "        table_name (str): The name of the Spark table to query.\n",
    "        month (str): The start date of a month in the format 'YYYY-MM-DD'.\n",
    "        time_col (str): The column name containing timestamp data (default: 'event_time').\n",
    "\n",
    "    Returns:\n",
    "        pyspark.sql.dataframe.DataFrame: A DataFrame containing the records within the specified timeframe.\n",
    "    \"\"\"\n",
    "    # Convert the start and end of the month to Unix timestamps\n",
    "    start_date, end_date = get_start_end_of_month(month)\n",
    "\n",
    "    # Convert Unix timestamps to Spark timestamp format\n",
    "    start_date_ts = to_timestamp(lit(start_date))\n",
    "    end_date_ts = to_timestamp(lit(end_date))\n",
    "\n",
    "    # Load the table\n",
    "    df = spark.table(table_name)\n",
    "\n",
    "    # Filter records based on the timestamp column\n",
    "    filtered_df = df.filter((col(time_col) >= start_date_ts) & (col(time_col) < end_date_ts))\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a098d5-2102-4811-9488-afa3e3707555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.16 ms, sys: 15.8 ms, total: 20 ms\n",
      "Wall time: 167 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Pull trajectories\n",
    "traj_sdf = get_data_within_timeframe(spark, table_name = 'project_opdi.osn_tracks', month= datetime(2025,1,1), time_col='event_time')\n",
    "#traj_sdf = traj_sdf.filter(col('track_id') == '000059f343f5301bc6aea34c66b51f445e511b319e6a0bc69961c37cadceeaab_0_2025_1')\n",
    "\n",
    "# Pull FIR\n",
    "fir_sdf = spark.table('project_opdi.opdi_h3_airspace_ref')\n",
    "offset = 3\n",
    "fir_sdf = fir_sdf.filter(\n",
    "    (col('airspace_type') == 'FIR') &\\\n",
    "    (col('airac_cfmu') == 524) &\\\n",
    "    (col('h3_res_7_lat') >= 26.74617 - offset) &\\\n",
    "    (col('h3_res_7_lat') <= 70.25976 + offset) &\\\n",
    "    (col('h3_res_7_lon') >= -25.86653 - offset) &\\\n",
    "    (col('h3_res_7_lon') <= 49.65699 + offset))\n",
    "fir_sdf = fir_sdf.dropDuplicates() \n",
    "fir_sdf = fir_sdf.select('code','h3_res_7', 'min_fl','max_fl')\n",
    "\n",
    "# Combine both\n",
    "#.withColumn('FL',col('baro_altitude_c') * 3.28084/100)\\\n",
    "traj_sdf = traj_sdf\\\n",
    "    .withColumn('FL',round(col('baro_altitude_c') * 3.28084/100))\\\n",
    "    .select('track_id', 'event_time', 'lat', 'lon','baro_altitude_c','h3_res_7','FL')\\\n",
    "    .join(fir_sdf, on='h3_res_7', how='left')\\\n",
    "    .filter((col('FL') >= col('min_fl')) & (col('FL') <= col('max_fl')))\n",
    "\n",
    "traj_sdf = traj_sdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe32f7ed-ca0c-4021-84e7-742d19fe1816",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_sdf = traj_sdf.groupBy(\"h3_res_7\", \"track_id\", 'event_time', 'lat', 'lon','baro_altitude_c', 'FL').agg(\n",
    "    concat_ws(\",\", array_sort(collect_list(\"code\"))).alias(\"code\"),\n",
    "    concat_ws(\",\", array_sort(collect_list(\"min_fl\"))).alias(\"min_fl\"),\n",
    "    concat_ws(\",\", array_sort(collect_list(\"max_fl\"))).alias(\"max_fl\")\n",
    ")\n",
    "\n",
    "# Define window specification for each track, ordered by event_time\n",
    "window_spec_minmax = Window.partitionBy(\"track_id\")\n",
    "window_spec = Window.partitionBy(\"track_id\").orderBy(\"event_time\")\n",
    "\n",
    "# Identify changes in 'code' over time within each track\n",
    "traj_sdf = traj_sdf\\\n",
    "    .withColumn(\n",
    "        \"begin_track\",\n",
    "        # This columns tags the beginning of the track with True\n",
    "        F.when(F.min(col('event_time')).over(window_spec_minmax) == col(\"event_time\"), True).otherwise(False)\n",
    "    ).withColumn(\n",
    "        \"end_track\",\n",
    "         # This columns tags the end of the track with True\n",
    "        F.when(F.max(col('event_time')).over(window_spec_minmax) == col(\"event_time\"), True).otherwise(False)\n",
    "    ).withColumn(\n",
    "        \"code_change\",\n",
    "        # This column tags every row for which the track:\n",
    "        #    1) begins \n",
    "        #    2) ends \n",
    "        #    3) is about to cross a FIR boundary with the next state vector\n",
    "        #    4) has just crossed a FIR boundary from the prev state vector\n",
    "        F.when((F.lag(\"code\").over(window_spec) != F.col(\"code\")) |\\\n",
    "               (F.lead(\"code\").over(window_spec) != F.col(\"code\")) |\\\n",
    "               col('begin_track') |\\\n",
    "               col('end_track')\n",
    "               , True).otherwise(False))\n",
    "\n",
    "traj_sdf = traj_sdf.filter(col('code_change') == True)\n",
    "\n",
    "traj_sdf = traj_sdf\\\n",
    "    .withColumn('after_lat', F.lead('lat').over(window_spec))\\\n",
    "    .withColumn('after_lon', F.lead('lon').over(window_spec))\\\n",
    "    .withColumn('after_FL', F.lead('FL').over(window_spec))\\\n",
    "    .withColumn('after_event_time', F.lead('event_time').over(window_spec))\\\n",
    "    .withColumn('after_code', F.lead('code').over(window_spec))\\\n",
    "    .withColumn('after_min_fl', F.lead('min_fl').over(window_spec))\\\n",
    "    .withColumn('after_max_fl', F.lead('max_fl').over(window_spec))\n",
    "\n",
    "traj_sdf = traj_sdf\\\n",
    "    .withColumnRenamed('lat', 'before_lat')\\\n",
    "    .withColumnRenamed('lon', 'before_lon')\\\n",
    "    .withColumnRenamed('FL', 'before_FL')\\\n",
    "    .withColumnRenamed('event_time', 'before_event_time')\\\n",
    "    .withColumnRenamed('code', 'before_code')\\\n",
    "    .withColumnRenamed('min_fl', 'before_min_fl')\\\n",
    "    .withColumnRenamed('max_fl', 'before_max_fl')\n",
    "\n",
    "columns = ['lat','lon','FL','event_time', 'code', 'min_fl', 'max_fl']\n",
    "\n",
    "for col_name in columns:\n",
    "    traj_sdf = traj_sdf\\\n",
    "        .withColumn( # If it's the beginning of the track, we set the after values to the first point (so that mid point is original)\n",
    "            'after_' + col_name,  \n",
    "            F.when(col('begin_track'), col('before_'+ col_name)).otherwise(col('after_' + col_name)))\\\n",
    "        .withColumn(\n",
    "            'after_' + col_name, \n",
    "            F.when(col('end_track'), col('before_'+ col_name)).otherwise(col('after_' + col_name)))\\\n",
    "    \n",
    "\n",
    "traj_sdf = traj_sdf.filter((col('before_code') != col('after_code')) | col('begin_track') | col('end_track'))\n",
    "\n",
    "all_columns = ['track_id'] +\\\n",
    "    ['before_' + x for x in columns] +\\\n",
    "    ['after_' + x for x in columns]\n",
    "\n",
    "\n",
    "traj_sdf = traj_sdf.select(all_columns)\n",
    "\n",
    "# Define the midpoint calculation function\n",
    "def compute_midpoint(lat1, lon1, fl1, lat2, lon2, fl2):\n",
    "    \"\"\"\n",
    "    Compute the geographic midpoint and altitude-adjusted midpoint \n",
    "    between two latitude-longitude-altitude points.\n",
    "    \"\"\"\n",
    "    if None in (lat1, lon1, fl1, lat2, lon2, fl2):\n",
    "        return None, None, None  # Handle missing data gracefully\n",
    "\n",
    "    # Convert degrees to radians\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "\n",
    "    # Convert to Cartesian coordinates\n",
    "    x1, y1, z1 = math.cos(lat1) * math.cos(lon1), math.cos(lat1) * math.sin(lon1), math.sin(lat1)\n",
    "    x2, y2, z2 = math.cos(lat2) * math.cos(lon2), math.cos(lat2) * math.sin(lon2), math.sin(lat2)\n",
    "\n",
    "    # Compute midpoint in Cartesian coordinates\n",
    "    x_m, y_m, z_m = (x1 + x2) / 2, (y1 + y2) / 2, (z1 + z2) / 2\n",
    "\n",
    "    # Convert back to latitude and longitude\n",
    "    lon_m = math.atan2(y_m, x_m)\n",
    "    hyp = math.sqrt(x_m**2 + y_m**2)\n",
    "    lat_m = math.atan2(z_m, hyp)\n",
    "\n",
    "    # Compute average altitude (FL)\n",
    "    fl_m = (fl1 + fl2) / 2\n",
    "\n",
    "    # Convert radians to degrees\n",
    "    return float(math.degrees(lat_m)), float(math.degrees(lon_m)), float(fl_m)\n",
    "\n",
    "# Register UDF with return type as StructType(DoubleType, DoubleType, DoubleType)\n",
    "midpoint_udf = udf(compute_midpoint, returnType=\"struct<mid_lat:double, mid_lon:double, mid_FL:double>\")\n",
    "\n",
    "# Load DataFrame (assuming it's already loaded as `df`)\n",
    "traj_sdf = traj_sdf.withColumn(\"midpoint\", midpoint_udf(col(\"before_lat\"), col(\"before_lon\"), col(\"before_FL\"), \n",
    "                                            col(\"after_lat\"), col(\"after_lon\"), col(\"after_FL\")))\n",
    "\n",
    "# Extract computed values\n",
    "traj_sdf = traj_sdf.withColumn(\"mid_lat\", col(\"midpoint.mid_lat\")) \\\n",
    "       .withColumn(\"mid_lon\", col(\"midpoint.mid_lon\")) \\\n",
    "       .withColumn(\"mid_FL\", col(\"midpoint.mid_FL\")) \\\n",
    "       .drop(\"midpoint\")\n",
    "\n",
    "# Compute midpoint timestamp as average of before and after timestamps\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"mid_event_time\",\n",
    "    ((col(\"before_event_time\").cast(\"long\") + col(\"after_event_time\").cast(\"long\")) / 2).cast(TimestampType())\n",
    ").withColumn(\n",
    "    \"mid_time_range\", \n",
    "    col('after_event_time').cast('long') - col('before_event_time').cast('long'))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"before_event_time\", col(\"before_event_time\").cast(\"string\")\n",
    ").withColumn(\n",
    "    \"after_event_time\", col(\"after_event_time\").cast(\"string\")\n",
    ").withColumn(\n",
    "    \"mid_event_time\", col(\"mid_event_time\").cast(\"string\")\n",
    ")\n",
    "\n",
    "# Define the Haversine formula using PySpark functions\n",
    "def haversine_distance(df, lat1, lon1, lat2, lon2, output_col):\n",
    "    # Convert degrees to radians\n",
    "    df = df.withColumn(lat1, F.radians(F.col(lat1))) \\\n",
    "           .withColumn(lon1, F.radians(F.col(lon1))) \\\n",
    "           .withColumn(lat2, F.radians(F.col(lat2))) \\\n",
    "           .withColumn(lon2, F.radians(F.col(lon2)))\n",
    "\n",
    "    # Haversine formula components\n",
    "    delta_lat = F.col(lat2) - F.col(lat1)\n",
    "    delta_lon = F.col(lon2) - F.col(lon1)\n",
    "\n",
    "    a = F.pow(F.sin(delta_lat / 2), 2) + \\\n",
    "        F.cos(F.col(lat1)) * F.cos(F.col(lat2)) * F.pow(F.sin(delta_lon / 2), 2)\n",
    "\n",
    "    c = 2 * F.atan2(F.sqrt(a), F.sqrt(1 - a))\n",
    "\n",
    "    # Earth's radius in kilometers\n",
    "    R = 6371.0\n",
    "\n",
    "    return df.withColumn(output_col, R * c)\n",
    "\n",
    "traj_sdf = haversine_distance(traj_sdf, \"before_lat\", \"before_lon\", \"after_lat\", \"after_lon\", 'mid_distance_range')\n",
    "\n",
    "traj_sdf = traj_sdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d376fa34-5203-4911-9545-dc6b5666cc82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the window specification\n",
    "window_spec = Window.partitionBy(\"track_id\").orderBy(\"mid_event_time\")\n",
    "\n",
    "# Perform lag operations to shift values within each track_id group\n",
    "traj_sdf = traj_sdf.withColumns({\n",
    "    \"AIRSPACE_ID\": F.lag(\"after_code\").over(window_spec),\n",
    "    \"entry_lon\": F.lag(\"mid_lon\").over(window_spec),\n",
    "    \"entry_lat\": F.lag(\"mid_lat\").over(window_spec),\n",
    "    \"entry_FL\": F.lag(\"mid_FL\").over(window_spec),\n",
    "    \"entry_time\": F.lag(\"mid_event_time\").over(window_spec),\n",
    "    \"entry_time_range\": F.lag(\"mid_time_range\").over(window_spec),\n",
    "    \"entry_distance_range\": F.lag(\"mid_distance_range\").over(window_spec) \n",
    "})\n",
    "\n",
    "# Rename columns for exit values\n",
    "traj_sdf = traj_sdf.withColumnRenamed(\"mid_lon\", \"exit_lon\")\\\n",
    "    .withColumnRenamed(\"mid_lat\",\"exit_lat\")\\\n",
    "    .withColumnRenamed(\"mid_FL\", \"exit_FL\")\\\n",
    "    .withColumnRenamed(\"mid_event_time\", \"exit_time\")\\\n",
    "    .withColumnRenamed(\"mid_time_range\", \"exit_time_range\")\\\n",
    "    .withColumnRenamed(\"mid_distance_range\", \"exit_distance_range\")\n",
    "    \n",
    "\n",
    "# Select final columns and filter out rows where AIRSPACE_ID is null\n",
    "traj_sdf = traj_sdf.select(\n",
    "    \"track_id\", \"AIRSPACE_ID\", \"entry_time\", \"entry_lon\", \"entry_lat\", \"entry_FL\", \"entry_time_range\", \"entry_distance_range\",\n",
    "    \"exit_time\", \"exit_lon\", \"exit_lat\", \"exit_FL\", \"exit_time_range\", \"exit_distance_range\"\n",
    ").filter(col(\"AIRSPACE_ID\").isNotNull())\n",
    "\n",
    "traj_df = traj_sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79626400-50f0-443d-b998-4f83f771f88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df.to_parquet('crossings_opdi_rounded.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4570e3a2-1b70-45e9-9bfa-469e13ce4144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec785cd7-f0da-4def-a906-d22f7c789915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35cdd271-d8a4-4b23-8a17-36d927831375",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    'track_id',\n",
    "    'before_code',\n",
    "    'after_code',\n",
    "    'mid_lon',\n",
    "    'mid_lat',\n",
    "    'mid_FL',\n",
    "    'mid_event_time'\n",
    "]\n",
    "\n",
    "traj_df = traj_df[cols]\n",
    "\n",
    "traj_df['AIRSPACE_ID'] = traj_df.after_code.shift(1)\n",
    "traj_df['entry_lon'] = traj_df.mid_lon.shift(1)\n",
    "traj_df['entry_lat'] = traj_df.mid_lat.shift(1)\n",
    "traj_df['entry_FL'] = traj_df.mid_FL.shift(1)\n",
    "traj_df['entry_time'] = traj_df.mid_event_time.shift(1)\n",
    "\n",
    "traj_df = traj_df.rename({\n",
    "    'mid_lon':'exit_lon', \n",
    "    'mid_lat':'exit_lat', \n",
    "    'mid_FL':'exit_FL',\n",
    "    'mid_FL':'exit_FL',\n",
    "    'mid_event_time':'exit_time'}, axis=1)\n",
    "\n",
    "traj_df = traj_df[['track_id', 'entry_time', 'entry_lon', 'entry_lat', 'entry_FL', 'exit_time','exit_lon','exit_lat', 'exit_FL','AIRSPACE_ID']]\n",
    "traj_df = traj_df[~traj_df.AIRSPACE_ID.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2b2d635d-1bd9-4f07-ac27-e1f63be1cff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9afe654-136d-47ba-b90a-44445aadbd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7657cd75-c1ce-4bc5-b71c-8528caa575bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111f1317-f76d-4dc8-b208-ba6e4039fc07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4c35bb2-7c1d-49f1-9357-d4290b268d9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_id\n",
       "1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd325bccff2414f6ae8a6_4_2025_1     149\n",
       "9986e2b7d038c99eb75facbb0c91325f752be48e5cf7a0f69726309362373d85_2_2025_1     142\n",
       "9986e2b7d038c99eb75facbb0c91325f752be48e5cf7a0f69726309362373d85_9_2025_1     126\n",
       "f03a46721d35a8ed6c8a394c923214a721da4eef746f44b35dab4236b4d24892_0_2025_1     119\n",
       "1a1f3790a6d19ac8826fc03a1e532b25ba04f3d945578e6a7e41b8dea987b57b_0_2025_1     110\n",
       "                                                                             ... \n",
       "012d40b81e0d94ac516680e04faf8c9a58d0779e928d6da497e6484174b50b12_98_2025_1      1\n",
       "0132efb712207971964151cb4dbb5abafca55809c49002d52dd9b0db5907eb22_19_2025_1      1\n",
       "01a43a73319c665ebd039a916378e38cde882c8db6cf4cf93e98319dd8cb99c2_48_2025_1      1\n",
       "01a50ab8ab96f6aed78028653abf47de06829840ed1702939b563a533e884aef_27_2025_1      1\n",
       "01b1adfa33899456f094bfe7e0c99d50f2ef8a8b6793ff7a80c64ea1322fb550_3_2025_1       1\n",
       "Name: count, Length: 1304571, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_df.track_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00ddff75-a5dd-42e5-b769-c6e8cb3ff2ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>before_lat</th>\n",
       "      <th>before_lon</th>\n",
       "      <th>before_FL</th>\n",
       "      <th>before_event_time</th>\n",
       "      <th>before_code</th>\n",
       "      <th>before_min_fl</th>\n",
       "      <th>before_max_fl</th>\n",
       "      <th>after_lat</th>\n",
       "      <th>after_lon</th>\n",
       "      <th>after_FL</th>\n",
       "      <th>after_event_time</th>\n",
       "      <th>after_code</th>\n",
       "      <th>after_min_fl</th>\n",
       "      <th>after_max_fl</th>\n",
       "      <th>mid_lat</th>\n",
       "      <th>mid_lon</th>\n",
       "      <th>mid_FL</th>\n",
       "      <th>mid_event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2305113</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>50.941136</td>\n",
       "      <td>0.930023</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2025-01-12 17:02:30</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305114</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.003609</td>\n",
       "      <td>1.456451</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2025-01-12 17:13:00</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.005425</td>\n",
       "      <td>1.461639</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2025-01-12 17:13:05</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>51.004517</td>\n",
       "      <td>1.459045</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305115</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.009754</td>\n",
       "      <td>1.470947</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2025-01-12 17:13:25</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>51.013199</td>\n",
       "      <td>1.479263</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2025-01-12 17:13:30</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.011477</td>\n",
       "      <td>1.475105</td>\n",
       "      <td>6.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305116</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.175323</td>\n",
       "      <td>1.999215</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 17:30:05</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.171066</td>\n",
       "      <td>1.999289</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 17:30:10</td>\n",
       "      <td>EBBUFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>51.173195</td>\n",
       "      <td>1.999252</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305117</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.138890</td>\n",
       "      <td>1.998520</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 17:31:40</td>\n",
       "      <td>EBBUFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>51.121200</td>\n",
       "      <td>1.995773</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 17:31:45</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.130045</td>\n",
       "      <td>1.997146</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305257</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.011078</td>\n",
       "      <td>1.497926</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:25:00</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>51.010173</td>\n",
       "      <td>1.493607</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:25:05</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.010626</td>\n",
       "      <td>1.495766</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305258</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.009201</td>\n",
       "      <td>1.489390</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:25:10</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.009009</td>\n",
       "      <td>1.488495</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:25:15</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>51.009105</td>\n",
       "      <td>1.488942</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305259</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>51.001923</td>\n",
       "      <td>1.462295</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:25:45</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>50.999588</td>\n",
       "      <td>1.459771</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:25:50</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>51.000755</td>\n",
       "      <td>1.461033</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305260</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>50.902267</td>\n",
       "      <td>1.459029</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:28:45</td>\n",
       "      <td>EGTTFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>245</td>\n",
       "      <td>50.898331</td>\n",
       "      <td>1.459177</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:28:50</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>50.900299</td>\n",
       "      <td>1.459103</td>\n",
       "      <td>2.0</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2305261</th>\n",
       "      <td>1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...</td>\n",
       "      <td>50.898331</td>\n",
       "      <td>1.459177</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2025-01-12 19:33:55</td>\n",
       "      <td>LFFFFIR</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  track_id  before_lat  \\\n",
       "2305113  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...         NaN   \n",
       "2305114  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.003609   \n",
       "2305115  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.009754   \n",
       "2305116  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.175323   \n",
       "2305117  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.138890   \n",
       "...                                                    ...         ...   \n",
       "2305257  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.011078   \n",
       "2305258  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.009201   \n",
       "2305259  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   51.001923   \n",
       "2305260  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   50.902267   \n",
       "2305261  1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd3...   50.898331   \n",
       "\n",
       "         before_lon  before_FL    before_event_time before_code before_min_fl  \\\n",
       "2305113         NaN        NaN                 None        None          None   \n",
       "2305114    1.456451        6.0  2025-01-12 17:13:00     EGTTFIR             0   \n",
       "2305115    1.470947        6.0  2025-01-12 17:13:25     LFFFFIR             0   \n",
       "2305116    1.999215        2.0  2025-01-12 17:30:05     EGTTFIR             0   \n",
       "2305117    1.998520        2.0  2025-01-12 17:31:40     EBBUFIR             0   \n",
       "...             ...        ...                  ...         ...           ...   \n",
       "2305257    1.497926        2.0  2025-01-12 19:25:00     LFFFFIR             0   \n",
       "2305258    1.489390        2.0  2025-01-12 19:25:10     EGTTFIR             0   \n",
       "2305259    1.462295        2.0  2025-01-12 19:25:45     LFFFFIR             0   \n",
       "2305260    1.459029        2.0  2025-01-12 19:28:45     EGTTFIR             0   \n",
       "2305261    1.459177        2.0  2025-01-12 19:33:55     LFFFFIR             0   \n",
       "\n",
       "        before_max_fl  after_lat  after_lon  after_FL     after_event_time  \\\n",
       "2305113          None  50.941136   0.930023       0.0  2025-01-12 17:02:30   \n",
       "2305114           245  51.005425   1.461639       6.0  2025-01-12 17:13:05   \n",
       "2305115           195  51.013199   1.479263       6.0  2025-01-12 17:13:30   \n",
       "2305116           245  51.171066   1.999289       2.0  2025-01-12 17:30:10   \n",
       "2305117           195  51.121200   1.995773       2.0  2025-01-12 17:31:45   \n",
       "...               ...        ...        ...       ...                  ...   \n",
       "2305257           195  51.010173   1.493607       2.0  2025-01-12 19:25:05   \n",
       "2305258           245  51.009009   1.488495       2.0  2025-01-12 19:25:15   \n",
       "2305259           195  50.999588   1.459771       2.0  2025-01-12 19:25:50   \n",
       "2305260           245  50.898331   1.459177       2.0  2025-01-12 19:28:50   \n",
       "2305261           195        NaN        NaN       NaN                 None   \n",
       "\n",
       "        after_code after_min_fl after_max_fl    mid_lat   mid_lon  mid_FL  \\\n",
       "2305113    EGTTFIR            0          245        NaN       NaN     NaN   \n",
       "2305114    LFFFFIR            0          195  51.004517  1.459045     6.0   \n",
       "2305115    EGTTFIR            0          245  51.011477  1.475105     6.0   \n",
       "2305116    EBBUFIR            0          195  51.173195  1.999252     2.0   \n",
       "2305117    EGTTFIR            0          245  51.130045  1.997146     2.0   \n",
       "...            ...          ...          ...        ...       ...     ...   \n",
       "2305257    EGTTFIR            0          245  51.010626  1.495766     2.0   \n",
       "2305258    LFFFFIR            0          195  51.009105  1.488942     2.0   \n",
       "2305259    EGTTFIR            0          245  51.000755  1.461033     2.0   \n",
       "2305260    LFFFFIR            0          195  50.900299  1.459103     2.0   \n",
       "2305261       None         None         None        NaN       NaN     NaN   \n",
       "\n",
       "        mid_event_time  \n",
       "2305113           None  \n",
       "2305114           None  \n",
       "2305115           None  \n",
       "2305116           None  \n",
       "2305117           None  \n",
       "...                ...  \n",
       "2305257           None  \n",
       "2305258           None  \n",
       "2305259           None  \n",
       "2305260           None  \n",
       "2305261           None  \n",
       "\n",
       "[149 rows x 19 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_df[traj_df.track_id == '1a450c69faf934d7a4e369cb67eeb17bd0d4658e9dccd325bccff2414f6ae8a6_4_2025_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccc2b3d-5c87-4421-9580-33bb0e835f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('SELECT * FROM project_opdi.opdi_flight_list WHERE year(dof) == 2025;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b88e85-a902-4652-b3e6-c96fe0fc06e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "track_id\n",
       "38d0deaa5832fa8e5a181726450165d972807ef673404ff95ef2cb232e04b631_6_2025_1      29\n",
       "3b2918e367250d918f894a4bab2fcb57c50f2f13b64adfa3ef956541d92bfd92_0_2025_1      28\n",
       "1177b98d81f34ee3224e3b6a4765a394a6cfacbc676dc139ea6924ebc759acbf_0_2025_1      25\n",
       "3d246b2cde8bf536a721d94618baa364b8da6d661dccb52a15039ca7a3a4392e_0_2025_1      25\n",
       "25562f47ee4dd92018f19ca0e7fb4a29f5d52a12b221802de90be7f9b1a26aa3_0_2025_1      24\n",
       "                                                                               ..\n",
       "0b989d13c8c46b5900cf968d6e4c2ae1bb0b0c41593cb9bef0817257f90e2a5e_25_2025_1      1\n",
       "0bb817090dd642660a0b7e64c120d768931943bac52c8b8046abf868be6a23d0_32_2025_1      1\n",
       "0bef62801da3d094805b08b31636bab7531d1118480c9ad6d779df17f09a4276_104_2025_1     1\n",
       "51368e48ccfab6b6fbe41b1847528dd38e04279aa4e9a8353a753959d27bc6a6_0_2025_1       1\n",
       "51408f7731e2dc1dd674dd0ace20101a090f2937db8e8c1cf2bb67026b7da403_0_2025_1       1\n",
       "Name: count, Length: 2458, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "traj_df.track_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b5d24b-ba0d-48b4-a4ab-15dcef442ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df.to_parquet('crossings_example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ff02f-3cdc-47e4-a01e-b457630e9326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    Calculate the great-circle distance between two points \n",
    "    on the Earth using the Haversine formula.\n",
    "    \n",
    "    Parameters:\n",
    "    lat1, lon1 : float - Latitude and longitude of the first point in degrees\n",
    "    lat2, lon2 : float - Latitude and longitude of the second point in degrees\n",
    "    \n",
    "    Returns:\n",
    "    float - Distance in kilometers\n",
    "    \"\"\"\n",
    "    R = 6371.0  # Earth's radius in km\n",
    "    \n",
    "    lat1, lon1, lat2, lon2 = map(radians, [lat1, lon1, lat2, lon2])\n",
    "    \n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    \n",
    "    a = sin(dlat / 2)**2 + cos(lat1) * cos(lat2) * sin(dlon / 2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    \n",
    "    return R * c\n",
    "\n",
    "# Ensure proper handling of NaN values before calculation\n",
    "traj_df[\"haversine_distance_km\"] = traj_df.apply(lambda row: np.nan if any(pd.isnull([row[\"before_lat\"], row[\"before_lon\"], row[\"after_lat\"], row[\"after_lon\"]])) \n",
    "                                       else haversine(row[\"before_lat\"], row[\"before_lon\"], row[\"after_lat\"], row[\"after_lon\"]), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f5ed6-c687-4d1a-a3a0-010cf8e8c1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda943d6-eff7-48ac-ba44-81f8b483e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_df['distance'] = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3e0fb-bc85-4598-a502-f5bd650399d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_sdf = traj_sdf.withColumn(\n",
    "    'grouping_id_lead', \n",
    "    F.sum(\n",
    "        F.when(\n",
    "            col('begin_track') |\\\n",
    "            col('end_track') |\\\n",
    "            (F.lead('code').over(window_spec) != F.col('code'))\\\n",
    "            , 1).otherwise(0))\\\n",
    "    .over(window_spec)).withColumn(\n",
    "    'grouping_id_lag', # This one is added because you might have subsequent points that are crossings. We will treat these seperately. We can later drop duplicates! \n",
    "    F.sum(\n",
    "        F.when(F.lag('code').over(window_spec) != F.col('code'), 1).otherwise(0))\\\n",
    "    .over(window_spec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531e21b0-910d-43ec-a737-b5207f5d7f52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc95fb-5253-480f-a6b1-efc116d01674",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_sdf"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32b56059-dcb0-428a-96b0-bd042eb9a599",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19ea91-43b4-4394-b1a7-7567ebf05849",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_window = Window.partitionBy(\"track_id\", \"grouping_id_lead\").orderBy(\"event_time\")\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lead\", \n",
    "    F.row_number().over(group_window))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lead\",\n",
    "    F.when(col('begin_track'), 2).otherwise(col('row_num_lead')))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lead\",\n",
    "    F.when(col('end_track'), 1).otherwise(col('row_num_lead')))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"point_type_lead\",\n",
    "    F.when(F.col(\"row_num\") == 1, \"exit\").otherwise(\"entry\")\n",
    ")\n",
    "\n",
    "traj_sdf_lead = traj_sdf.orderBy(\"event_time\").groupBy(\"track_id\", \"grouping_id_lead\").pivot(\"point_type_lead\").agg(\n",
    "    F.first(\"lat\").alias(\"lat\"),\n",
    "    F.first(\"lon\").alias(\"lon\"),\n",
    "    F.first(\"event_time\").alias(\"event_time\"),\n",
    "    F.first(\"FL\").alias(\"FL\"),\n",
    "    F.first(\"code\").alias(\"code\"),\n",
    "    F.first(\"min_fl\").alias(\"min_fl\"),\n",
    "    F.first(\"max_fl\").alias(\"max_fl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a456eb-2ed2-4f16-89b3-cec0657f01cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_window = Window.partitionBy(\"track_id\", \"grouping_id_lag\").orderBy(\"event_time\")\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lag\", \n",
    "    F.row_number().over(group_window))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lag\",\n",
    "    F.when(col('begin_track'), 2).otherwise(col('row_num_lag')))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lag\",\n",
    "    F.when(col('end_track'), 1).otherwise(col('row_num_lag')))\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"row_num_lag\",\n",
    "    F.when(F.col(\"row_num\") == 1, \"entry\").otherwise(\"exit\")\n",
    ")\n",
    "\n",
    "traj_sdf_lag = traj_sdf.orderBy(\"event_time\").groupBy(\"track_id\", \"grouping_id_lag\").pivot(\"point_type_lag\").agg(\n",
    "    F.first(\"lat\").alias(\"lat\"),\n",
    "    F.first(\"lon\").alias(\"lon\"),\n",
    "    F.first(\"event_time\").alias(\"event_time\"),\n",
    "    F.first(\"FL\").alias(\"FL\"),\n",
    "    F.first(\"code\").alias(\"code\"),\n",
    "    F.first(\"min_fl\").alias(\"min_fl\"),\n",
    "    F.first(\"max_fl\").alias(\"max_fl\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedae0c-e99d-43b9-aa6f-2ec19f9369bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_sdf_lag = traj_sdf_lag.withColumn(\n",
    "    \"entry_event_time\", col(\"entry_event_time\").cast(\"string\")\n",
    ").withColumn(\n",
    "    \"exit_event_time\", col(\"exit_event_time\").cast(\"string\")\n",
    ")\n",
    "df_lag = traj_sdf_lag.limit(10000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5739dd-1dac-4910-af49-a4c71ffb5711",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_sdf_lead = traj_sdf_lead.withColumn(\n",
    "    \"entry_event_time\", col(\"entry_event_time\").cast(\"string\")\n",
    ").withColumn(\n",
    "    \"exit_event_time\", col(\"exit_event_time\").cast(\"string\")\n",
    ")\n",
    "df_lead = traj_sdf_lead.limit(10000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74d9805-9631-4d26-b5bc-2e34f6eabda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lead[df_lead['entry_code'] != df_lead['exit_code']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae1c07-74d6-4dee-8508-608f0c8f817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_lag, df_lead], axis=0)\n",
    "df = df[[x for x in df.columns if x not in ['grouping_id_lag', 'grouping_id_lead']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa293345-5321-4f0a-9218-1831c7d79198",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7714232-b757-4e42-ac0f-8d1479795ae3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50ebc51-1c6b-46f3-b22a-008e147c1b13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbc339a-0b1a-4edb-bfe7-46e0e74e16fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"entry_event_time\", col(\"entry_event_time\").cast(\"string\")\n",
    ")\n",
    "\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"exit_event_time\", col(\"exit_event_time\").cast(\"string\")\n",
    ")\n",
    "pd.set_option('display.max_rows', 2338)\n",
    "tmp = traj_sdf.toPandas()\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e06b634-0f0f-4e92-af7c-d25bee2d506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_parquet('crossing_example.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf837ee9-227b-440c-938f-3f03268b365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Sample dataframe\n",
    "df = tmp # Replace with your actual data source\n",
    "\n",
    "# Convert timestamps to datetime\n",
    "df[\"entry_event_time\"] = pd.to_datetime(df[\"entry_event_time\"], errors=\"coerce\")\n",
    "df[\"exit_event_time\"] = pd.to_datetime(df[\"exit_event_time\"], errors=\"coerce\")\n",
    "\n",
    "# Sort by entry time to maintain correct order\n",
    "df = df.sort_values(by=\"entry_event_time\")\n",
    "\n",
    "# Create figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add lines connecting entry and exit points\n",
    "for _, row in df.iterrows():\n",
    "    fig.add_trace(go.Scattermapbox(\n",
    "        mode=\"lines\",\n",
    "        lat=[row[\"entry_lat\"], row[\"exit_lat\"]],\n",
    "        lon=[row[\"entry_lon\"], row[\"exit_lon\"]],\n",
    "        line=dict(width=2, color=\"black\"),\n",
    "        name=f\"Group {row['grouping_id']}: {row['entry_code']} → {row['exit_code']}\",\n",
    "        hovertext=f\"FL {row['entry_FL']} → FL {row['exit_FL']}\",\n",
    "        hoverinfo=\"text\"\n",
    "    ))\n",
    "\n",
    "# Add entry points (blue)\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    mode=\"markers\",\n",
    "    lat=df[\"entry_lat\"],\n",
    "    lon=df[\"entry_lon\"],\n",
    "    marker=dict(size=8, color=\"blue\"),\n",
    "    name=\"Entry Points\",\n",
    "    hovertext=df[\"entry_code\"],\n",
    "    hoverinfo=\"text\"\n",
    "))\n",
    "\n",
    "# Add exit points (red)\n",
    "fig.add_trace(go.Scattermapbox(\n",
    "    mode=\"markers\",\n",
    "    lat=df[\"exit_lat\"],\n",
    "    lon=df[\"exit_lon\"],\n",
    "    marker=dict(size=8, color=\"red\"),\n",
    "    name=\"Exit Points\",\n",
    "    hovertext=df[\"exit_code\"],\n",
    "    hoverinfo=\"text\"\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    mapbox=dict(style=\"open-street-map\"),\n",
    "    title=\"Entry and Exit Points with Flight Paths\",\n",
    "    margin=dict(l=2, r=2, t=40, b=2)\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "\n",
    "fig.write_html('crossing_example.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da96ae56-2214-4808-8ea0-761ddf0ca6bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c0d202-d984-42b0-8151-201f1cfc2f4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07fc2e1-d29e-4090-a617-eacd459e9591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c2d314-4c70-4d6a-9ad6-ebaa6cdc3f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new window by track and code_group\n",
    "group_window = Window.partitionBy(\"track_id\", \"code_group\").orderBy(\"event_time\")\n",
    "max_window = Window.partitionBy(\"track_id\", \"code_group\")\n",
    "\n",
    "# Identify first and last row in each code group\n",
    "traj_sdf = traj_sdf.withColumn(\"row_num\", F.row_number().over(group_window))\n",
    "traj_sdf = traj_sdf.withColumn(\"max_row\", F.max(\"row_num\").over(max_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32071e8-a857-411a-8fc2-0c006c4920d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transition group that includes the first row of a new code_group and the last row of the previous group\n",
    "traj_sdf = traj_sdf.withColumn(\n",
    "    \"transition_code_group\",\n",
    "    F.when(F.col(\"row_num\") == 1, F.col(\"prev_code_group\")).otherwise(F.col(\"code_group\"))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c62ab8-1167-4ad1-b7fd-423dbef3da20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e96306-7e39-4708-839f-2f40ec47b049",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Keep only the first and last row per transition_code_group\n",
    "filtered_traj_sdf = traj_sdf.filter(\n",
    "    (F.col(\"row_num\") == 1) | (F.col(\"row_num\") == F.col(\"max_row\"))\n",
    ")\n",
    "\n",
    "# Add a column to indicate first or last point\n",
    "filtered_traj_sdf = filtered_traj_sdf.withColumn(\n",
    "    \"point_type\",\n",
    "    F.when(F.col(\"row_num\") == 1, \"entry\").otherwise(\"exit\")\n",
    ")\n",
    "\n",
    "\n",
    "filtered_traj_sdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded3db27-f133-477c-8015-158b69041b2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Pivot the dataset to have separate columns for first and last points\n",
    "pivoted_traj_sdf = filtered_traj_sdf.orderBy(\"event_time\").groupBy(\"track_id\", \"transition_code_group\").pivot(\"point_type\").agg(\n",
    "    F.first(\"lat\").alias(\"lat\"),\n",
    "    F.first(\"lon\").alias(\"lon\"),\n",
    "    F.first(\"event_time\").alias(\"event_time\"),\n",
    "    F.first(\"FL\").alias(\"FL\"),\n",
    "    F.first(\"code\").alias(\"code\"),\n",
    "    F.first(\"min_fl\").alias(\"min_fl\"),\n",
    "    F.first(\"max_fl\").alias(\"max_fl\")\n",
    ")\n",
    "\n",
    "# Show result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23550d60-21a8-4a77-9088-e62aa395911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted_traj_sdf = pivoted_traj_sdf.withColumn(\n",
    "    \"last_event_time\", col(\"last_event_time\").cast(\"string\")\n",
    ").withColumn(\n",
    "    \"first_event_time\", col(\"first_event_time\").cast(\"string\")\n",
    ")\n",
    "df = pivoted_traj_sdf.limit(10000).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89c66dd-a52a-41bd-9221-e23c6b54070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.track_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e971a6f-03d8-4dd7-ad0b-a6c1b6a5bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To-do:\n",
    "# - Refresh opdi_h3_airspace_ref once Enrico fixes the airac 481 file for FIRs (currently 481 == 406)\n",
    "# - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e892199-e5e7-49ff-9ff6-e34c15fd3d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fir_sdf.filter((col('airspace_type') == 'FIR')).select(col('airac_cfmu')).dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bfb8f-32b2-4b90-8a2b-226c609a5b34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
