{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb462ee-f430-4d57-827c-e56b14668129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system('export PYTHONPATH=$PYTHONPATH:~/libs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d3a47f-085a-4de3-b388-38376c0222a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to quinten.goens\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, to_date, from_unixtime\n",
    "from datetime import datetime, date, timedelta\n",
    "import subprocess\n",
    "import os, shutil, time\n",
    "import os.path\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Settings\n",
    "## Config\n",
    "project = \"project_opdi\"\n",
    "start_month = date(2022, 1, 1)\n",
    "import_data = True\n",
    "move_data = True\n",
    "\n",
    "## Which months to process\n",
    "today = date.today()\n",
    "end_month = today - dateutil.relativedelta.relativedelta(months=1) # We work on the d-1 months\n",
    "\n",
    "# Getting today's date formatted\n",
    "today = today.strftime('%d %B %Y')\n",
    "\n",
    "# Spark Session Initialization\n",
    "#shutil.copy(\"/runtime-addons/cmladdon-2.0.40-b150/log4j.properties\", \"/etc/spark/conf/\") # Setting logging properties\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OPDI Ingestion\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memory\", \"12G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3G\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb8177d5-274a-4f96-a6af-9f47174fe221",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 659c150f-a957-4d8b-a660-3e729c501653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           min(dof)|           max(dof)|\n",
      "+-------------------+-------------------+\n",
      "|2022-01-01 00:00:00|2024-12-31 00:00:00|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT min(dof), max(dof) from project_opdi.opdi_flight_list_v2\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3716e985-d62b-4d96-bae6-be002b842c6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b5ab9-1fa1-4883-bba9-b64f51aeaa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_months(start_date, end_date):\n",
    "    \"\"\"Generate a list of dates corresponding to the first day of each month between two dates.\n",
    "\n",
    "    Args:\n",
    "    start_date (datetime.date): The starting date.\n",
    "    end_date (datetime.date): The ending date.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of date objects for the first day of each month within the specified range.\n",
    "    \"\"\"\n",
    "    current = start_date\n",
    "    months = []\n",
    "    while current <= end_date:\n",
    "        months.append(current)\n",
    "        # Increment month\n",
    "        month = current.month\n",
    "        year = current.year\n",
    "        if month == 12:\n",
    "            current = date(year + 1, 1, 1)\n",
    "        else:\n",
    "            current = date(year, month + 1, 1)\n",
    "    return months\n",
    "\n",
    "def get_start_end_of_month(date):\n",
    "    \"\"\"Return a datetime object for the first and last second  of the given month and year.\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    first_second = datetime(year, month, 1, 0, 0, 0)\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    last_second = datetime(year, month, last_day, 23, 59, 59)\n",
    "    return first_second.timestamp(), last_second.timestamp()\n",
    "\n",
    "def move_data(project, month):\n",
    "    print(f\"Adding statevectors for {project}.osn_statevectors to {project}.osn_statevectors_v2 table for month {month}\")\n",
    "\n",
    "    start_time, end_time = get_start_end_of_month(month)\n",
    "\n",
    "    df = spark.table(f\"{project}.osn_statevectors\")\n",
    "    df = df.filter(\n",
    "        (df.event_time >= from_unixtime(lit(start_time)).cast(\"timestamp\")) &\n",
    "        (df.event_time < from_unixtime(lit(end_time)).cast(\"timestamp\"))\n",
    "    )\n",
    "\n",
    "    # Add event_time_day column derived from event_time        \n",
    "    df_with_partition = df.withColumn(\"event_time_day\", to_date(col(\"event_time\")))\n",
    "    df_partitioned = df_with_partition.repartition(\"event_time_day\").orderBy(\"event_time_day\")\n",
    "    \n",
    "    # Drop event_time_day before writing\n",
    "    df_cleaned = df_partitioned.drop(\"event_time_day\")\n",
    "    \n",
    "    # Write the data for the month\n",
    "    df_cleaned.writeTo(f\"`{project}`.`osn_statevectors_v2`\").append()\n",
    "    \n",
    "\n",
    "if move_data:\n",
    "\n",
    "  #spark.sql(create_clustered_db)\n",
    "\n",
    "  to_process_months = generate_months(start_month, end_month)\n",
    "\n",
    "  ## Load logs\n",
    "  fpath = 'logs/01_osn-statevectors-moving-to-v2.parquet'\n",
    "  if os.path.isfile(fpath):\n",
    "    processed_months = pd.read_parquet(fpath).months.to_list()\n",
    "  else:\n",
    "    processed_months = []\n",
    "\n",
    "\n",
    "  ## Process loop\n",
    "  for month in to_process_months:\n",
    "    print(f'Processing month: {month}')\n",
    "    if month in processed_months:\n",
    "      print(f'Already processed {month}')\n",
    "      continue\n",
    "    else:\n",
    "      move_data(project, month)\n",
    "      processed_months.append(month)\n",
    "\n",
    "      ## Logging\n",
    "      processed_df = pd.DataFrame({'months':processed_months})\n",
    "      processed_df.to_parquet(fpath)\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
