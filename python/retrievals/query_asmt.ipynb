{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83599aa3-0c04-4673-bede-26d2afc82a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to quinten.goens\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import (\n",
    "    lit, lag, when, to_date, concat, avg, abs, col, unix_timestamp, to_timestamp\n",
    ")\n",
    "from pyspark.sql import DataFrame\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import h3_pyspark\n",
    "import gc\n",
    "\n",
    "# Settings\n",
    "## Config\n",
    "project = \"project_opdi\"\n",
    "max_h3_resolution = 12\n",
    "start_month = date(2022, 1, 1)\n",
    "\n",
    "## Which months to process\n",
    "today = date.today()\n",
    "end_month = today - dateutil.relativedelta.relativedelta(months=1) # We work on the d-1 months\n",
    "\n",
    "# Getting today's date formatted\n",
    "today = today.strftime('%d %B %Y')\n",
    "#    .config(\"spark.log.level\", \"ERROR\") \\\n",
    "#    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OPDI Ingestion\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"20G\") \\\n",
    "    .config(\"spark.executor.memory\", \"6G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3G\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e10f18d-e40e-4cb0-b99e-e24598ebd986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dec 04, 2024 5:08:32 PM org.apache.knox.gateway.shell.KnoxSession createClient\n",
      "INFO: Using default JAAS configuration\n",
      "24/12/04 17:08:33 ERROR common.DefaultRequestExecutor: Error executing request: org.apache.knox.gateway.shell.ErrorResponse: https://dl-live-ent-idbroker0.az-live.x9er-zkvz.cloudera.site:8444/gateway/azure-cab/cab/api/v1/credentials: HTTP/1.1 403 Forbidden\n",
      "24/12/04 17:08:33 ERROR idbroker.AbstractIDBClient: Cloud Access Broker response: {\n",
      "  \"error\": \"Ambiguous group role mappings for the authenticated user.\",\n",
      "  \"auth_id\": \"quinten.goens\"\n",
      "}\n",
      "\n",
      "24/12/04 17:08:33 ERROR common.DefaultRequestExecutor: Error executing request: org.apache.knox.gateway.shell.ErrorResponse: https://dl-live-ent-idbroker0.az-live.x9er-zkvz.cloudera.site:8444/gateway/azure-cab/cab/api/v1/credentials: HTTP/1.1 403 Forbidden\n",
      "24/12/04 17:08:33 ERROR idbroker.AbstractIDBClient: Cloud Access Broker response: {\n",
      "  \"error\": \"Ambiguous group role mappings for the authenticated user.\",\n",
      "  \"auth_id\": \"quinten.goens\"\n",
      "}\n",
      "\n",
      "24/12/04 17:08:33 ERROR common.DefaultRequestExecutor: Error executing request: org.apache.knox.gateway.shell.ErrorResponse: https://dl-live-ent-idbroker0.az-live.x9er-zkvz.cloudera.site:8444/gateway/azure-cab/cab/api/v1/credentials: HTTP/1.1 403 Forbidden\n",
      "24/12/04 17:08:33 ERROR idbroker.AbstractIDBClient: Cloud Access Broker response: {\n",
      "  \"error\": \"Ambiguous group role mappings for the authenticated user.\",\n",
      "  \"auth_id\": \"quinten.goens\"\n",
      "}\n",
      "\n",
      "24/12/04 17:08:33 ERROR common.DefaultRequestExecutor: Error executing request: org.apache.knox.gateway.shell.ErrorResponse: https://dl-live-ent-idbroker0.az-live.x9er-zkvz.cloudera.site:8444/gateway/azure-cab/cab/api/v1/credentials: HTTP/1.1 403 Forbidden\n",
      "24/12/04 17:08:33 ERROR idbroker.AbstractIDBClient: Cloud Access Broker response: {\n",
      "  \"error\": \"Ambiguous group role mappings for the authenticated user.\",\n",
      "  \"auth_id\": \"quinten.goens\"\n",
      "}\n",
      "\n",
      "ls: Status code: -1 error code: null error message: Auth failure: HTTP Error -1CustomTokenProvider getAccessToken threw java.io.IOException : Ambiguous group role mappings for the authenticated user. (user: quinten.goens)\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e914fe3-6152-46af-8f1f-5caf92d87235",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%fs` not found.\n"
     ]
    }
   ],
   "source": [
    "%fs rm -r abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205229a3-a79d-4655-aa74-f803be16b93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: az: not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32512"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the folder path\n",
    "folder_path = \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv\"\n",
    "\n",
    "# Use os.system to call Azure CLI or equivalent for deletion\n",
    "os.system(f\"az storage blob delete-batch --source my-container --pattern '{folder_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7812644-fbeb-43f3-b36a-ca067b327eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv/part-00000-6ff38fe7-c5b7-4409-8a08-c8ffa4a4704f-c000.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "folder_path = \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv\"\n",
    "\n",
    "# Use Spark's wholeTextFiles to inspect the folder\n",
    "file_paths = spark.sparkContext.wholeTextFiles(folder_path).keys().collect()\n",
    "print(file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "424260c6-1226-497c-b0e6-7306e1cbf237",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Wrong FS: abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv, expected: file:///",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Delete the folder recursively\u001b[39;00m\n\u001b[1;32m      8\u001b[0m folder_to_delete \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mhadoop\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mPath(folder_path)\n\u001b[0;32m----> 9\u001b[0m delete_status \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_to_delete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 'True' enables recursive delete\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Check the deletion status\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m delete_status:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Wrong FS: abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv, expected: file:///"
     ]
    }
   ],
   "source": [
    "# Define the folder path to delete\n",
    "folder_path = \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.csv\"\n",
    "\n",
    "# Access the Hadoop FileSystem\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "\n",
    "# Delete the folder recursively\n",
    "folder_to_delete = spark._jvm.org.apache.hadoop.fs.Path(folder_path)\n",
    "delete_status = fs.delete(folder_to_delete, True)  # 'True' enables recursive delete\n",
    "\n",
    "# Check the deletion status\n",
    "if delete_status:\n",
    "    print(f\"Folder {folder_path} deleted successfully.\")\n",
    "else:\n",
    "    print(f\"Failed to delete folder {folder_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "347cccae-f2f1-48f9-8d38-fae421453f0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9.38 ms, sys: 4.29 ms, total: 13.7 ms\n",
      "Wall time: 20.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT track_id, \n",
    "       icao24, \n",
    "       callsign,\n",
    "       CAST(event_time AS STRING) AS event_time,\n",
    "       lat, \n",
    "       lon, \n",
    "       baro_altitude, \n",
    "       geo_altitude\n",
    "FROM project_opdi.osn_tracks\n",
    "WHERE event_time BETWEEN TIMESTAMP('2022-01-01 00:00:00') AND TIMESTAMP('2022-01-01 06:00:00')\n",
    "\"\"\")\n",
    "\n",
    "csv_path = \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.parquet\"\n",
    "df.coalesce(1).write.mode(\"overwrite\").parquet(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cecc5b4a-a41c-4b95-a358-86f5154119f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Wrong FS: abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002, expected: file:///",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m path \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mhadoop\u001b[38;5;241m.\u001b[39mfs\u001b[38;5;241m.\u001b[39mPath(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mabfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# List all files in the directory\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m file_status \u001b[38;5;241m=\u001b[39m \u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistStatus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m files \u001b[38;5;241m=\u001b[39m [file\u001b[38;5;241m.\u001b[39mgetPath()\u001b[38;5;241m.\u001b[39mtoString() \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m file_status]\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Print all files in the folder\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Wrong FS: abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002, expected: file:///"
     ]
    }
   ],
   "source": [
    "# Get Hadoop FileSystem object\n",
    "fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())\n",
    "path = spark._jvm.org.apache.hadoop.fs.Path(\"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/\")\n",
    "\n",
    "# List all files in the directory\n",
    "file_status = fs.listStatus(path)\n",
    "files = [file.getPath().toString() for file in file_status]\n",
    "\n",
    "# Print all files in the folder\n",
    "for file in files:\n",
    "    print(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e55522-ecfd-45a3-961c-1bb81124967c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>track_id</th>\n",
       "      <th>icao24</th>\n",
       "      <th>callsign</th>\n",
       "      <th>event_time</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>baro_altitude</th>\n",
       "      <th>geo_altitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:19:45</td>\n",
       "      <td>26.748776</td>\n",
       "      <td>47.931501</td>\n",
       "      <td>11727.18</td>\n",
       "      <td>12062.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:19:50</td>\n",
       "      <td>26.756882</td>\n",
       "      <td>47.944647</td>\n",
       "      <td>11742.42</td>\n",
       "      <td>12077.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:19:55</td>\n",
       "      <td>26.762054</td>\n",
       "      <td>47.953975</td>\n",
       "      <td>11757.66</td>\n",
       "      <td>12085.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:00</td>\n",
       "      <td>26.769196</td>\n",
       "      <td>47.968641</td>\n",
       "      <td>11772.90</td>\n",
       "      <td>12108.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:05</td>\n",
       "      <td>26.774613</td>\n",
       "      <td>47.981415</td>\n",
       "      <td>11788.14</td>\n",
       "      <td>12123.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:10</td>\n",
       "      <td>26.776614</td>\n",
       "      <td>47.986644</td>\n",
       "      <td>11795.76</td>\n",
       "      <td>12131.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:15</td>\n",
       "      <td>26.783690</td>\n",
       "      <td>48.008247</td>\n",
       "      <td>11818.62</td>\n",
       "      <td>12153.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:20</td>\n",
       "      <td>26.786499</td>\n",
       "      <td>48.018753</td>\n",
       "      <td>11826.24</td>\n",
       "      <td>12153.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:25</td>\n",
       "      <td>26.786499</td>\n",
       "      <td>48.018753</td>\n",
       "      <td>11826.24</td>\n",
       "      <td>12153.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b8976e557478d88d686719e3f426611957318986833f69...</td>\n",
       "      <td>399a40</td>\n",
       "      <td>VLJ584H</td>\n",
       "      <td>2022-01-01 01:20:30</td>\n",
       "      <td>26.786499</td>\n",
       "      <td>48.018753</td>\n",
       "      <td>11826.24</td>\n",
       "      <td>12161.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            track_id  icao24  callsign  \\\n",
       "0  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "1  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "2  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "3  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "4  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "5  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "6  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "7  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "8  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "9  b8976e557478d88d686719e3f426611957318986833f69...  399a40  VLJ584H    \n",
       "\n",
       "            event_time        lat        lon  baro_altitude  geo_altitude  \n",
       "0  2022-01-01 01:19:45  26.748776  47.931501       11727.18      12062.46  \n",
       "1  2022-01-01 01:19:50  26.756882  47.944647       11742.42      12077.70  \n",
       "2  2022-01-01 01:19:55  26.762054  47.953975       11757.66      12085.32  \n",
       "3  2022-01-01 01:20:00  26.769196  47.968641       11772.90      12108.18  \n",
       "4  2022-01-01 01:20:05  26.774613  47.981415       11788.14      12123.42  \n",
       "5  2022-01-01 01:20:10  26.776614  47.986644       11795.76      12131.04  \n",
       "6  2022-01-01 01:20:15  26.783690  48.008247       11818.62      12153.90  \n",
       "7  2022-01-01 01:20:20  26.786499  48.018753       11826.24      12153.90  \n",
       "8  2022-01-01 01:20:25  26.786499  48.018753       11826.24      12153.90  \n",
       "9  2022-01-01 01:20:30  26.786499  48.018753       11826.24      12161.52  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_path = \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged/v002/test.parquet\"\n",
    "df = spark.read.parquet(csv_path, header=True, inferSchema=True).limit(10)\n",
    "df.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972a2c09-3787-4430-9003-b82065ec4edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.write.mode(\"overwrite\").csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f5d5e-20d8-44e4-a3f2-46f7a3d21c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903002e5-ef2c-432e-b975-e3d7d26ab289",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69f1a081-9386-4f4c-b2df-092d9ed0d4a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/tmp/asmt_2022-01-01.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/tmp/asmt_2022-01-01.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: '/tmp/asmt_2022-01-01.csv'"
     ]
    }
   ],
   "source": [
    "pd.read_csv('/tmp/asmt_2022-01-01.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c38606db-7834-42d7-8878-59976c5c042f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1679032"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c97a3da0-04cd-4740-9f81-3658a23a0121",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(\"/tmp/asmt_2022-01-01.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697523d0-7862-485a-a33f-4b7b0d4003ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m written_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/tmp/asmt_2022-01-01.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m written_df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:364\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    353\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    355\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    356\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    362\u001b[0m )\n\u001b[0;32m--> 364\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "written_df = spark.read.parquet(\"/tmp/asmt_2022-01-01.parquet\")\n",
    "written_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a2c21-ee47-4ef0-91be-ae60c2c8c505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d19d4ed5-e790-43a8-bb4d-803ce9ca8e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9149687"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f04130-915f-4f5d-8bf2-31860fa359a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf21827f-8fe3-450e-a96a-c411881e63e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['event_time'] = pd.to_datetime(df['event_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820a94d6-157e-4922-90ab-45288fb6b6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc447623-4d1e-429f-8022-34e151b64a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd73c1b-51c0-4783-88f8-31e8d5176353",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
