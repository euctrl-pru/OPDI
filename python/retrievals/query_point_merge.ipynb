{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7d9f06-be3e-44b7-b544-1cc6c2dd20fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94049784-dbd7-4b4f-b06d-0ddc3d5d3930",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to quinten.goens\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://spark-zcq9idem03kxcm06.cml-live.az-live.x9er-zkvz.cloudera.site\">https://spark-zcq9idem03kxcm06.cml-live.az-live.x9er-zkvz.cloudera.site</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ADES/ADEP: EGLL / EHAM / EDDF\n",
      "Processing: 2024-06-30 00:00:00\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719576000) AND FROM_UNIXTIME(1719835200)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-30')\n",
      "    AND oft.ADES = 'EGLL' \n",
      "    ;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 19ab0a73-9c94-40db-8949-f35b54cb4284\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File EGLL-to-*/tracks_adep_EGLL_ades_*_2024-06-30.parquet exists already.\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719576000) AND FROM_UNIXTIME(1719835200)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-30')\n",
      "    AND oft.ADES = 'EHAM' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719576000) AND FROM_UNIXTIME(1719835200)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-30')\n",
      "     \n",
      "    AND oft.ADEP = 'EHAM';\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719576000) AND FROM_UNIXTIME(1719835200)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-30')\n",
      "    AND oft.ADES = 'EDDF' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719576000) AND FROM_UNIXTIME(1719835200)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-30')\n",
      "     \n",
      "    AND oft.ADEP = 'EDDF';\n",
      "Processing: 2024-06-29 00:00:00\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719489600) AND FROM_UNIXTIME(1719748800)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-29')\n",
      "    AND oft.ADES = 'EGLL' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719489600) AND FROM_UNIXTIME(1719748800)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-29')\n",
      "     \n",
      "    AND oft.ADEP = 'EGLL';\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719489600) AND FROM_UNIXTIME(1719748800)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-29')\n",
      "    AND oft.ADES = 'EHAM' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719489600) AND FROM_UNIXTIME(1719748800)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-29')\n",
      "     \n",
      "    AND oft.ADEP = 'EHAM';\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719489600) AND FROM_UNIXTIME(1719748800)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-29')\n",
      "    AND oft.ADES = 'EDDF' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719489600) AND FROM_UNIXTIME(1719748800)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-29')\n",
      "     \n",
      "    AND oft.ADEP = 'EDDF';\n",
      "Processing: 2024-06-28 00:00:00\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719403200) AND FROM_UNIXTIME(1719662400)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-28')\n",
      "    AND oft.ADES = 'EGLL' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719403200) AND FROM_UNIXTIME(1719662400)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-28')\n",
      "     \n",
      "    AND oft.ADEP = 'EGLL';\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719403200) AND FROM_UNIXTIME(1719662400)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-28')\n",
      "    AND oft.ADES = 'EHAM' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719403200) AND FROM_UNIXTIME(1719662400)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-28')\n",
      "     \n",
      "    AND oft.ADEP = 'EHAM';\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719403200) AND FROM_UNIXTIME(1719662400)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-28')\n",
      "    AND oft.ADES = 'EDDF' \n",
      "    ;\n",
      "\n",
      "    SELECT otc.* \n",
      "    FROM (\n",
      "        SELECT *\n",
      "        FROM project_opdi.osn_tracks\n",
      "        WHERE event_time BETWEEN FROM_UNIXTIME(1719403200) AND FROM_UNIXTIME(1719662400)\n",
      "        ) otc\n",
      "    JOIN project_opdi.opdi_flight_list oft \n",
      "        ON otc.track_id = oft.id\n",
      "    WHERE oft.DOF = TO_DATE('2024-06-28')\n",
      "     \n",
      "    AND oft.ADEP = 'EDDF';\n"
     ]
    }
   ],
   "source": [
    "# Spark imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "# Regular imports\n",
    "from IPython.display import display, HTML\n",
    "import os, time,shutil\n",
    "import subprocess\n",
    "from datetime import datetime, date, timedelta\n",
    "import pandas as pd\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "# Settings\n",
    "project = \"project_opdi\"\n",
    "\n",
    "# Getting today's date\n",
    "today = datetime.today().strftime('%d %B %Y')\n",
    "\n",
    "# Setting logging properties\n",
    "#shutil.copy(\"/runtime-addons/cmladdon-2.0.40-b154/log4j.properties\", \"/etc/spark/conf/\") \n",
    "\n",
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OPDI Ingestion\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.executor.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/spark/optional-lib/iceberg-spark-runtime-3.3_2.12-1.3.1.1.20.7216.0-70.jar\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.type\", \"hive\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.iceberg.handle-timestamp-without-timezone\", \"true\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog.warehouse\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"5G\") \\\n",
    "    .config(\"spark.executor.memory\", \"16G\") \\\n",
    "    .config(\"spark.executor.memoryOverhead\", \"3G\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.instances\", \"3\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"6g\") \\\n",
    "    .config(\"spark.shuffle.compress\", \"true\") \\\n",
    "    .config(\"spark.shuffle.spill.compress\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "\n",
    "# Settings\n",
    "# Get environment variables\n",
    "engine_id = os.getenv('CDSW_ENGINE_ID')\n",
    "domain = os.getenv('CDSW_DOMAIN')\n",
    "\n",
    "# Format the URL\n",
    "url = f\"https://spark-{engine_id}.{domain}\"\n",
    "\n",
    "# Display the clickable URL\n",
    "display(HTML(f'<a href=\"{url}\">{url}</a>'))\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Radius of the Earth in kilometers\n",
    "    R = 6371.0\n",
    "    \n",
    "    # Convert latitude and longitude from degrees to radians\n",
    "    lat1_rad = math.radians(lat1)\n",
    "    lon1_rad = math.radians(lon1)\n",
    "    lat2_rad = math.radians(lat2)\n",
    "    lon2_rad = math.radians(lon2)\n",
    "    \n",
    "    # Differences in coordinates\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Haversine formula\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(lat1_rad) * math.cos(lat2_rad) * math.sin(dlon / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    \n",
    "    # Distance in kilometers\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def fetch_tracks(\n",
    "    project,\n",
    "    spark, \n",
    "    date, \n",
    "    ades, \n",
    "    adep,\n",
    "    apt, # filter params\n",
    "    apt_lat,\n",
    "    apt_lon):\n",
    "    \n",
    "    # assigned regular string date\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "    \n",
    "\n",
    "    # displaying unix timestamp after conversion\n",
    "    date_unix = int(time.mktime(date.timetuple()))\n",
    "\n",
    "    start_time_unix = date_unix - 1.5*24*60*60\n",
    "    end_time_unix = date_unix + 1.5*24*60*60\n",
    "\n",
    "    ades_sql = ''\n",
    "    adep_sql = ''\n",
    "    \n",
    "    if pd.isnull(ades):\n",
    "        ades = '*'\n",
    "    else:\n",
    "        ades_sql = f\"AND oft.ADES = '{ades}'\"\n",
    "    \n",
    "    if pd.isnull(adep):\n",
    "        adep = '*'\n",
    "    else:\n",
    "        adep_sql = f\"AND oft.ADEP = '{adep}'\"\n",
    "    \n",
    "    path = f'{adep}-to-{ades}/'\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "    fname = path + f'tracks_adep_{adep}_ades_{ades}_{date_str}.parquet'\n",
    "    \n",
    "    if os.path.exists(fname):\n",
    "        print(f'File {fname} exists already.')\n",
    "        return None\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT otc.* \n",
    "    FROM (\n",
    "        SELECT *\n",
    "        FROM {project}.osn_tracks\n",
    "        WHERE event_time BETWEEN FROM_UNIXTIME({int(start_time_unix)}) AND FROM_UNIXTIME({int(end_time_unix)})\n",
    "        ) otc\n",
    "    JOIN {project}.opdi_flight_list oft \n",
    "        ON otc.track_id = oft.id\n",
    "    WHERE oft.DOF = TO_DATE('{date_str}')\n",
    "    {ades_sql} \n",
    "    {adep_sql};\"\"\"\n",
    "    \n",
    "    df = spark.sql(query).toPandas()\n",
    "\n",
    "    df[f'distance_from_{apt}_NM'] = df.apply(lambda l: haversine(l['lat'], l['lon'], apt_lat, apt_lon), axis = 1)/1.852\n",
    "    df = df[df[f'distance_from_{apt}_NM']<200]\n",
    "    \n",
    "    df.to_parquet(fname)\n",
    "\n",
    "\n",
    "def get_all_days_between(start_date: datetime, end_date: datetime) -> List[datetime]:\n",
    "    \"\"\"\n",
    "    Given two datetimes (at 00:00:00), return a list of all days in between, including the given ones.\n",
    "    \n",
    "    Parameters:\n",
    "    start_date (datetime): The start date.\n",
    "    end_date (datetime): The end date.\n",
    "    \n",
    "    Returns:\n",
    "    List[datetime]: A list of datetime objects representing each day in the range.\n",
    "    \"\"\"\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"Start date must be before or equal to end date\")\n",
    "    \n",
    "    delta = end_date - start_date\n",
    "    all_days = [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    \n",
    "    return all_days\n",
    "\n",
    "# Processing...  \n",
    "start = datetime(2024, 3, 1)\n",
    "end = datetime(2024, 6, 30)\n",
    "days_list = get_all_days_between(start, end)\n",
    "\n",
    "days_list.sort(reverse=True)\n",
    "\n",
    "print(\"Starting ADES/ADEP: EGLL / EHAM / EDDF\")\n",
    "for date in days_list:\n",
    "    print(f\"Processing: {date}\")\n",
    "    print(\"Processing EGLL\")\n",
    "    fetch_tracks(project, spark, date, ades = 'EGLL', adep = None, apt = \"EGLL\", apt_lat = 51.4775, apt_lon = 0.04472)\n",
    "    fetch_tracks(project, spark, date, adep = 'EGLL', ades = None, apt = \"EGLL\", apt_lat = 51.4775, apt_lon = 0.04472)\n",
    "    print(\"Processing EHAM\")\n",
    "    fetch_tracks(project, spark, date, ades = 'EHAM', adep = None, apt = \"EHAM\", apt_lat = 52.3080555555555, apt_lon = 4.76416666666667)\n",
    "    fetch_tracks(project, spark, date, adep = 'EHAM', ades = None, apt = \"EHAM\", apt_lat = 52.3080555555555, apt_lon = 4.76416666666667)\n",
    "    print(\"Processing EDDF\")\n",
    "    fetch_tracks(project, spark, date, ades = 'EDDF', adep = None, apt = \"EDDF\", apt_lat = 50.0333333333333, apt_lon = 8.57055555555555)\n",
    "    fetch_tracks(project, spark, date, adep = 'EDDF', ades = None, apt = \"EDDF\", apt_lat = 50.0333333333333, apt_lon = 8.57055555555555)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5838d6e6-9e01-4327-85c7-3da3b3e08a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3486d3e4-3b6f-411e-8036-1ed08aaeba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_parquet('../retrievals/*-to-EGLL/tracks_adep_*_ades_EGLL_2024-06-30.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01d58f48-050b-44e1-ab02-9173f0b0b492",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "670"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.track_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ea1405-19ea-4638-886f-4268157e723d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c4d5b-e7c7-4c81-b105-600e9bcfbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, pandas_udf, col, PandasUDFType, lit, round, array_contains, from_unixtime\n",
    "from pyspark.sql.functions import col, radians, sin, cos, sqrt, atan2, array, collect_list, struct, row_number, expr\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, col\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "from pyspark.sql.functions import when, split, col, concat_ws,  min, max, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Regular imports\n",
    "from IPython.display import display, HTML\n",
    "import os, time\n",
    "import subprocess\n",
    "import os,shutil\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3pandas\n",
    "import h3\n",
    "\n",
    "# Settings\n",
    "project = \"project_opdi\"\n",
    "\n",
    "# Getting today's date\n",
    "today = datetime.today().strftime('%d %B %Y')\n",
    "\n",
    "# Setting logging properties\n",
    "#shutil.copy(\"/runtime-addons/cmladdon-2.0.40-b154/log4j.properties\", \"/etc/spark/conf/\") \n",
    "\n",
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OSN Flight Table\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"20\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6ac40-7f4f-42a1-ab58-1165680f0faf",
   "metadata": {},
   "source": [
    "## Query testing sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d44c7-60e3-48e6-a725-cd56b3122b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import h3_pyspark\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb13348-3b52-40e2-b69a-6544797cf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_months(start_date, end_date):\n",
    "    \"\"\"Generate a list of dates corresponding to the first day of each month between two dates.\n",
    "\n",
    "    Args:\n",
    "    start_date (datetime.date): The starting date.\n",
    "    end_date (datetime.date): The ending date.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of date objects for the first day of each month within the specified range.\n",
    "    \"\"\"\n",
    "    current = start_date\n",
    "    months = []\n",
    "    while current <= end_date:\n",
    "        months.append(current)\n",
    "        # Increment month\n",
    "        month = current.month\n",
    "        year = current.year\n",
    "        if month == 12:\n",
    "            current = date(year + 1, 1, 1)\n",
    "        else:\n",
    "            current = date(year, month + 1, 1)\n",
    "    return months\n",
    "\n",
    "def get_start_end_of_month(date):\n",
    "    \"\"\"Return a datetime object for the first and last second  of the given month and year.\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    first_second = datetime(year, month, 1, 0, 0, 0)\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    last_second = datetime(year, month, last_day, 23, 59, 59)\n",
    "    return first_second.timestamp(), last_second.timestamp()\n",
    "\n",
    "# Settings\n",
    "## Config\n",
    "project = \"project_opdi\"\n",
    "max_h3_resolution = 12\n",
    "start_month = date(2022, 1, 1)\n",
    "\n",
    "## Which months to process\n",
    "today = date.today()\n",
    "end_month = today - dateutil.relativedelta.relativedelta(months=1) # We work on the d-1 months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a85752-1ebf-480e-be6b-e59eff617133",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process_months = generate_months(start_month, end_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c9d7d-7cc8-4a25-b501-283d4ae633f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb27f7b-3e25-4e71-a7b8-0b1ce2929173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load logs\n",
    "fpath = '../../logs/02_osn-tracks-etl-log.parquet'\n",
    "processed_months = pd.read_parquet(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9ab62-2a2d-4da3-8c9c-b469c30abad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_months.sort_values('months').reset_index(drop=True).iloc[0:29].to_parquet(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c550f0e-387e-44df-8d9f-363b728c056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d317de-5e8a-4d87-abc1-db489a10f112",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc785ee-a1bf-43ed-97c7-ff943faa82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing datetime module\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "def fetch_tracks(\n",
    "    spark, \n",
    "    date, \n",
    "    ades, \n",
    "    adep):\n",
    "    # assigned regular string date\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # displaying unix timestamp after conversion\n",
    "    date_unix = int(time.mktime(date.timetuple()))\n",
    "\n",
    "    start_time_unix = date_unix - 1.5*24*60*60\n",
    "    end_time_unix = date_unix + 1.5*24*60*60\n",
    "\n",
    "    ades_sql = ''\n",
    "    adep_sql = ''\n",
    "    \n",
    "    if pd.isnull(ades):\n",
    "        ades = '*'\n",
    "    else:\n",
    "        ades_sql = f\"AND oft.ADES = '{ades}'\"\n",
    "    \n",
    "    if pd.isnull(adep):\n",
    "        adep = '*'\n",
    "    else:\n",
    "        adep_sql = f\"AND oft.ADEP = '{adep}'\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT otc.* \n",
    "    FROM (\n",
    "        SELECT *\n",
    "        FROM project_opdi.osn_tracks\n",
    "        WHERE event_time BETWEEN {start_time_unix} AND {end_time_unix}\n",
    "        ) otc\n",
    "    JOIN project_opdi.osn_flight_table oft \n",
    "        ON otc.track_id = oft.id\n",
    "    WHERE oft.DOF = TO_DATE('{date_str}')\n",
    "    {ades_sql} \n",
    "    {adep_sql};\"\"\"\n",
    "    \n",
    "    print(query)\n",
    "    \n",
    "    #df = spark.sql(query).toPandas()\n",
    "\n",
    "    path = f'{adep}-to-{ades}/'\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "    fname = path + f'tracks_ades_{ades}_adep_{adep}_{date_str}.parquet'\n",
    "    print(fname)\n",
    "    df.to_parquet(fpath)\n",
    "\n",
    "\n",
    "def get_all_days_between(start_date: datetime, end_date: datetime) -> List[datetime]:\n",
    "    \"\"\"\n",
    "    Given two datetimes (at 00:00:00), return a list of all days in between, including the given ones.\n",
    "    \n",
    "    Parameters:\n",
    "    start_date (datetime): The start date.\n",
    "    end_date (datetime): The end date.\n",
    "    \n",
    "    Returns:\n",
    "    List[datetime]: A list of datetime objects representing each day in the range.\n",
    "    \"\"\"\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"Start date must be before or equal to end date\")\n",
    "    \n",
    "    delta = end_date - start_date\n",
    "    all_days = [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    \n",
    "    return all_days\n",
    "\n",
    "# Example usage\n",
    "start = datetime(2024, 3, 1)\n",
    "end = datetime(2024, 6, 30)\n",
    "days_list = get_all_days_between(start, end)\n",
    "\n",
    "print(\"Starting ADES: LPPT\")\n",
    "for date in days_list:\n",
    "    print(f\"Processing: {date}\")\n",
    "    fetch_tracks(spark, date, ades = 'LPPT', adep = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99da59-3ed4-42f4-9145-7457665914dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4f766-0565-461d-b58c-7a2e420126c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f0db4-8e83-4cb6-8305-45cfa98f33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['event_time'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d76e5-b23e-4988-bb28-d491357b0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c61e0-05de-4cb9-a55a-df1b23ef8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07129161-1d32-4ee3-8497-0bf4c03dc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df,x='time',y='baro_altitude', color = 'track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce9665-0618-4915-855a-d5a8f93e5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df,x='lat',y='lon', color = 'track_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
