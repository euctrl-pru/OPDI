{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38c4d5b-e7c7-4c81-b105-600e9bcfbc6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark imports\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import udf, pandas_udf, col, PandasUDFType, lit, round, array_contains, from_unixtime\n",
    "from pyspark.sql.functions import col, radians, sin, cos, sqrt, atan2, array, collect_list, struct, row_number, expr\n",
    "from pyspark.sql.functions import monotonically_increasing_id, row_number, col\n",
    "from pyspark.sql.types import DoubleType, StructType, StructField\n",
    "from pyspark.sql.functions import when, split, col, concat_ws,  min, max, to_date\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "\n",
    "# Regular imports\n",
    "from IPython.display import display, HTML\n",
    "import os, time\n",
    "import subprocess\n",
    "import os,shutil\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h3pandas\n",
    "import h3\n",
    "\n",
    "# Settings\n",
    "project = \"project_opdi\"\n",
    "\n",
    "# Getting today's date\n",
    "today = datetime.today().strftime('%d %B %Y')\n",
    "\n",
    "# Setting logging properties\n",
    "#shutil.copy(\"/runtime-addons/cmladdon-2.0.40-b154/log4j.properties\", \"/etc/spark/conf/\") \n",
    "\n",
    "# Spark Session Initialization\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OSN Flight Table\") \\\n",
    "    .config(\"spark.log.level\", \"ERROR\")\\\n",
    "    .config(\"spark.hadoop.fs.azure.ext.cab.required.group\", \"eur-app-opdi\") \\\n",
    "    .config(\"spark.kerberos.access.hadoopFileSystems\", \"abfs://storage-fs@cdpdllive.dfs.core.windows.net/data/project/opdi.db/unmanaged\") \\\n",
    "    .config(\"spark.driver.cores\", \"1\") \\\n",
    "    .config(\"spark.driver.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.memory\", \"8G\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"20\") \\\n",
    "    .config(\"spark.network.timeout\", \"800s\") \\\n",
    "    .config(\"spark.executor.heartbeatInterval\", \"400s\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"4g\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Get environment variables\n",
    "engine_id = os.getenv('CDSW_ENGINE_ID')\n",
    "domain = os.getenv('CDSW_DOMAIN')\n",
    "\n",
    "# Format the URL\n",
    "url = f\"https://spark-{engine_id}.{domain}\"\n",
    "\n",
    "# Display the clickable URL\n",
    "display(HTML(f'<a href=\"{url}\">{url}</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b6ac40-7f4f-42a1-ab58-1165680f0faf",
   "metadata": {},
   "source": [
    "## Query testing sample function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3d44c7-60e3-48e6-a725-cd56b3122b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n",
    "import h3_pyspark\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, date\n",
    "import dateutil.relativedelta\n",
    "import calendar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb13348-3b52-40e2-b69a-6544797cf793",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_months(start_date, end_date):\n",
    "    \"\"\"Generate a list of dates corresponding to the first day of each month between two dates.\n",
    "\n",
    "    Args:\n",
    "    start_date (datetime.date): The starting date.\n",
    "    end_date (datetime.date): The ending date.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of date objects for the first day of each month within the specified range.\n",
    "    \"\"\"\n",
    "    current = start_date\n",
    "    months = []\n",
    "    while current <= end_date:\n",
    "        months.append(current)\n",
    "        # Increment month\n",
    "        month = current.month\n",
    "        year = current.year\n",
    "        if month == 12:\n",
    "            current = date(year + 1, 1, 1)\n",
    "        else:\n",
    "            current = date(year, month + 1, 1)\n",
    "    return months\n",
    "\n",
    "def get_start_end_of_month(date):\n",
    "    \"\"\"Return a datetime object for the first and last second  of the given month and year.\"\"\"\n",
    "    year = date.year\n",
    "    month = date.month\n",
    "    \n",
    "    first_second = datetime(year, month, 1, 0, 0, 0)\n",
    "    last_day = calendar.monthrange(year, month)[1]\n",
    "    last_second = datetime(year, month, last_day, 23, 59, 59)\n",
    "    return first_second.timestamp(), last_second.timestamp()\n",
    "\n",
    "# Settings\n",
    "## Config\n",
    "project = \"project_opdi\"\n",
    "max_h3_resolution = 12\n",
    "start_month = date(2022, 1, 1)\n",
    "\n",
    "## Which months to process\n",
    "today = date.today()\n",
    "end_month = today - dateutil.relativedelta.relativedelta(months=1) # We work on the d-1 months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a85752-1ebf-480e-be6b-e59eff617133",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process_months = generate_months(start_month, end_month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c9d7d-7cc8-4a25-b501-283d4ae633f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_process_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb27f7b-3e25-4e71-a7b8-0b1ce2929173",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load logs\n",
    "fpath = '../../logs/02_osn-tracks-etl-log.parquet'\n",
    "processed_months = pd.read_parquet(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c9ab62-2a2d-4da3-8c9c-b469c30abad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_months.sort_values('months').reset_index(drop=True).iloc[0:29].to_parquet(fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c550f0e-387e-44df-8d9f-363b728c056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "end_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc785ee-a1bf-43ed-97c7-ff943faa82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing datetime module\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List\n",
    "\n",
    "def fetch_tracks(\n",
    "    spark, \n",
    "    date, \n",
    "    ades, \n",
    "    adep):\n",
    "    # assigned regular string date\n",
    "    date_str = date.strftime('%Y-%m-%d')\n",
    "\n",
    "    # displaying unix timestamp after conversion\n",
    "    date_unix = int(time.mktime(date.timetuple()))\n",
    "\n",
    "    start_time_unix = date_unix - 1.5*24*60*60\n",
    "    end_time_unix = date_unix + 1.5*24*60*60\n",
    "\n",
    "    ades_sql = ''\n",
    "    adep_sql = ''\n",
    "    \n",
    "    if pd.isnull(ades):\n",
    "        ades = '*'\n",
    "    else:\n",
    "        ades_sql = f\"AND oft.ADES = '{ades}'\"\n",
    "    \n",
    "    if pd.isnull(adep):\n",
    "        adep = '*'\n",
    "    else:\n",
    "        adep_sql = f\"AND oft.ADEP = '{adep}'\"\n",
    "    \n",
    "    query = f\"\"\"\n",
    "    SELECT otc.* \n",
    "    FROM (\n",
    "        SELECT *\n",
    "        FROM project_opdi.osn_tracks_clustered\n",
    "        WHERE event_time BETWEEN {start_time_unix} AND {end_time_unix}\n",
    "        ) otc\n",
    "    JOIN project_opdi.osn_flight_table oft \n",
    "        ON otc.track_id = oft.id\n",
    "    WHERE oft.DOF = TO_DATE('{date_str}')\n",
    "    {ades_sql} \n",
    "    {adep_sql};\"\"\"\n",
    "    \n",
    "    print(query)\n",
    "    \n",
    "    #df = spark.sql(query).toPandas()\n",
    "\n",
    "    path = f'{adep}-to-{ades}/'\n",
    "    os.makedirs(path,exist_ok=True)\n",
    "    fname = path + f'tracks_ades_{ades}_adep_{adep}_{date_str}.parquet'\n",
    "    print(fname)\n",
    "    #df.to_parquet(fpath)\n",
    "\n",
    "\n",
    "def get_all_days_between(start_date: datetime, end_date: datetime) -> List[datetime]:\n",
    "    \"\"\"\n",
    "    Given two datetimes (at 00:00:00), return a list of all days in between, including the given ones.\n",
    "    \n",
    "    Parameters:\n",
    "    start_date (datetime): The start date.\n",
    "    end_date (datetime): The end date.\n",
    "    \n",
    "    Returns:\n",
    "    List[datetime]: A list of datetime objects representing each day in the range.\n",
    "    \"\"\"\n",
    "    if start_date > end_date:\n",
    "        raise ValueError(\"Start date must be before or equal to end date\")\n",
    "    \n",
    "    delta = end_date - start_date\n",
    "    all_days = [start_date + timedelta(days=i) for i in range(delta.days + 1)]\n",
    "    \n",
    "    return all_days\n",
    "\n",
    "# Example usage\n",
    "start = datetime(2024, 3, 1)\n",
    "end = datetime(2024, 6, 30)\n",
    "days_list = get_all_days_between(start, end)\n",
    "\n",
    "print(\"Starting ADES: LPPT\")\n",
    "for date in days_list:\n",
    "    print(f\"Processing: {date}\")\n",
    "    fetch_tracks(spark, date, ades = 'LPPT', adep = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99da59-3ed4-42f4-9145-7457665914dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b4f766-0565-461d-b58c-7a2e420126c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f0db4-8e83-4cb6-8305-45cfa98f33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df['event_time'],unit='s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556d76e5-b23e-4988-bb28-d491357b0454",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35c61e0-05de-4cb9-a55a-df1b23ef8ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(df, x='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07129161-1d32-4ee3-8497-0bf4c03dc863",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df,x='time',y='baro_altitude', color = 'track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ce9665-0618-4915-855a-d5a8f93e5b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(df,x='lat',y='lon', color = 'track_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
