import pandas as pd
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, pandas_udf, PandasUDFType, lit
from pyspark.sql.types import DoubleType, StructType, StructField
from pyspark.sql.functions import round
from IPython.display import display, HTML
import time
import os
import pandas as pd

# Hotfix
get_ipython().getoutput("cp /runtime-addons/cmladdon-2.0.40-b150/log4j.properties /etc/spark/conf/")

# Create a Spark session
spark = SparkSession\
    .builder\
    .appName("geodistance")\
    .config("spark.hadoop.fs.azure.ext.cab.required.group","eur-app-aiu-dev")\
    .config("spark.yarn.access.hadoopFileSystems","abfs://storage-fs@cdpdldev0.dfs.core.windows.net/")\
    .config("spark.driver.cores","1")\
    .config("spark.driver.memory","8G")\
    .config("spark.executor.memory","5G")\
    .config("spark.executor.cores","1")\
    .config("spark.executor.instances","2")\
    .config("spark.dynamicAllocation.maxExecutors", "6")\
    .config("spark.rpc.message.maxSize", "200")\
    .getOrCreate()

# Get environment variables
engine_id = os.getenv('CDSW_ENGINE_ID')
domain = os.getenv('CDSW_DOMAIN')

# Format the URL
url = f"https://spark-{engine_id}.{domain}"


df = spark.sql("""
    SELECT 
        icao24,
        flt_id,
        track_id,
        adep,
        adep_min_distance_km,
        cast(adep_min_distance_time as string) as adep_min_distance_time_str,
        ades,
        ades_min_distance_km,
        cast(ades_min_distance_time as string) as ades_min_distance_time_str
    FROM `project_aiu`.`osn_flight_table`
    WHERE ADEP IS NOT NULL AND ADES IS NOT NULL
    LIMIT 100000;
""")


pdf = df.toPandas()

pdf['adep_min_distance_time'] = pd.to_datetime(pdf['adep_min_distance_time_str'])
pdf['ades_min_distance_time'] = pd.to_datetime(pdf['ades_min_distance_time_str'])
pdf['time'] = pdf['ades_min_distance_time'] - pdf['adep_min_distance_time']
pdf['time'] = pd.to_timedelta(pdf['time'])
pdf['time_in_hours'] = pdf['time'].dt.total_seconds() / 3600.0


track_ids = pdf.track_id[:10].to_list()
track_ids_str = ', '.join([f"'{track_id}'" for track_id in track_ids])


# Complete the SQL query
track_df = spark.sql(f"""
    SELECT *
    FROM `project_aiu`.`osn_tracks_clustered`
    WHERE track_id IN ({track_ids_str});
""").toPandas()




