# pyspark.sql imports
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import (
    udf, pandas_udf, PandasUDFType, lit, col,
    to_timestamp, expr, from_unixtime
)
from pyspark.sql import functions as F, SparkSession, Window
from pyspark.sql.types import DoubleType, StructType, StructField, LongType

# Other imports
import pandas as pd
import numpy as np
from IPython.display import display, HTML
import time
import os
from pyspark.sql.functions import broadcast

get_ipython().getoutput("cp /runtime-addons/cmladdon-2.0.40-b150/log4j.properties /etc/spark/conf/")

# Create a Spark session
spark = SparkSession\
    .builder\
    .appName("geodistance")\
    .config("spark.hadoop.fs.azure.ext.cab.required.group","eur-app-aiu-dev")\
    .config("spark.yarn.access.hadoopFileSystems","abfs://storage-fs@cdpdldev0.dfs.core.windows.net/")\
    .config("spark.driver.cores","1")\
    .config("spark.driver.memory","10G")\
    .config("spark.executor.memory","8G")\
    .config("spark.executor.cores","1")\
    .config("spark.executor.instances","2")\
    .config("spark.dynamicAllocation.maxExecutors", "6")\
    .config("spark.rpc.message.maxSize", "200")\
    .config("spark.sql.shuffle.partitions", "400")\
    .getOrCreate()

# Get environment variables
engine_id = os.getenv('CDSW_ENGINE_ID')
domain = os.getenv('CDSW_DOMAIN')

# Format the URL
url = f"https://spark-{engine_id}.{domain}"

# Display the clickable URL
display(HTML(f'<a href="{url}">{url}</a>'))


flight_data = spark.sql(
    f"SELECT DISTINCT * FROM `project_aiu`.`airport_proximity_flight_data_clustered`;"
)
df =  flight_data.orderBy(['icao24', 'callsign', 'airport_ident', 'event_time', 'track_id']).na.drop(subset=['callsign','track_id']) # Check if both need to be na

avg_vert_rate = df.groupBy(['icao24', 'callsign', 'track_id', 'airport_ident']).agg(F.avg('vert_rate').alias('avg_vert_rate')).toPandas()
#avg_vert_rate = avg_vert_rate.withColumn('status',
#                                        F.when(avg_vert_rate['avg_vert_rate'] > 2, 'Take-off')
#                                        .when(avg_vert_rate['avg_vert_rate'] < 2, 'Landing')
#                                        .otherwise('Ambiguous'))


avg_vert_rate[avg_vert_rate.avg_vert_rate.apply(lambda l:abs(l)<20)].avg_vert_rate.hist(bins=100)


df = spark.sql("""
    SELECT 
        icao24,
        flt_id,
        track_id,
        adep,
        adep_min_distance_km,
        cast(adep_min_distance_time as string) as adep_min_distance_time_str,
        ades,
        ades_min_distance_km,
        cast(ades_min_distance_time as string) as ades_min_distance_time_str
    FROM `project_aiu`.`osn_flight_table`
    WHERE ADEP IS NOT NULL AND ADES IS NOT NULL
    LIMIT 100000;
""")


pdf = df.toPandas()

import pandas as pd

pdf['adep_min_distance_time'] = pd.to_datetime(pdf['adep_min_distance_time_str'])
pdf['ades_min_distance_time'] = pd.to_datetime(pdf['ades_min_distance_time_str'])


pdf['time']=pdf['ades_min_distance_time'] - pdf['adep_min_distance_time']








import plotly.express as px





pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) < 24].shape


pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].shape





pdf.time_in_hours.hist(bins=100)


tmp = pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24]


pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].time_in_hours.hist(bins=100)


track_ids = pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].track_id.to_list()


track_ids = pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].track_id.to_list()
track_ids_str = ', '.join([f"'{track_id}'" for track_id in track_ids])

# Complete the SQL query
track_df = spark.sql(f"""
    SELECT *
    FROM `project_aiu`.`osn_tracks_clustered`
    WHERE track_id IN ({track_ids_str});
""").toPandas()



track_df.to_parquet("dump_evening.parquet")


spark.stop()


import plotly.express as px


df.write.mode("overwrite").saveAsTable("`project_aiu`.`osn_ec_datadump_bucketed5`")





# Save the DataFrame with the new column to a temporary table
df.createOrReplaceTempView("temp_table")



# Run an SQL query to add the new column to the original table
spark.sql("""
ALTER TABLE `project_aiu`.`osn_ec_datadump_bucketed`
ADD COLUMN unique_id STRING
""")





df.write.mode("append").


fig = px.scatter(df_p_f, x="event_time", y="geo_altitude")
fig.layout.update(showlegend=False)
fig


from pyspark.sql import functions as F
from pyspark.sql import Window

# Filter out rows where callsign or icao24 are NULL
filtered_df = df.filter((F.col("callsign").isNotNull()) & (F.col("icao24").isNotNull()))

# Define a Window specification
windowSpec = Window.partitionBy("callsign", "icao24").orderBy("event_time")

# Calculate the time difference (delta) between subsequent event_time
#filtered_df = filtered_df.withColumn("time_diff", F.col("event_time") - F.lag("event_time").over(windowSpec))

# Create a column that checks if the time difference exceeds one hour (3600 seconds)
#filtered_df = filtered_df.withColumn("new_track", (F.col("time_diff") > 3600).cast("integer"))

# Generate unique IDs for tracks
#track_window = Window.orderBy("callsign", "icao24", "event_time")
#filtered_df = filtered_df.withColumn("track_id", F.sum("new_track").over(track_window))

# Now filtered_df should contain the data with calculated 'time_diff' and 'track_id'
# You can write it back to Hive or perform other operations
#filtered_df.write.mode("overwrite").saveAsTable("project_aiu.osn_ec_datadump_processed")



filtered_df.toPandas()


flight_data = spark.sql(
    f"SELECT DISTINCT * FROM `project_aiu`.`airport_proximity_flight_data` LIMIT 10000000;"
)
flight_data = flight_data.withColumn('event_time', F.from_unixtime('event_time', format='yyyy-MM-dd HH:mm:ss'))


# Get environment variables
engine_id = os.getenv('CDSW_ENGINE_ID')
domain = os.getenv('CDSW_DOMAIN')

# Format the URL
url = f"https://spark-{engine_id}.{domain}"

# Display the clickable URL
#display(HTML(f'<a href="{url}">{url}</a>'))

def load_flight_data_spark():
    """Load and preprocess flight data from Spark."""

    return flight_data.orderBy(['icao24', 'callsign', 'airport_ident', 'event_time']).na.drop(subset=['callsign'])

def categorize_by_vert_rate_spark(df):
    """Categorize the vertical rate into 'Take-off', 'Landing', or 'Ambiguous'."""
    avg_vert_rate = df.groupBy(['icao24', 'callsign', 'airport_ident']).agg(F.avg('vert_rate').alias('avg_vert_rate'))
    avg_vert_rate = avg_vert_rate.withColumn('status',
                                            F.when(avg_vert_rate['avg_vert_rate'] > 2, 'Take-off')
                                            .when(avg_vert_rate['avg_vert_rate'] < 2, 'Landing')
                                            .otherwise('Ambiguous'))
    return df.join(avg_vert_rate, on=['icao24', 'callsign', 'airport_ident'], how='left')

def compute_distance_event_times_spark(df):
    """Compute event times for min and max distances."""
    window_spec = Window.partitionBy(['icao24', 'callsign', 'airport_ident', 'status'])
    df = df.withColumn('min_distance', F.min('distance').over(window_spec))
    df = df.withColumn('max_distance', F.max('distance').over(window_spec))

    df_min = df.filter(df.distance == df.min_distance).select(
        ['icao24', 'callsign', 'airport_ident', 'status', 'event_time', 'min_distance']).withColumnRenamed('event_time', 'event_time_min_distance')
    df_max = df.filter(df.distance == df.max_distance).select(
        ['icao24', 'callsign', 'airport_ident', 'status', 'event_time', 'max_distance']).withColumnRenamed('event_time', 'event_time_max_distance')
    # df_max is superfluous and not necessary.. I could leave this out. 
    return df_min.join(df_max, on=['icao24', 'callsign', 'airport_ident', 'status'], how='inner')


def compute_flight_ids_spark(df):
    """Compute flight IDs based on time difference threshold."""
    window_spec = Window.partitionBy(['icao24', 'callsign', 'status']).orderBy('event_time_min_distance')
    df = df.withColumn('time_diff', F.lag('event_time_min_distance').over(window_spec) - df['event_time_min_distance'])
    df = df.withColumn('flight_increment', F.when(df['time_diff'] > 7200, 1).otherwise(0)) # 7200 seconds = 2 hours
    df = df.withColumn('flight_id', F.sum('flight_increment').over(window_spec) + 1)
    return df

def process_and_pivot_data_spark(df):
    """Process the dataframe to remove 'Ambiguous' rows, pivot, and select specific columns."""
    df = df.filter(df.status != "Ambiguous")
    df_pivot = df.groupBy(['icao24', 'callsign']).pivot('status').agg(
        F.first('airport_ident').alias('airport_ident'),
        F.first('min_distance').alias('min_distance'),
        F.first('event_time_min_distance').alias('event_time_min_distance'),
    )
    
    # Select and rename
    df_pivot = df_pivot.select(
        col('icao24').alias('ICAO24'),
        col('callsign').alias('FLT_ID'),
        col('Take-off_airport_ident').alias('ADEP'),
        col('Take-off_min_distance').alias('ADEP_MIN_DISTANCE_KM'),
        col('Take-off_event_time_min_distance').alias('ADEP_MIN_DISTANCE_TIME'),
        col('Landing_airport_ident').alias('ADES'),
        col('Landing_min_distance').alias('ADES_MIN_DISTANCE_KM'),
        col('Landing_event_time_min_distance').alias('ADES_MIN_DISTANCE_TIME')
    )
  
    df_pivot = df_pivot.withColumn('ADEP_MIN_DISTANCE_TIME', to_timestamp('ADEP_MIN_DISTANCE_TIME'))
    df_pivot = df_pivot.withColumn('ADES_MIN_DISTANCE_TIME', to_timestamp('ADES_MIN_DISTANCE_TIME'))

    return df_pivot

# Main process
#flight_data_spark = load_flight_data_spark()
#flight_data_spark = categorize_by_vert_rate_spark(flight_data_spark)
#grouped_df_spark = compute_distance_event_times_spark(flight_data_spark)
#df_spark = compute_flight_ids_spark(grouped_df_spark)
#df_pivot_spark = process_and_pivot_data_spark(df_spark)
#df_pivot_spark = df_pivot_spark.write.mode('overwrite').insertInto(f"project_aiu.osn_flight_table")


flight_data_spark = load_flight_data_spark()


flight_data_spark.show()


flight_data_spark = categorize_by_vert_rate_spark(flight_data_spark)


flight_data_spark.show()


import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, pandas_udf, PandasUDFType, lit
from pyspark.sql.types import DoubleType, StructType, StructField
import numpy as np
from IPython.display import display, HTML
import time
import os

# Create a Spark session
spark = SparkSession\
    .builder\
    .appName("geodistance")\
    .config("spark.hadoop.fs.azure.ext.cab.required.group","eur-app-aiu-dev")\
    .config("spark.kerberos.access.hadoopFileSystems","abfs://storage-fs@cdpdldev0.dfs.core.windows.net/")\
    .config("spark.driver.cores","1")\
    .config("spark.driver.memory","8G")\
    .config("spark.executor.memory","5G")\
    .config("spark.executor.cores","1")\
    .config("spark.executor.instances","2")\
    .config("spark.dynamicAllocation.maxExecutors", "6")\
    .getOrCreate()
osn_flight_table = spark.table("project_aiu.osn_flight_table").dropDuplicates().toPandas()



osn_flight_table['flight_time'] = osn_flight_table.ades_min_distance_time - osn_flight_table.adep_min_distance_time
osn_flight_table['flight_time'] = osn_flight_table.flight_time.dt.total_seconds()/60/60

oft = osn_flight_table.copy()
oft['adep2'] = oft['adep'].str[:2]
oft['ades2'] = oft['ades'].str[:2]
median_flight_time = oft.groupby(['adep2','ades2']).median()['flight_time'].reset_index()
median_flight_time.rename({"flight_time":"median_flight_time"},axis=1,inplace=True)


oft[np.logical_and.reduce(
    [
        abs(oft.flight_time) <= 24, 
        oft.flight_time > 0
    ])]


oft[~oft.flight_time.isna()].flight_time.hist(bins=100)


oft = oft.merge(median_flight_time, on = ['adep2','ades2'], how='left')


oft['diff_ft'] =  oft.flight_time - oft.median_flight_time


oft = oft[oft.diff_ft <= 5]


oft.adep_min


osn_flight_table[osn_flight_table.flight_time > 10]





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
from traffic.data import opensky
from traffic.core import Traffic


airport_bbox = pd.read_csv("APT_BBOX.csv")
airport_info = pd.read_csv("airports.csv")
airport_boxes = pd.merge(airport_bbox, airport_info, left_on='AIRPORT', right_on='icao')
sensor_info = pd.read_csv('sensors_2023-02-23.csv')
print(len(airport_bbox))
print(len(airport_info))
print(len(airport_boxes))


start_time = "2021-09-01 11:00"
end_time = "2021-09-01 13:00"


airports_bounds={}
for index, row in airport_boxes.iterrows():
    airport_code = row['AIRPORT']
    bound_left_x = row['BoundLeftX']
    bound_left_y = row['BoundLeftY']
    bound_right_x = row['BoundRightX']
    bound_right_y = row['BoundRightY']
    altitude = row["elevation_ft"]
    airports_bounds[airport_code] = ((bound_left_x, bound_left_y,bound_right_x, bound_right_y),altitude)


map = Basemap(llcrnrlon=-10, llcrnrlat=35, urcrnrlon=40, urcrnrlat=70, resolution='l')

map.drawcoastlines()
map.drawcountries()
airport_boxes = pd.read_csv("APT_BBOX.csv")

for index, row in airport_boxes.iterrows():
    bound_left_x = row['BoundLeftX']
    bound_left_y = row['BoundLeftY']
    bound_right_x = row['BoundRightX']
    bound_right_y = row['BoundRightY']
    x = [bound_left_x, bound_right_x, bound_right_x, bound_left_x, bound_left_x]
    y = [bound_left_y, bound_left_y, bound_right_y, bound_right_y, bound_left_y]
    map.plot(x, y, latlon=True, color='r')
plt.show()


data={}
for key,value in airports_bounds.items():
    flights = opensky.history(start_time,end_time,bounds=airports_bounds[key][0])
    if flights is not None : 
        data[key]= flights.data
print(data.keys())


data['EBBR']


for key, flights in data.items():
    if flights is not None:
        dataset = flights.copy()
        max_altitude = 500 + airports_bounds[key][1]*0.3048
        dataset = dataset[dataset['altitude'] < max_altitude ]
        dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])
        dataset['last_position'] = pd.to_datetime(dataset['last_position'])
        time_diff = (dataset['timestamp'] - dataset['last_position']).dt.total_seconds()
        dataset = dataset[(1 <= time_diff) & (time_diff <= 5)]
        callsigns = dataset['callsign'].unique()
        filtered_data = pd.DataFrame()
        for callsign in callsigns:
            subset = dataset[dataset['callsign'] == callsign]
            if subset['altitude'].min() < 30 + airports_bounds[key][1] * 0.3048:
                filtered_subset = subset.copy()
            else:
                filtered_subset = pd.DataFrame()
            filtered_data = pd.concat([filtered_data, filtered_subset])
        filtered_data.reset_index(drop=True, inplace=True)
        if 'callsign' in filtered_data.columns : 
            points_count = filtered_data['callsign'].value_counts()
            points_mean = points_count.mean()
            data_final = filtered_data.groupby('callsign').filter(lambda x: len(x) >= points_mean)
            data[key] = data_final
        else:
            data[key]=None



data['EBBR']


from geopy.distance import geodesic

def calculate_distances_trajectories(dataset):
    flight_distance_points = {}
    for flight in dataset['callsign'].unique():
        trajectory = dataset[dataset['callsign'] == flight].sort_values('timestamp')
        distances_points = []
        for i in range(1, len(trajectory)):
            point_prec = trajectory.iloc[i-1]
            point_actuel = trajectory.iloc[i]
            coord_prec = (point_prec['latitude'], point_prec['longitude'])
            coord_actuel = (point_actuel['latitude'], point_actuel['longitude'])
            diff_altitude = np.abs(point_actuel['altitude']-point_prec['altitude'])
            distance = geodesic(coord_prec, coord_actuel).meters
            distances_points.append(np.sqrt(distance**2+diff_altitude**2))
        flight_distance_points[flight]=distances_points
    return flight_distance_points



ebbr = calculate_distances_trajectories(data['EBBR'])


data['EBBR'][data['EBBR']['callsign']=='BEL3736']


ebbr['BEL3736']



def coverage_count(airports_bounds, start_time, end_time, max_height, data):
    airports_covered = {}
    airports_not_covered = {}
    for key in airports_bounds.keys():
        #flights = opensky.history(start_time, end_time, bounds=airports_bounds[key][0])
        flights = data[key]
        if flights is not None:
            max_altitude = max_height + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            if flights_airport is not None:
                airports_covered[key] = flights_airport.data.shape[0]
            else:
                #airports_not_covered[key] = 0
                airports_covered[key] = 0
                
    airports_covered_final = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True))            
    airports_covered = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(airports_covered)), airports_covered.values(), align='center')
    plt.xticks(range(len(airports_covered)), airports_covered.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Number of points with height <= 500 m')
    plt.show()
    return airports_covered_final




def mean_coverage(airports_bounds, start_time, end_time, data):
    airports_coverage = {}
    for key in airports_bounds.keys():
        flights = data[key]
        if flights is not None:
            max_altitude = 500 + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            flights


from geopy import distance

def surface(bounds):
    latitudeleft,latituderight, longitudeleft,longituderight = bounds[0],bounds[1],bounds[2],bounds[3]
    point_haut_droit = (latituderight, longituderight)
    point_bas_gauche = (latitudeleft, longitudeleft)
    distance_geodesique = distance.distance(point_haut_droit, point_bas_gauche).meters
    surface_km2 = (distance_geodesique / 1000) ** 2
    return surface_km2

def coverage_count_surface(airports_bounds,start_time,end_time, max_height, data):
    airports_covered={}
    airports_not_covered={}
    for key in airports_bounds.keys():
        #flights = opensky.history(start_time,end_time,bounds=airports_bounds[key][0],)
        flights = data[key]
        if flights is not None:
            max_altitude = max_height + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            if flights_airport is not None:
                bounds = airports_bounds[key][0]
                airport_area = surface(bounds)
                airports_covered[key] = flights_airport.data.shape[0] / airport_area
            else:
                #airports_not_covered[key] = 0
                airports_covered[key] = 0
    airports_covered_final = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True))
    airports_covered = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(airports_covered)), airports_covered.values(), align='center')
    plt.xticks(range(len(airports_covered)), airports_covered.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Number of points / surface      with height <= 500 m')
    plt.show()
    return airports_covered_final



def calculate_entropy(proportions):
    entropy = 0.0
    for proportion in proportions:
        if proportion > 0:
            entropy -= proportion * np.log2(proportion)
    return entropy

def calculate_subbox_proportions(airport_flights, subboxes):
    subbox_counts = np.zeros(len(subboxes), dtype=int)
    for flight in airport_flights.iterrows():
        for i, subbox in enumerate(subboxes):
            if subbox[0][0]<flight[1][9]<subbox[1][0] and subbox[0][1]<flight[1][8]<subbox[1][1]:
                subbox_counts[i] += 1
                break
    total_count = len(airport_flights)
    proportions = subbox_counts / total_count
    return proportions

def calculate_spatial_entropy(airports_bounds, start_time, end_time, max_height, data, num_subboxes_per_dimension,):
    airports_entropy = {}
    for airport, bounds in airports_bounds.items():
        subboxes = []
        lon_step = (bounds[0][3] - bounds[0][1]) / num_subboxes_per_dimension
        lat_step = (bounds[0][2] - bounds[0][0]) / num_subboxes_per_dimension
        for i in range(num_subboxes_per_dimension):
            for j in range(num_subboxes_per_dimension):
                subbox = [[bounds[0][0] + i * lon_step, bounds[0][1] + j * lat_step],
                      [bounds[0][0] + (i+1) * lon_step, bounds[0][1] + (j+1) * lat_step]]
                subboxes.append(subbox)
        flights = data[airport]
        if flights is not None:
            max_altitude = max_height + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            if flights_airport is not None:
                proportions = calculate_subbox_proportions(flights_airport.data, subboxes)
                entropy = calculate_entropy(proportions)
                airports_entropy[airport] = entropy
            else:
                airports_entropy[airport] = 0.0
                
    airports_entropy_final = dict(sorted(airports_entropy.items(), key=lambda x: x[1], reverse=True))
    airports_entropy = dict(sorted(airports_entropy.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(airports_entropy)), airports_entropy.values(), align='center')
    plt.xticks(range(len(airports_entropy)), airports_entropy.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Spatial Entropy')
    plt.show()
    return airports_entropy_final


count         = coverage_count(airports_bounds,start_time,end_time,data)
count_surface = coverage_count_surface(airports_bounds,start_time,end_time,data)
entropy       = calculate_spatial_entropy(airports_bounds, start_time, end_time,data, 10)


def normalization(dict):
    max_value = max(list(dict.values()))
    for key,value in dict.items():
        if max_value!=0:
            dict[key]=value/max_value
        else:
            dict[key]=0
    return dict

def score(dict1,dict2,dict3):
    scores={}
    d1,d2,d3 = normalization(dict1),normalization(dict2),normalization(dict3)
    w1 = 0.3
    w2= 0.3
    w3 = 0.4
    for key in dict1.keys():
        if d1[key]==0 or d1[key]==0 or d1[key]==0 : 
            scores[key] = 0
        else:
            scores[key]= w1 * d1[key] + w2*d2[key] + w3*d3[key]
    scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))
    score_ = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(score_)), score_.values(), align='center')
    plt.xticks(range(len(score_)), score_.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Score')
    plt.show()
    return scores

score(count,count_surface,entropy)
        


def calculate_distance(coord1, coord2):
    distance_geodesique = distance.distance(coord1, coord2).meters
    return distance_geodesique/1000

sensors ={}
sensor_info = sensor_info.dropna()
for index, row in sensor_info.iterrows():
    sensor_id = row['id']
    latitude = row['latitude']
    longitude = row['longitude']
    if -90 <= latitude <= 90: 
        sensors[sensor_id] = (latitude, longitude)
    
airports = {}
for index, row in airport_info.iterrows():
    icao = row['icao']
    latitude = row['latitude']
    longitude = row['longitude']
    airports[icao] =(latitude, longitude)

            
nearest_sensors={}
number_sensors={}
for icao, airport_position in airports.items():
    nearest_sensors[icao]=[]
    for sensor_id, sensor_position in sensors.items():
        distance_km = calculate_distance(airport_position, sensor_position)
        if distance_km < 10:
            nearest_sensors[icao].append(sensor_id)
    number_sensors[icao]=len(nearest_sensors[icao])
    


print(dict(sorted(number_sensors.items(), key=lambda x: x[1], reverse=True)))


data['LSZH'].data.isna().sum()


