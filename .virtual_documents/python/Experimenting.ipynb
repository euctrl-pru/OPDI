#import h3_pyspark
from pyspark.sql.functions import explode, col
#from h3_pyspark import h3_distance
import numpy as np


import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, pandas_udf, PandasUDFType, lit
from pyspark.sql.types import DoubleType, StructType, StructField
import numpy as np
from IPython.display import display, HTML
import time
import os
import shutil
shutil.copy("/runtime-addons/cmladdon-2.0.40-b150/log4j.properties", "/etc/spark/conf/")

# Create a Spark session
spark = SparkSession\
    .builder\
    .appName("geodistance")\
    .config("spark.hadoop.fs.azure.ext.cab.required.group","eur-app-aiu-dev")\
    .config("spark.kerberos.access.hadoopFileSystems","abfs://storage-fs@cdpdldev0.dfs.core.windows.net/")\
    .config("spark.driver.cores","1")\
    .config("spark.driver.memory","8G")\
    .config("spark.executor.memory","5G")\
    .config("spark.executor.cores","1")\
    .config("spark.executor.instances","2")\
    .config("spark.dynamicAllocation.maxExecutors", "6")\
    .config("spark.rpc.message.maxSize", "200")\
    .getOrCreate()

# Get environment variables
engine_id = os.getenv('CDSW_ENGINE_ID')
domain = os.getenv('CDSW_DOMAIN')

# Format the URL
url = f"https://spark-{engine_id}.{domain}"

# Display the clickable URL
display(HTML(f'<a href="{url}">{url}</a>'))





create_db_sql = """
CREATE TABLE osn_ec_data (
  event_time BIGINT COMMENT 'This column contains the unix (aka POSIX or epoch) timestamp for which the state vector was valid.',
  icao24 STRING COMMENT 'This column contains the 24-bit ICAO transponder ID which can be used to track specific airframes over different flights.',
  lat DOUBLE COMMENT 'This column contains the last known latitude of the aircraft.',
  lon DOUBLE COMMENT 'This column contains the last known longitude of the aircraft.',
  velocity DOUBLE COMMENT 'This column contains the speed over ground of the aircraft in meters per second.',
  heading DOUBLE COMMENT 'This column represents the direction of movement (track angle in degrees) as the clockwise angle from the geographic north.',
  vert_rate DOUBLE COMMENT 'This column contains the vertical speed of the aircraft in meters per second.',
  callsign STRING COMMENT 'This column contains the callsign that was broadcast by the aircraft.',
  on_ground BOOLEAN COMMENT 'This flag indicates whether the aircraft is broadcasting surface positions (true) or airborne positions (false).',
  alert BOOLEAN COMMENT 'This flag is a special indicator used in ATC.',
  spi BOOLEAN COMMENT 'This flag is a special indicator used in ATC.',
  squawk STRING COMMENT 'This 4-digit octal number is another transponder code which is used by ATC and pilots for identification purposes and indication of emergencies.',
  baro_altitude DOUBLE COMMENT 'This column indicates the aircrafts altitude. As the names suggest, baroaltitude is the altitude measured by the barometer (in meter).',
  geo_altitude DOUBLE COMMENT 'This column indicates the aircrafts altitude. As the names suggest, geoaltitude is determined using the GNSS (GPS) sensor (in meter).',
  last_pos_update DOUBLE COMMENT 'This unix timestamp indicates the age of the position.',
  last_contact DOUBLE COMMENT 'This unix timestamp indicates the time at which OpenSky received the last signal of the aircraft.',
  serials ARRAY<INT> COMMENT 'The serials column is a list of serials of the ADS-B receivers which received the message.'
)
COMMENT 'OpenSky Network EUROCONTROL datadump (for PRU) - periodically updated.'
STORED AS parquet
TBLPROPERTIES ('transactional'='false');
"""

spark.sql(create_db_sql)


airports_df = spark.sql("""
    SELECT ident, latitude_deg, longitude_deg, elevation_ft, type
    FROM project_aiu.oa_airports
    WHERE (type = 'large_airport' or type = 'medium_airport');
""")


df = airports_df.toPandas()


resolution = 9 #h3 resolution
avg_edge_length_resolution_km = 0.200786148 #https://h3geo.org/docs/core-library/restable/
distance_max_ring = 10 #km

avg_inner_radius_hexagon_km = np.sqrt(3)*avg_edge_length_resolution_km/2
avg_distance_center_next_ring_km = avg_inner_radius_hexagon_km*2
approx_number_of_rings = np.ceil(distance_max_ring/avg_distance_center_next_ring_km)


center_cell_name = f"h3_{resolution}_center_cell_id"
area_cell_name = f"h3_{resolution}_area_cell_id"

airports_df = airports_df.withColumn("resolution", lit(resolution))

airports_df = airports_df.withColumn(center_cell_name, h3_pyspark.geo_to_h3('latitude_deg', 'longitude_deg', 'resolution'))

airports_df = airports_df.withColumn(area_cell_name, h3_pyspark.k_ring(center_cell_name, lit(approx_number_of_rings)))

# Explode the h3_9_buffer column
airports_df = airports_df.withColumn(area_cell_name, explode(col(area_cell_name)))

# Filter rows where h3_9 is not equal to h3_9_buffer
airports_df = airports_df.filter(col(center_cell_name) != col(area_cell_name))

airports_df = airports_df.withColumn("distance_center_to_area_cell", h3_pyspark.h3_distance(center_cell_name, area_cell_name))

airports_pdf = airports_df.toPandas()


airports_pdf.to_csv('test.csv')





get_ipython().run_line_magic('pinfo',  'hex_range_distances')


airports_pdf








airports_pdf['ring_number'] = list(range(0,number_of_rings+1))*airports_pdf['h3_9'].nunique()


airports_pdf


airports_pdf['h3_9_buffer'][0]


from h3_pyspark import k_ring_distances


airports_pdf




def create_local_grid_spark(lat_center, lon_center, lat_step_deg=0.0001, lon_step_deg=0.0001, radius_km=10, airport_ident=None):
    lat_step = lat_step_deg
    lon_step = lon_step_deg

    # Determine the number of decimal places in lat_step_deg and lon_step_deg
    lat_decimal_places = len(str(lat_step_deg).split('.')[1])
    lon_decimal_places = len(str(lon_step_deg).split('.')[1])

    lat_step_m = lat_step * 111.2 * 1000 # approximately 111.2km per degree
    lon_step_m = lon_step * 111.2 * 1000 * np.cos(np.radians(lat_center)) # the size of a degree of longitude depends on the latitude

    print(f"Each latitude step is about {lat_step_m} meters.")
    print(f"Each longitude step is about {lon_step_m} meters.")

    num_lat_points = int(2 * radius_km / (lat_step_m / 1000)) # since radius is in km
    num_lon_points = int(2 * radius_km / (lon_step_m / 1000)) # since radius is in km

    df = spark.range(num_lat_points).select(F.col("id").alias("lat_index")).crossJoin(
        spark.range(num_lon_points).select(F.col("id").alias("lon_index"))
    )
    
    df = df.withColumn("lat", F.round(lat_center + (F.col("lat_index") - num_lat_points/2) * lat_step, lat_decimal_places)).withColumn(
        "lon", F.round(lon_center + (F.col("lon_index") - num_lon_points/2) * lon_step, lon_decimal_places)
    ).drop("lat_index", "lon_index")

    if airport_ident is not None:
        df = df.withColumn("airport_ident", F.lit(airport_ident))
        
    return df


spark = SparkSession.builder.getOrCreate()



from pyspark.sql import functions as F

def spark_haversine(lat1, lon1, lat2, lon2):
    # Convert all inputs to radians
    lat1, lon1, lat2, lon2 = [x * F.lit(np.pi / 180) for x in [lat1, lon1, lat2, lon2]]
    
    dlon = lon2 - lon1
    dlat = lat2 - lat1
    
    a = F.sin(dlat / 2)**2 + F.cos(lat1) * F.cos(lat2) * F.sin(dlon / 2)**2
    c = 2 * F.asin(F.sqrt(a))
    r = 6371  # Radius of earth in kilometers
    return c * r

for row in airports_df.rdd.collect():
    print(f"Processing [{row['ident']}].")
    start_time = time.time()
    grid_df = create_local_grid_spark(row['latitude_deg'], row['longitude_deg'], lat_step_deg=0.001, lon_step_deg=0.001, radius_km=1.852*100, airport_ident=row['ident'])

    # Instead of using literals, add the central latitude and longitude as new columns
    grid_df = grid_df.withColumn('central_lat', F.lit(row['latitude_deg']))
    grid_df = grid_df.withColumn('central_lon', F.lit(row['longitude_deg']))

    # Now apply the haversine function using these new columns
    grid_df = grid_df.withColumn("distance", spark_haversine(grid_df['central_lat'], grid_df['central_lon'], grid_df['lat'], grid_df['lon']))

    # Drop the 'central_lat' and 'central_lon' columns
    grid_df = grid_df.drop('central_lat', 'central_lon')
    
    # Write the grid dataframe to the table
    grid_df.write.insertInto("project_aiu.airport_distance_reference", overwrite=False)
    
    end_time = time.time()

    execution_time = end_time - start_time
    print(f"The create_local_grid for [{row['ident']}] execution took [{execution_time} seconds].")


# pyspark.sql imports
from pyspark.sql import SparkSession, Window
from pyspark.sql.functions import (
    udf, pandas_udf, PandasUDFType, lit, col,
    to_timestamp, expr, from_unixtime
)
from pyspark.sql import functions as F, SparkSession, Window
from pyspark.sql.types import DoubleType, StructType, StructField, LongType

# Other imports
import pandas as pd
import numpy as np
from IPython.display import display, HTML
import time
import os
from pyspark.sql.functions import broadcast

get_ipython().getoutput("cp /runtime-addons/cmladdon-2.0.40-b150/log4j.properties /etc/spark/conf/")

# Create a Spark session
spark = SparkSession\
    .builder\
    .appName("geodistance")\
    .config("spark.hadoop.fs.azure.ext.cab.required.group","eur-app-aiu-dev")\
    .config("spark.yarn.access.hadoopFileSystems","abfs://storage-fs@cdpdldev0.dfs.core.windows.net/")\
    .config("spark.driver.cores","1")\
    .config("spark.driver.memory","10G")\
    .config("spark.executor.memory","8G")\
    .config("spark.executor.cores","1")\
    .config("spark.executor.instances","2")\
    .config("spark.dynamicAllocation.maxExecutors", "6")\
    .config("spark.rpc.message.maxSize", "200")\
    .config("spark.sql.shuffle.partitions", "400")\
    .getOrCreate()

# Get environment variables
engine_id = os.getenv('CDSW_ENGINE_ID')
domain = os.getenv('CDSW_DOMAIN')

# Format the URL
url = f"https://spark-{engine_id}.{domain}"

# Display the clickable URL
display(HTML(f'<a href="{url}">{url}</a>'))


flight_data = spark.sql(
    f"SELECT DISTINCT * FROM `project_aiu`.`airport_proximity_flight_data_clustered`;"
)
df =  flight_data.orderBy(['icao24', 'callsign', 'airport_ident', 'event_time', 'track_id']).na.drop(subset=['callsign','track_id']) # Check if both need to be na

avg_vert_rate = df.groupBy(['icao24', 'callsign', 'track_id', 'airport_ident']).agg(F.avg('vert_rate').alias('avg_vert_rate')).toPandas()
#avg_vert_rate = avg_vert_rate.withColumn('status',
#                                        F.when(avg_vert_rate['avg_vert_rate'] > 2, 'Take-off')
#                                        .when(avg_vert_rate['avg_vert_rate'] < 2, 'Landing')
#                                        .otherwise('Ambiguous'))


avg_vert_rate[avg_vert_rate.avg_vert_rate.apply(lambda l:abs(l)<20)].avg_vert_rate.hist(bins=100)


df = spark.sql("""
    SELECT 
        icao24,
        flt_id,
        track_id,
        adep,
        adep_min_distance_km,
        cast(adep_min_distance_time as string) as adep_min_distance_time_str,
        ades,
        ades_min_distance_km,
        cast(ades_min_distance_time as string) as ades_min_distance_time_str
    FROM `project_aiu`.`osn_flight_table`
    WHERE ADEP IS NOT NULL AND ADES IS NOT NULL
    LIMIT 100000;
""")


pdf = df.toPandas()

import pandas as pd

pdf['adep_min_distance_time'] = pd.to_datetime(pdf['adep_min_distance_time_str'])
pdf['ades_min_distance_time'] = pd.to_datetime(pdf['ades_min_distance_time_str'])


pdf['time']=pdf['ades_min_distance_time'] - pdf['adep_min_distance_time']








import plotly.express as px





pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) < 24].shape


pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].shape





pdf.time_in_hours.hist(bins=100)


tmp = pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24]


pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].time_in_hours.hist(bins=100)


track_ids = pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].track_id.to_list()


track_ids = pdf[pdf['time_in_hours'].apply(lambda l:abs(l)) > 24].track_id.to_list()
track_ids_str = ', '.join([f"'{track_id}'" for track_id in track_ids])

# Complete the SQL query
track_df = spark.sql(f"""
    SELECT *
    FROM `project_aiu`.`osn_tracks_clustered`
    WHERE track_id IN ({track_ids_str});
""").toPandas()



track_df.to_parquet("dump_evening.parquet")


spark.stop()


import plotly.express as px


df.write.mode("overwrite").saveAsTable("`project_aiu`.`osn_ec_datadump_bucketed5`")





# Save the DataFrame with the new column to a temporary table
df.createOrReplaceTempView("temp_table")



# Run an SQL query to add the new column to the original table
spark.sql("""
ALTER TABLE `project_aiu`.`osn_ec_datadump_bucketed`
ADD COLUMN unique_id STRING
""")





df.write.mode("append").


fig = px.scatter(df_p_f, x="event_time", y="geo_altitude")
fig.layout.update(showlegend=False)
fig


from pyspark.sql import functions as F
from pyspark.sql import Window

# Filter out rows where callsign or icao24 are NULL
filtered_df = df.filter((F.col("callsign").isNotNull()) & (F.col("icao24").isNotNull()))

# Define a Window specification
windowSpec = Window.partitionBy("callsign", "icao24").orderBy("event_time")

# Calculate the time difference (delta) between subsequent event_time
#filtered_df = filtered_df.withColumn("time_diff", F.col("event_time") - F.lag("event_time").over(windowSpec))

# Create a column that checks if the time difference exceeds one hour (3600 seconds)
#filtered_df = filtered_df.withColumn("new_track", (F.col("time_diff") > 3600).cast("integer"))

# Generate unique IDs for tracks
#track_window = Window.orderBy("callsign", "icao24", "event_time")
#filtered_df = filtered_df.withColumn("track_id", F.sum("new_track").over(track_window))

# Now filtered_df should contain the data with calculated 'time_diff' and 'track_id'
# You can write it back to Hive or perform other operations
#filtered_df.write.mode("overwrite").saveAsTable("project_aiu.osn_ec_datadump_processed")



filtered_df.toPandas()


flight_data = spark.sql(
    f"SELECT DISTINCT * FROM `project_aiu`.`airport_proximity_flight_data` LIMIT 10000000;"
)
flight_data = flight_data.withColumn('event_time', F.from_unixtime('event_time', format='yyyy-MM-dd HH:mm:ss'))


# Get environment variables
engine_id = os.getenv('CDSW_ENGINE_ID')
domain = os.getenv('CDSW_DOMAIN')

# Format the URL
url = f"https://spark-{engine_id}.{domain}"

# Display the clickable URL
#display(HTML(f'<a href="{url}">{url}</a>'))

def load_flight_data_spark():
    """Load and preprocess flight data from Spark."""

    return flight_data.orderBy(['icao24', 'callsign', 'airport_ident', 'event_time']).na.drop(subset=['callsign'])

def categorize_by_vert_rate_spark(df):
    """Categorize the vertical rate into 'Take-off', 'Landing', or 'Ambiguous'."""
    avg_vert_rate = df.groupBy(['icao24', 'callsign', 'airport_ident']).agg(F.avg('vert_rate').alias('avg_vert_rate'))
    avg_vert_rate = avg_vert_rate.withColumn('status',
                                            F.when(avg_vert_rate['avg_vert_rate'] > 2, 'Take-off')
                                            .when(avg_vert_rate['avg_vert_rate'] < 2, 'Landing')
                                            .otherwise('Ambiguous'))
    return df.join(avg_vert_rate, on=['icao24', 'callsign', 'airport_ident'], how='left')

def compute_distance_event_times_spark(df):
    """Compute event times for min and max distances."""
    window_spec = Window.partitionBy(['icao24', 'callsign', 'airport_ident', 'status'])
    df = df.withColumn('min_distance', F.min('distance').over(window_spec))
    df = df.withColumn('max_distance', F.max('distance').over(window_spec))

    df_min = df.filter(df.distance == df.min_distance).select(
        ['icao24', 'callsign', 'airport_ident', 'status', 'event_time', 'min_distance']).withColumnRenamed('event_time', 'event_time_min_distance')
    df_max = df.filter(df.distance == df.max_distance).select(
        ['icao24', 'callsign', 'airport_ident', 'status', 'event_time', 'max_distance']).withColumnRenamed('event_time', 'event_time_max_distance')
    # df_max is superfluous and not necessary.. I could leave this out. 
    return df_min.join(df_max, on=['icao24', 'callsign', 'airport_ident', 'status'], how='inner')


def compute_flight_ids_spark(df):
    """Compute flight IDs based on time difference threshold."""
    window_spec = Window.partitionBy(['icao24', 'callsign', 'status']).orderBy('event_time_min_distance')
    df = df.withColumn('time_diff', F.lag('event_time_min_distance').over(window_spec) - df['event_time_min_distance'])
    df = df.withColumn('flight_increment', F.when(df['time_diff'] > 7200, 1).otherwise(0)) # 7200 seconds = 2 hours
    df = df.withColumn('flight_id', F.sum('flight_increment').over(window_spec) + 1)
    return df

def process_and_pivot_data_spark(df):
    """Process the dataframe to remove 'Ambiguous' rows, pivot, and select specific columns."""
    df = df.filter(df.status != "Ambiguous")
    df_pivot = df.groupBy(['icao24', 'callsign']).pivot('status').agg(
        F.first('airport_ident').alias('airport_ident'),
        F.first('min_distance').alias('min_distance'),
        F.first('event_time_min_distance').alias('event_time_min_distance'),
    )
    
    # Select and rename
    df_pivot = df_pivot.select(
        col('icao24').alias('ICAO24'),
        col('callsign').alias('FLT_ID'),
        col('Take-off_airport_ident').alias('ADEP'),
        col('Take-off_min_distance').alias('ADEP_MIN_DISTANCE_KM'),
        col('Take-off_event_time_min_distance').alias('ADEP_MIN_DISTANCE_TIME'),
        col('Landing_airport_ident').alias('ADES'),
        col('Landing_min_distance').alias('ADES_MIN_DISTANCE_KM'),
        col('Landing_event_time_min_distance').alias('ADES_MIN_DISTANCE_TIME')
    )
  
    df_pivot = df_pivot.withColumn('ADEP_MIN_DISTANCE_TIME', to_timestamp('ADEP_MIN_DISTANCE_TIME'))
    df_pivot = df_pivot.withColumn('ADES_MIN_DISTANCE_TIME', to_timestamp('ADES_MIN_DISTANCE_TIME'))

    return df_pivot

# Main process
#flight_data_spark = load_flight_data_spark()
#flight_data_spark = categorize_by_vert_rate_spark(flight_data_spark)
#grouped_df_spark = compute_distance_event_times_spark(flight_data_spark)
#df_spark = compute_flight_ids_spark(grouped_df_spark)
#df_pivot_spark = process_and_pivot_data_spark(df_spark)
#df_pivot_spark = df_pivot_spark.write.mode('overwrite').insertInto(f"project_aiu.osn_flight_table")


flight_data_spark = load_flight_data_spark()


flight_data_spark.show()


flight_data_spark = categorize_by_vert_rate_spark(flight_data_spark)


flight_data_spark.show()


import pandas as pd
from pyspark.sql import SparkSession
from pyspark.sql.functions import udf, pandas_udf, PandasUDFType, lit
from pyspark.sql.types import DoubleType, StructType, StructField
import numpy as np
from IPython.display import display, HTML
import time
import os

# Create a Spark session
spark = SparkSession\
    .builder\
    .appName("geodistance")\
    .config("spark.hadoop.fs.azure.ext.cab.required.group","eur-app-aiu-dev")\
    .config("spark.kerberos.access.hadoopFileSystems","abfs://storage-fs@cdpdldev0.dfs.core.windows.net/")\
    .config("spark.driver.cores","1")\
    .config("spark.driver.memory","8G")\
    .config("spark.executor.memory","5G")\
    .config("spark.executor.cores","1")\
    .config("spark.executor.instances","2")\
    .config("spark.dynamicAllocation.maxExecutors", "6")\
    .getOrCreate()
osn_flight_table = spark.table("project_aiu.osn_flight_table").dropDuplicates().toPandas()



osn_flight_table['flight_time'] = osn_flight_table.ades_min_distance_time - osn_flight_table.adep_min_distance_time
osn_flight_table['flight_time'] = osn_flight_table.flight_time.dt.total_seconds()/60/60

oft = osn_flight_table.copy()
oft['adep2'] = oft['adep'].str[:2]
oft['ades2'] = oft['ades'].str[:2]
median_flight_time = oft.groupby(['adep2','ades2']).median()['flight_time'].reset_index()
median_flight_time.rename({"flight_time":"median_flight_time"},axis=1,inplace=True)


oft[np.logical_and.reduce(
    [
        abs(oft.flight_time) <= 24, 
        oft.flight_time > 0
    ])]


oft[~oft.flight_time.isna()].flight_time.hist(bins=100)


oft = oft.merge(median_flight_time, on = ['adep2','ades2'], how='left')


oft['diff_ft'] =  oft.flight_time - oft.median_flight_time


oft = oft[oft.diff_ft <= 5]


oft.adep_min


osn_flight_table[osn_flight_table.flight_time > 10]





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.basemap import Basemap
from traffic.data import opensky
from traffic.core import Traffic


airport_bbox = pd.read_csv("APT_BBOX.csv")
airport_info = pd.read_csv("airports.csv")
airport_boxes = pd.merge(airport_bbox, airport_info, left_on='AIRPORT', right_on='icao')
sensor_info = pd.read_csv('sensors_2023-02-23.csv')
print(len(airport_bbox))
print(len(airport_info))
print(len(airport_boxes))


start_time = "2021-09-01 11:00"
end_time = "2021-09-01 13:00"


airports_bounds={}
for index, row in airport_boxes.iterrows():
    airport_code = row['AIRPORT']
    bound_left_x = row['BoundLeftX']
    bound_left_y = row['BoundLeftY']
    bound_right_x = row['BoundRightX']
    bound_right_y = row['BoundRightY']
    altitude = row["elevation_ft"]
    airports_bounds[airport_code] = ((bound_left_x, bound_left_y,bound_right_x, bound_right_y),altitude)


map = Basemap(llcrnrlon=-10, llcrnrlat=35, urcrnrlon=40, urcrnrlat=70, resolution='l')

map.drawcoastlines()
map.drawcountries()
airport_boxes = pd.read_csv("APT_BBOX.csv")

for index, row in airport_boxes.iterrows():
    bound_left_x = row['BoundLeftX']
    bound_left_y = row['BoundLeftY']
    bound_right_x = row['BoundRightX']
    bound_right_y = row['BoundRightY']
    x = [bound_left_x, bound_right_x, bound_right_x, bound_left_x, bound_left_x]
    y = [bound_left_y, bound_left_y, bound_right_y, bound_right_y, bound_left_y]
    map.plot(x, y, latlon=True, color='r')
plt.show()


data={}
for key,value in airports_bounds.items():
    flights = opensky.history(start_time,end_time,bounds=airports_bounds[key][0])
    if flights is not None : 
        data[key]= flights.data
print(data.keys())


data['EBBR']


for key, flights in data.items():
    if flights is not None:
        dataset = flights.copy()
        max_altitude = 500 + airports_bounds[key][1]*0.3048
        dataset = dataset[dataset['altitude'] < max_altitude ]
        dataset['timestamp'] = pd.to_datetime(dataset['timestamp'])
        dataset['last_position'] = pd.to_datetime(dataset['last_position'])
        time_diff = (dataset['timestamp'] - dataset['last_position']).dt.total_seconds()
        dataset = dataset[(1 <= time_diff) & (time_diff <= 5)]
        callsigns = dataset['callsign'].unique()
        filtered_data = pd.DataFrame()
        for callsign in callsigns:
            subset = dataset[dataset['callsign'] == callsign]
            if subset['altitude'].min() < 30 + airports_bounds[key][1] * 0.3048:
                filtered_subset = subset.copy()
            else:
                filtered_subset = pd.DataFrame()
            filtered_data = pd.concat([filtered_data, filtered_subset])
        filtered_data.reset_index(drop=True, inplace=True)
        if 'callsign' in filtered_data.columns : 
            points_count = filtered_data['callsign'].value_counts()
            points_mean = points_count.mean()
            data_final = filtered_data.groupby('callsign').filter(lambda x: len(x) >= points_mean)
            data[key] = data_final
        else:
            data[key]=None



data['EBBR']


from geopy.distance import geodesic

def calculate_distances_trajectories(dataset):
    flight_distance_points = {}
    for flight in dataset['callsign'].unique():
        trajectory = dataset[dataset['callsign'] == flight].sort_values('timestamp')
        distances_points = []
        for i in range(1, len(trajectory)):
            point_prec = trajectory.iloc[i-1]
            point_actuel = trajectory.iloc[i]
            coord_prec = (point_prec['latitude'], point_prec['longitude'])
            coord_actuel = (point_actuel['latitude'], point_actuel['longitude'])
            diff_altitude = np.abs(point_actuel['altitude']-point_prec['altitude'])
            distance = geodesic(coord_prec, coord_actuel).meters
            distances_points.append(np.sqrt(distance**2+diff_altitude**2))
        flight_distance_points[flight]=distances_points
    return flight_distance_points



ebbr = calculate_distances_trajectories(data['EBBR'])


data['EBBR'][data['EBBR']['callsign']=='BEL3736']


ebbr['BEL3736']



def coverage_count(airports_bounds, start_time, end_time, max_height, data):
    airports_covered = {}
    airports_not_covered = {}
    for key in airports_bounds.keys():
        #flights = opensky.history(start_time, end_time, bounds=airports_bounds[key][0])
        flights = data[key]
        if flights is not None:
            max_altitude = max_height + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            if flights_airport is not None:
                airports_covered[key] = flights_airport.data.shape[0]
            else:
                #airports_not_covered[key] = 0
                airports_covered[key] = 0
                
    airports_covered_final = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True))            
    airports_covered = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(airports_covered)), airports_covered.values(), align='center')
    plt.xticks(range(len(airports_covered)), airports_covered.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Number of points with height <= 500 m')
    plt.show()
    return airports_covered_final




def mean_coverage(airports_bounds, start_time, end_time, data):
    airports_coverage = {}
    for key in airports_bounds.keys():
        flights = data[key]
        if flights is not None:
            max_altitude = 500 + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            flights


from geopy import distance

def surface(bounds):
    latitudeleft,latituderight, longitudeleft,longituderight = bounds[0],bounds[1],bounds[2],bounds[3]
    point_haut_droit = (latituderight, longituderight)
    point_bas_gauche = (latitudeleft, longitudeleft)
    distance_geodesique = distance.distance(point_haut_droit, point_bas_gauche).meters
    surface_km2 = (distance_geodesique / 1000) ** 2
    return surface_km2

def coverage_count_surface(airports_bounds,start_time,end_time, max_height, data):
    airports_covered={}
    airports_not_covered={}
    for key in airports_bounds.keys():
        #flights = opensky.history(start_time,end_time,bounds=airports_bounds[key][0],)
        flights = data[key]
        if flights is not None:
            max_altitude = max_height + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            if flights_airport is not None:
                bounds = airports_bounds[key][0]
                airport_area = surface(bounds)
                airports_covered[key] = flights_airport.data.shape[0] / airport_area
            else:
                #airports_not_covered[key] = 0
                airports_covered[key] = 0
    airports_covered_final = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True))
    airports_covered = dict(sorted(airports_covered.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(airports_covered)), airports_covered.values(), align='center')
    plt.xticks(range(len(airports_covered)), airports_covered.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Number of points / surface      with height <= 500 m')
    plt.show()
    return airports_covered_final



def calculate_entropy(proportions):
    entropy = 0.0
    for proportion in proportions:
        if proportion > 0:
            entropy -= proportion * np.log2(proportion)
    return entropy

def calculate_subbox_proportions(airport_flights, subboxes):
    subbox_counts = np.zeros(len(subboxes), dtype=int)
    for flight in airport_flights.iterrows():
        for i, subbox in enumerate(subboxes):
            if subbox[0][0]<flight[1][9]<subbox[1][0] and subbox[0][1]<flight[1][8]<subbox[1][1]:
                subbox_counts[i] += 1
                break
    total_count = len(airport_flights)
    proportions = subbox_counts / total_count
    return proportions

def calculate_spatial_entropy(airports_bounds, start_time, end_time, max_height, data, num_subboxes_per_dimension,):
    airports_entropy = {}
    for airport, bounds in airports_bounds.items():
        subboxes = []
        lon_step = (bounds[0][3] - bounds[0][1]) / num_subboxes_per_dimension
        lat_step = (bounds[0][2] - bounds[0][0]) / num_subboxes_per_dimension
        for i in range(num_subboxes_per_dimension):
            for j in range(num_subboxes_per_dimension):
                subbox = [[bounds[0][0] + i * lon_step, bounds[0][1] + j * lat_step],
                      [bounds[0][0] + (i+1) * lon_step, bounds[0][1] + (j+1) * lat_step]]
                subboxes.append(subbox)
        flights = data[airport]
        if flights is not None:
            max_altitude = max_height + airports_bounds[key][1]*0.3048
            flights_airport = flights.query(f"altitude <= {max_altitude}")
            if flights_airport is not None:
                proportions = calculate_subbox_proportions(flights_airport.data, subboxes)
                entropy = calculate_entropy(proportions)
                airports_entropy[airport] = entropy
            else:
                airports_entropy[airport] = 0.0
                
    airports_entropy_final = dict(sorted(airports_entropy.items(), key=lambda x: x[1], reverse=True))
    airports_entropy = dict(sorted(airports_entropy.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(airports_entropy)), airports_entropy.values(), align='center')
    plt.xticks(range(len(airports_entropy)), airports_entropy.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Spatial Entropy')
    plt.show()
    return airports_entropy_final


count         = coverage_count(airports_bounds,start_time,end_time,data)
count_surface = coverage_count_surface(airports_bounds,start_time,end_time,data)
entropy       = calculate_spatial_entropy(airports_bounds, start_time, end_time,data, 10)


def normalization(dict):
    max_value = max(list(dict.values()))
    for key,value in dict.items():
        if max_value!=0:
            dict[key]=value/max_value
        else:
            dict[key]=0
    return dict

def score(dict1,dict2,dict3):
    scores={}
    d1,d2,d3 = normalization(dict1),normalization(dict2),normalization(dict3)
    w1 = 0.3
    w2= 0.3
    w3 = 0.4
    for key in dict1.keys():
        if d1[key]==0 or d1[key]==0 or d1[key]==0 : 
            scores[key] = 0
        else:
            scores[key]= w1 * d1[key] + w2*d2[key] + w3*d3[key]
    scores = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))
    score_ = dict(sorted(scores.items(), key=lambda x: x[1], reverse=True)[:10])
    plt.bar(range(len(score_)), score_.values(), align='center')
    plt.xticks(range(len(score_)), score_.keys(), rotation='vertical')
    plt.xlabel('Airports')
    plt.ylabel('Score')
    plt.show()
    return scores

score(count,count_surface,entropy)
        


def calculate_distance(coord1, coord2):
    distance_geodesique = distance.distance(coord1, coord2).meters
    return distance_geodesique/1000

sensors ={}
sensor_info = sensor_info.dropna()
for index, row in sensor_info.iterrows():
    sensor_id = row['id']
    latitude = row['latitude']
    longitude = row['longitude']
    if -90 <= latitude <= 90: 
        sensors[sensor_id] = (latitude, longitude)
    
airports = {}
for index, row in airport_info.iterrows():
    icao = row['icao']
    latitude = row['latitude']
    longitude = row['longitude']
    airports[icao] =(latitude, longitude)

            
nearest_sensors={}
number_sensors={}
for icao, airport_position in airports.items():
    nearest_sensors[icao]=[]
    for sensor_id, sensor_position in sensors.items():
        distance_km = calculate_distance(airport_position, sensor_position)
        if distance_km < 10:
            nearest_sensors[icao].append(sensor_id)
    number_sensors[icao]=len(nearest_sensors[icao])
    


print(dict(sorted(number_sensors.items(), key=lambda x: x[1], reverse=True)))


data['LSZH'].data.isna().sum()


